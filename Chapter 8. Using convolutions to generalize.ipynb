{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Using convolutions to generalize\n",
    "This chapter covers\n",
    "* Understanding convolution\n",
    "* Building a convolutional neural network\n",
    "* Creating custom `nn.Module` subclasses\n",
    "* The difference between the module and functional APIs\n",
    "* Design choices for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 The case for convolutions\n",
    "In this section, we’ll see how convolutions deliver locality and translation invariance.\n",
    "We’ll do so by taking a close look at the formula defining convolutions and applying it\n",
    "using pen and paper—but don’t worry, the gist will be in pictures, not formulas.\n",
    "We said earlier that taking a 1D view of our input image and multiplying it by an\n",
    "`n_output_features × n_input_features` weight matrix, as is done in `nn.Linear` ,\n",
    "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature.\n",
    "\n",
    "We also said that, if we want to recognize patterns corresponding to objects, like an\n",
    "airplane in the sky, we will likely need to look at how nearby pixels are arranged, and\n",
    "we will be less interested in how pixels that are far from each other appear in combination. Essentially, it doesn’t matter if our image of a Spitfire has a tree or cloud or kite in the corner or not.\n",
    "\n",
    "In order to translate this intuition into mathematical form, we could compute the\n",
    "weighted sum of a pixel with its immediate neighbors, rather than with all other pixels\n",
    "in the image. This would be equivalent to building weight matrices, one per output\n",
    "feature and output pixel location, in which all weights beyond a certain distance from\n",
    "a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 What convolutions do\n",
    "We identified one more desired property earlier: we would like these localized patterns\n",
    "to have an effect on the output regardless of their location in the image: that is, to be\n",
    "translation invariant. To achieve this goal in a matrix applied to the image-as-a-vector we\n",
    "used in chapter 7 would require implementing a rather complicated pattern of weights\n",
    "(don’t worry if it is too complicated; it’ll get better shortly): most of the weight matrix\n",
    "would be zero (for entries corresponding to input pixels too far away from the output\n",
    "pixel to have an influence). For other weights, we would have to find a way to keep\n",
    "entries in sync that correspond to the same relative position of input and output pixels.\n",
    "This means we would need to initialize them to the same values and ensure that all these\n",
    "tied weights stayed the same while the network is updated during training. This way, we\n",
    "would ensure that weights operate in neighborhoods to respond to local patterns, and\n",
    "local patterns are identified no matter where they occur in the image.\n",
    "\n",
    "Of course, this approach is more than impractical. Fortunately, there is a readily\n",
    "available, local, translation-invariant linear operation on the image: a `convolution`. We\n",
    "can come up with a more compact description of a convolution, but what we are going\n",
    "to describe is exactly what we just delineated—only taken from a different angle.\n",
    "\n",
    "Convolution, or more precisely, `discrete convolution` (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the `kernel`, with every neighborhood in the input. Consider a\n",
    "3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as\n",
    "a 2D tensor\n",
    "\n",
    "```\n",
    "weight = torch.tensor([[w00, w01, w02],\n",
    "                       [w10, w11, w12],\n",
    "                       [w20, w21, w22]])\n",
    "```\n",
    "and a 1-channel, MxN image:\n",
    "```\n",
    "mage = torch.tensor([[i00,i01,i02,i03,...,i0N],\n",
    "                     [i10,i11,i12,i13,...,i1N],\n",
    "                     [i20,i21,i22,i23,...,i2N],\n",
    "                     [i30,i31,i32,i33,...,i3N],\n",
    "                     ...\n",
    "                     [iM0,iM1m iM2, iM3, ..., iMN]])\n",
    "```\n",
    "We can compute an element of the output image (without bias) as follows:\n",
    "```\n",
    "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\n",
    "      i21 * w10 + i22 * w11 + i23 * w12 +\n",
    "      i31 * w20 + i32 * w21 + i33 * w22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.1.png)\n",
    "\n",
    "Figure 8.1 shows this computation in action.\n",
    "That is, we “translate” the kernel on the `i11` location of the input image, and we\n",
    "multiply each weight by the value of the input image at the corresponding location.\n",
    "Thus, the output image is created by translating the kernel on all input locations and\n",
    "performing the weighted sum. For a multichannel image, like our RGB image, the\n",
    "weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\n",
    "\n",
    "Note that, just like the elements in the `weight` matrix of `nn.Linear` , the weights in\n",
    "the kernel are not known in advance, but they are initialized randomly and updated\n",
    "through backpropagation. Note also that the same kernel, and thus each weight in the\n",
    "kernel, is reused across the whole image. Thinking back to autograd, this means the use\n",
    "of each weight has a history spanning the entire image. Thus, the derivative of the loss\n",
    "with respect to a convolution weight includes contributions from the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s now possible to see the connection to what we were stating earlier: a convolution is\n",
    "equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.\n",
    "Summarizing, by switching to convolutions, we get\n",
    "* Local operations on neighborhoods\n",
    "* Translation invariance\n",
    "* Models with a lot fewer parameters\n",
    "\n",
    "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case\n",
    "in our fully connected model, but rather on the size of the convolution kernel (3 × 3,\n",
    "5 × 5, and so on) and on how many convolution filters (or output channels) we decide\n",
    "to use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutions in action\n",
    "Well, it looks like we’ve spent enough time down a rabbit hole! Let’s see some PyTorch\n",
    "in action on our birds versus airplanes challenge. The `torch.nn` module provides convolutions for 1, 2, and 3 dimensions: `nn.Conv1d` for time series, `nn.Conv2d` for images, and `nn.Conv3d` for volumes or videos.\n",
    "\n",
    "For our CIFAR-10 data, we’ll resort to `nn.Conv2d` . At a minimum, the arguments we\n",
    "provide to `nn.Conv2d` are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output\n",
    "features, and the size of the kernel. For instance, for our first convolutional module,\n",
    "we’ll have 3 input features per pixel (the RGB channels) and an arbitrary number of\n",
    "channels in the output—say, 16. The more channels in the output image, the more the\n",
    "capacity of the network. We need the channels to be able to detect many different types\n",
    "of features. Also, because we are randomly initializing them, some of the features we’ll\n",
    "get, even after training, will turn out to be useless. 2 Let’s stick to a kernel size of 3 × 3.\n",
    "\n",
    "It is very common to have kernel sizes that are the same in all directions, so\n",
    "PyTorch has a shortcut for this: whenever `kernel_size=3` is specified for a 2D convo-\n",
    "lution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it\n",
    "means 3 × 3 × 3. The CT scans we will see in part 2 of the book have a different voxel\n",
    "(volumetric pixel) resolution in one of the three axes. In such a case, it makes sense to\n",
    "consider kernels that have a different size for the exceptional dimension. But for now,\n",
    "we stick with having the same size of convolutions across all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f970572edb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3) # Instead of the shortcut kernel_size=3, we\n",
    "                                       # could equivalently pass in the tuple that we\n",
    "                                       # see in the output: kernel_size=(3, 3).\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to be the shape of the weight tensor? The kernel is of size 3 × 3, so\n",
    "we want the weight to consist of 3 × 3 parts. For a single output pixel value, our kernel\n",
    "would consider, say, `in_ch = 3` input channels, so the weight component for a single\n",
    "output pixel value (and by translation the invariance for the entire output channel) is\n",
    "of shape `in_ch × 3 × 3`. Finally, we have as many of those as we have output channels,\n",
    "here `out_ch = 16`, so the complete weight tensor is `out_ch × in_ch × 3 × 3`, in our case\n",
    "16 × 3 × 3 × 3. The bias will have size 16 (we haven’t talked about bias for a while for\n",
    "simplicity, but just as in the linear module case, it’s a constant value we add to each\n",
    "channel of the output image). Let’s verify our assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how convolutions are a convenient choice for learning from images. We\n",
    "have smaller models looking for local patterns whose weights are optimized across the\n",
    "entire image.\n",
    "\n",
    "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\n",
    "sum over neighborhoods of the input image. In our case, both the kernel weights and the bias `conv.weight` are initialized randomly, so the output image will not be particularly meaningful. As usual, we need to add the zeroth batch dimension with\n",
    "`unsqueeze` if we want to call the `conv` module with one input image, since `nn.Conv2d`\n",
    "expects a B × C × H × W shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re curious, so we can display the output, shown in figure 8.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute. Let’s take a look a the size of `output` : it’s `torch.Size([1, 16, 30,\n",
    "30])` . Huh; we lost a few pixels in the process. How did that happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Padding the boundary\n",
    "The fact that our output image is smaller than the input is a side effect of deciding what\n",
    "to do at the boundary of the image. Applying a convolution kernel as a weighted sum\n",
    "of pixels in a 3 × 3 neighborhood requires that there are neighbors in all directions. If\n",
    "we are at i00, we only have pixels to the right of and below us. By default, PyTorch will\n",
    "slide the convolution kernel within the input picture, getting `width - kernel_width + 1`\n",
    "horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side.\n",
    "This explains why we’re missing two pixels in each dimension.\n",
    "\n",
    "However, PyTorch gives us the possibility of **padding** the image by creating **ghost** pixels around the border that have value zero as far as the convolution is concerned. Figure 8.3 shows padding in action.\n",
    "In our case, specifying `padding=1` when `kernel_size=3` means i00 has an extra set\n",
    "of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image. 3 The net result is that the output has\n",
    "now the exact same size as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.2.png)\n",
    "Note that the sizes of `weight` and `bias` don’t change, regardless of whether padding is\n",
    "used.\n",
    "\n",
    "There are two main reasons to pad convolutions. First, doing so helps us separate\n",
    "the matters of convolution and changing image sizes, so we have one less thing to\n",
    "remember. And second, when we have more elaborate structures such as skip con-\n",
    "nections (discussed in section 8.5.3) or the U-Nets we’ll cover in part 2, we want the\n",
    "tensors before and after a few convolutions to be of compatible size so that we can\n",
    "add them or take differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Detecting features with convolutions\n",
    "We said earlier that `weight` and `bias` are parameters that are learned through back-\n",
    "propagation, exactly as it happens for `weight` and `bias` in `nn.Linear` . However, we can\n",
    "play with convolution by setting weights by hand and see what happens.\n",
    "Let’s first zero out `bias` , just to remove any confounding factors, and then set\n",
    "`weights` to a constant value so that each pixel in the output gets the mean of its neighbors. For each 3 × 3 neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have gone with `conv.weight.one_()` —that would result in each pixel in the\n",
    "output being the sum of the pixels in the neighborhood. Not a big difference, except\n",
    "that the values in the output image would have been nine times larger.\n",
    "\n",
    "Anyway, let’s see the effect on our CIFAR image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAXSklEQVR4nO2dXYxdZ3WGnxX//8XO+C+OY+oE+aIINQGNIqRUiJYWpQgpcAGCC5SLCHORSEWiF1EqlfSOVgXERYVkmghTESAqIKIqaomiogipSjE0JE6dJgG7xPHgmThx7ISQxDOrF7MjJuGsd8Z7Zs4Z8r2PNJoze51v77W/vdecc9Z71voiMzHGvPW5ZNQOGGOGg4PdmEZwsBvTCA52YxrBwW5MIzjYjWmE1YsZHBE3AF8GVgH/lJmfV8/ftGlTbtu2baBNSYDT09MDt19ySf2/atWqVaWt77jVqwdPl9qfOq+ZmZletmESEUtq67u/vvNY3Ttqf31tfa9nZVNjqrk6d+4cL7/88kBj72CPiFXAPwJ/DpwEfhwR92bm/1Rjtm3bxi233DLQ9pvf/KY81osvvjhw+/r168sxl156aWnbtGlTadu6dWtpGxsbu+j9vfbaa6WtOi+AX//616VtqVH/4NasWVPa1D+5tWvXDty+bt26Xse6cOFCaTt37lxpq+b4lVdeKcdU/yAAXn311dKmrpmyVff+yy+/XI6pXnjuvvvucsxi3sZfBzyVmb/IzFeBbwE3LmJ/xphlZDHBvhd4es7fJ7ttxpgVyGKCfdDngt/5QBMRByPiSEQceemllxZxOGPMYlhMsJ8E9s35+0rg1JuflJmHMnM8M8fVZ1tjzPKymGD/MXAgIq6KiLXAx4F7l8YtY8xS0zsbn5kXIuJW4N+Zld7uyszH1JiIKLOIKhNbZd03bNhw0WNAyycqe15lnzdv3tzrWNVcgJ4PJclUx1MZdzWP6t1YlXGHOuuusvFqPpRao7LnVTZezaHyo6+8pu6rKlOvVIZq7uV5lZYFkJn3AfctZh/GmOHgb9AZ0wgOdmMawcFuTCM42I1pBAe7MY2wqGx8HyqZRFU8KWmoom+RiZLKzpw5M3D7VVddVY6pimdAF1WobxsqGaqS2JQ8qKQmNfdqXCVTqoIWdQ/0LYSZmpoauF0VmagiKnV/qOIaZavuR3Wf9qmU8yu7MY3gYDemERzsxjSCg92YRnCwG9MIQ83GT09Pc/78+YE2VVRRZZL7FrSowgmVBa8yqqo9U3W+oP1X2WLFxo0bB25XhTDVGNAFRSpDXikNfZcbU1lmlemu/FBj+t47fTLuUCtHSlGqlAs1v35lN6YRHOzGNIKD3ZhGcLAb0wgOdmMawcFuTCMMXXp7/vnnB9qUNFTJJ30KMUAXd/RZSuiFF14oxygpRBV+KD+U/9U8qvlQKBlKFeRUMpS6zkq6UpJoH4lK+aEkxb7FLn3mSsl86t6p8Cu7MY3gYDemERzsxjSCg92YRnCwG9MIDnZjGmFR0ltEnADOA9PAhcwcV8+fnp4uZRIlM1TSm6rIUrLc1q1bS5vqGVctXaRkHHVeStZSkl2faj8lXakebn17xlVSn5qPycnJ0vb000+XtuPHj5e26njq/lAVh8p/Ja+peaxQPlY2dU2WQmf/k8x8dgn2Y4xZRvw23phGWGywJ/CDiPhJRBxcCoeMMcvDYt/GX5+ZpyJiF3B/RDyemQ/OfUL3T+Ag6OV/jTHLy6Je2TPzVPd7EvgecN2A5xzKzPHMHFdrcxtjlpfewR4RmyJiy+uPgQ8AR5fKMWPM0rKYt/G7ge91qf7VwN2Z+W9qQGaWMpqSLSr6yBmgK8pU88VKYlPNMtesWVPalEyi5B9V5VVJb2qulHSoZD5VfVedt1ry6tSpU6Xtscce6zWu+uh42WWXlWMU6popWU7ZqvtRXRcly5VjLnpER2b+Arim73hjzHCx9GZMIzjYjWkEB7sxjeBgN6YRHOzGNMJQG05mZilBKEmm+jKOqhpT0pVad6taVw5qGUrJU6oyT0mAfRpfQi3X9GlQuBgq/5XEqqrvlNzYR1ZUcqmyqWut/FD3Y3UfqyahlU1WIpYWY8xbCge7MY3gYDemERzsxjSCg92YRhh6Nr5a6kZlW6tsvMpWqmy2ysarbHE17rnnnivHqBp+ZVMZYVUqXO1T9d279NJLex1LZaYrxUDNr7pmO3bsKG27d+8ubVdeeeXA7eq8lI9nzpzpNa5PXzt1D/TBr+zGNIKD3ZhGcLAb0wgOdmMawcFuTCM42I1phKFLb1WxgJLeqn5mfQpCQBcsKDmvKlg4f/58OaZPAQTAzp07S5uSyqreamqM6iWnqGRUqOdYyZSqGEpJh7t27Spt+/fvH7hd3Ttnz54tbcpHVfSkZNbKF7W/Sjp0IYwxxsFuTCs42I1pBAe7MY3gYDemERzsxjTCvNJbRNwFfAiYzMx3dtvGgG8D+4ETwMcy8/n59qWq3pSkoaStir6LSPbp/aaWeFJSnlrCR0k127ZtK21btmwZuL3vfCjpUMloVT85tVST6kGnegNW56xs6rr0kbzms6lKy0qC7dM3UN6LCxj/NeCGN227DXggMw8AD3R/G2NWMPMGe7fe+pv/hd8IHO4eHwY+vMR+GWOWmL6f2Xdn5gRA97v+CpMxZkWw7F+XjYiDwEHo/7nRGLN4+r6yn46IPQDd78nqiZl5KDPHM3N8qdvsGGMWTt9gvxe4qXt8E/D9pXHHGLNcLER6+ybwPmBHRJwEPgd8HrgnIm4Gfgl8dCEHy8yySaGSvKoKKlU1pqre+jaqrHxXx1Iy2djYWGmrqtcANm7cWNqqCjZ1zqricGJiorSdPHmytFWVY6qiTM2jqsxTVYyV5KWkvD5VhQB79+4tbarKrmpUqcZUjS/VR+V5gz0zP1GY3j/fWGPMysHfoDOmERzsxjSCg92YRnCwG9MIDnZjGmGoDScjoqz0UpVGlUyiKsOU1KSkGrVeVyW9qeo1dV5q/bK+DSer4ymZUslhqrJtamqqtFUVbKqCUZ2Xos+1VvOhUBKgklLVuMoXdZ+eOHFi4PbFVr0ZY94CONiNaQQHuzGN4GA3phEc7MY0goPdmEYYuvRWVTYp+UrJCRV9quiglteUTVVrqQo1tX5Z37XZqp4BfSvKlJyk5Ct13hWqYkvJrGoeq/lQjTRfeuml0qbOWTWIVL0cqntV3Yvq/q7wK7sxjeBgN6YRHOzGNIKD3ZhGcLAb0whDzcYrVIa8T9GCyu73pcoWq35mKousMrQqs9unh57K7Kps/BVXXFHatm/fXtpUcU2FypCrc+6TxVfLMVVLlM1ne/75egU0tbxZdc2UglIVbKksvV/ZjWkEB7sxjeBgN6YRHOzGNIKD3ZhGcLAb0wgLWf7pLuBDwGRmvrPbdgfwKeD1JmS3Z+Z98+3rkksuKQskVOFHJZ+oYgvV+00tq6OklUriUf3ilPSmZJI+vfDUPpV8qeZDSZhbtmwpbZWP6pqpAhR1XZSM1qcnX9+iLCUfq957lcSm5reSbaVkW1p+y9eAGwZs/1JmXtv9zBvoxpjRMm+wZ+aDQN1i1Bjze8FiPrPfGhGPRMRdEVEvbWmMWRH0DfavAG8HrgUmgC9UT4yIgxFxJCKOqM9dxpjlpVewZ+bpzJzOzBngq8B14rmHMnM8M8fVd5iNMctLr2CPiD1z/vwIcHRp3DHGLBcLkd6+CbwP2BERJ4HPAe+LiGuBBE4An17IwdauXcvb3va2gTZVXVVJEEquU5VoStZ69tlnS1tVlaXesaiPLmppJVUBpqqhquoq5YeSjJSUo2SoStpSEtSLL75Y2lTV2+TkZGmr5kNVoSmUdKh8VLbq/t61a9dFj1H3xrzBnpmfGLD5zvnGGWNWFv4GnTGN4GA3phEc7MY0goPdmEZwsBvTCENtOLlhwwauueaagbYdO3aU4yq5Q1WUKYnkzJkzpe2JJ54obU8++eTA7efOnSvHKMlLNXpUVXvKVlWb9a0aUxKgkq8q6U3NvZJEVWWekjCr81Z+qPlQkm7fxqPVeStpuTovVRHpV3ZjGsHBbkwjONiNaQQHuzGN4GA3phEc7MY0wlClt/Xr13PgwIGBtmo71A0AZYVPz6aBx48fL22VXKOkH+WjktCUj1u3bi1tlZynpKsXXnihtKkqQDWukuVUU0k1V2o+1LpylRSlqu/Umm1qPpRUpmzVnCgZrbIpOdSv7MY0goPdmEZwsBvTCA52YxrBwW5MIww1G7969Wq2b98+0LZ79+5yXNUjTfVOU6jMtMrE/upXvxq4XWXj+/YlU0tDqaKhyy4b3MJf7U9ls0+fPl3aVAFQVdyh5n7btm2lTV1rZauOp4qQVBZcFRSp66nGVaqG2l+lNqm58Cu7MY3gYDemERzsxjSCg92YRnCwG9MIDnZjGmEhyz/tA74OXA7MAIcy88sRMQZ8G9jP7BJQH8vMWrf67f4u2smqn5nqj6YkCCWvqd5klUSlli1SkpcqClH7VMUYSparUEUhU1NTpU35XxVkqD5tl19+eWmrljsCvfxWhfKjki/nQ0llaq4qGU3FSp84Wsgr+wXgs5n5h8B7gFsi4h3AbcADmXkAeKD72xizQpk32DNzIjN/2j0+DxwD9gI3Aoe7px0GPrxcThpjFs9FfWaPiP3Au4CHgN2ZOQGz/xCAeslJY8zIWXCwR8Rm4DvAZzKz/p7k7447GBFHIuKI+qxsjFleFhTsEbGG2UD/RmZ+t9t8OiL2dPY9wMBFsjPzUGaOZ+Z438SHMWbxzBvsMZv2uxM4lplfnGO6F7ipe3wT8P2ld88Ys1QspOrteuCTwKMR8XC37Xbg88A9EXEz8Evgo/PtKDNLSayS16CWcdQyPUrqUB8nVMVT1ftNLVvUt9eZkt5Un7FKjlSykJpH5b+qYKtQMpmqequqJUH3d6t8VP3/1DtQdSw1x6p6sJJn+1RMykq50tKRmT8CKlHv/fONN8asDPwNOmMawcFuTCM42I1pBAe7MY3gYDemEYbacBJqKURVqVXShBrTtyJu06ZNpe2KK64YuH3t2rXlGCVdKRlKNbFUklclHSp5UNmUlLNx48bSVp2bktBU1du+fftKm5LKKglWLSellg5T56z2qRpOVlKqus59ZE+/shvTCA52YxrBwW5MIzjYjWkEB7sxjeBgN6YRhiq9qaq3PnLSqlWryjFKMlLN+tS4qlJKVWupxoZVFR3oppLV2mBQVw8qCVDNY9/qsOq81Tmr9dfU/aGq9qpx6pyVFKmqEZUEq86tkimVXKf8qPAruzGN4GA3phEc7MY0goPdmEZwsBvTCEPPxlfZUdWDrrKpXmznztXdrpVNZcGrcSp7qzK0KouvFAOV2a2y7iqz26egBbTSUNlUkYlSDE6ePFna+hSuqOuiFBm1PJiaq7GxsdJWKRRqyasqJqTSVFqMMW8pHOzGNIKD3ZhGcLAb0wgOdmMawcFuTCPMK71FxD7g68DlwAxwKDO/HBF3AJ8Cprqn3p6Z96l9zczMlL3hlBxWSTJq+aSnnnqqtB0/fry0PfPMM6Vtampq4HZViKGkENXvTsk/qtdZJXmpuVLSlZLX+kiHqv+fkj1VTz41V5XkpWQytQRY1Q8R9DxW/QsBrr766oHblVzXh4Xo7BeAz2bmTyNiC/CTiLi/s30pM/9hST0yxiwLC1nrbQKY6B6fj4hjwN7ldswYs7Rc1Gf2iNgPvAt4qNt0a0Q8EhF3RYQXXzdmBbPgYI+IzcB3gM9k5jngK8DbgWuZfeX/QjHuYEQciYgjqumCMWZ5WVCwR8QaZgP9G5n5XYDMPJ2Z05k5A3wVuG7Q2Mw8lJnjmTmuupQYY5aXeYM9ZtOqdwLHMvOLc7bvmfO0jwBHl949Y8xSsZBs/PXAJ4FHI+LhbtvtwCci4loggRPAp+fb0czMTCmxnT59uhxXVRpNTEyUYx5//PHSpsapjxpVlZ2qKFPVfEriUbKLqlKreqspeUpJRqoHnZLlqrmqlmMCPfd9++RVNuWHup5qnPJRSY7VO161v2p+1T21kGz8j4BBoqnU1I0xKwt/g86YRnCwG9MIDnZjGsHBbkwjONiNaYShNpy8cOECZ8+eLW0VVcXTqVOnLnoM6MaGqkqtkpqU9FNV+YFueqi+gKRs1bJAykdVyaXkH9X4spL6lBSpJC8lDyo/qvNW95uqwFT3lULNf3UfKx+r+VXLZPmV3ZhGcLAb0wgOdmMawcFuTCM42I1pBAe7MY0wVOltenq6bHxYSUZQN3RU0oRq5qjGqfXjKrlDjVGVXEryUhKgkuwqeXDXrl3lGOV/n2NB3YxSVfOpddSUj0qWq5pzqvPqe83UfaXWA6zOu0+zUuWfX9mNaQQHuzGN4GA3phEc7MY0goPdmEZwsBvTCEOV3jKzV6O8SvJS0o+q1lJrpamKuKpiq6+EpirA1PpxqjqsOu/t27eXY2STwp7NKCubGrNjx47SpqoH1VxVEpu6LkoeVNdM3TuqGq2So9XcVzFh6c0Y42A3phUc7MY0goPdmEZwsBvTCPNm4yNiPfAgsK57/r9k5uciYgz4NrCf2eWfPpaZg9OKcw9YZBhV9rzKnKrMo8q4q55lKrNbZYRVkYPyUaGKKlTWt8o+q+WfVFGIWmpKzWM1/6p/3rp160qbyp6rwpWqwEpdM1VEtXPnztKm5kopHlWmXikQlSKz2Gz8K8CfZuY1zC7PfENEvAe4DXggMw8AD3R/G2NWKPMGe87y+r/HNd1PAjcCh7vth4EPL4uHxpglYaHrs6/qVnCdBO7PzIeA3Zk5AdD9rgumjTEjZ0HBnpnTmXktcCVwXUS8c6EHiIiDEXEkIo6o5W6NMcvLRWXjM/Ms8EPgBuB0ROwB6H5PFmMOZeZ4Zo6rxIcxZnmZN9gjYmdEbOsebwD+DHgcuBe4qXvaTcD3l8tJY8ziWUghzB7gcESsYvafwz2Z+a8R8Z/APRFxM/BL4KMLOaCSICoqaULJU6rwQPmgJMBKNlTvWFThh0LJUEpWrOZEjVHHUrKcoioYUUU8feVSVRDVR/pU94C61qpIpo882+ceUBLlvMGemY8A7xqw/Qzw/vnGG2NWBv4GnTGN4GA3phEc7MY0goPdmEZwsBvTCNFHCut9sIgp4P+6P3cAzw7t4DX2443Yjzfy++bHH2TmwNK8oQb7Gw4ccSQzx0dycPthPxr0w2/jjWkEB7sxjTDKYD80wmPPxX68EfvxRt4yfozsM7sxZrj4bbwxjTCSYI+IGyLifyPiqYgYWe+6iDgREY9GxMMRcWSIx70rIiYj4uicbWMRcX9EPNn9vmxEftwREc90c/JwRHxwCH7si4j/iIhjEfFYRPxlt32ocyL8GOqcRMT6iPiviPhZ58ffdtsXNx+ZOdQfYBXwc+BqYC3wM+Adw/aj8+UEsGMEx30v8G7g6Jxtfw/c1j2+Dfi7EflxB/BXQ56PPcC7u8dbgCeAdwx7ToQfQ50TIIDN3eM1wEPAexY7H6N4Zb8OeCozf5GZrwLfYrZ5ZTNk5oPAc2/aPPQGnoUfQyczJzLzp93j88AxYC9DnhPhx1DJWZa8yesogn0v8PScv08yggntSOAHEfGTiDg4Ih9eZyU18Lw1Ih7p3uYv+8eJuUTEfmb7J4y0qemb/IAhz8lyNHkdRbAPaqUxKkng+sx8N/AXwC0R8d4R+bGS+ArwdmbXCJgAvjCsA0fEZuA7wGcy89ywjrsAP4Y+J7mIJq8Vowj2k8C+OX9fCZwagR9k5qnu9yTwPWY/YoyKBTXwXG4y83R3o80AX2VIcxIRa5gNsG9k5ne7zUOfk0F+jGpOumNfdJPXilEE+4+BAxFxVUSsBT7ObPPKoRIRmyJiy+uPgQ8AR/WoZWVFNPB8/Wbq+AhDmJOYbZx2J3AsM784xzTUOan8GPacLFuT12FlGN+Ubfwgs5nOnwN/PSIfrmZWCfgZ8Ngw/QC+yezbwdeYfadzM7Cd2WW0nux+j43Ij38GHgUe6W6uPUPw44+Z/Sj3CPBw9/PBYc+J8GOocwL8EfDf3fGOAn/TbV/UfPgbdMY0gr9BZ0wjONiNaQQHuzGN4GA3phEc7MY0goPdmEZwsBvTCA52Yxrh/wEhXah2Ak1fegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have predicted, the filter produces a blurred version of the image, as shown\n",
    "in figure 8.4. After all, every pixel of the output is the average of a neighborhood of the\n",
    "input, so pixels in the output are correlated and change more smoothly.\n",
    "\n",
    "Next, let’s try something different. The following kernel may look a bit mysterious\n",
    "at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the weighted sum for an arbitrary pixel in position 2,2, as we did earlier\n",
    "for the generic convolution kernel, we get\n",
    "```\n",
    "o22 = i13 - i11 +\n",
    "      i23 - i21 +\n",
    "      i33 - i31\n",
    "```\n",
    "\n",
    "which performs the difference of all pixels on the right of i22 minus the pixels on the\n",
    "left of i22. If the kernel is applied on a vertical boundary between two adjacent regions\n",
    "of different intensity, o22 will have a high value. If the kernel is applied on a region of\n",
    "uniform intensity, o22 will be zero. It’s an **edge-detection** kernel: the kernel highlights the\n",
    "vertical edge between two horizontally adjacent regions.\n",
    "Applying the convolution kernel to our image, we see the result shown in figure\n",
    "8.5. \n",
    "\n",
    "![](images/8.5.png)\n",
    "\n",
    "As expected, the convolution kernel enhances the vertical edges. We could build lots more elaborate filters, such as for detecting horizontal or diagonal edges, or crosslike or checkerboard patterns, where “detecting” means the output has a high magnitude. In fact, the job of a computer vision expert has historically been to come up with the most effective combination of filters so that certain features are highlighted in images and objects can be recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With deep learning, we let kernels be estimated from data in whatever way the dis-\n",
    "crimination is most effective: for instance, in terms of minimizing the negative cross-\n",
    "entropy loss between the output and the ground truth that we introduced in section\n",
    "7.2.5. From this angle, the job of a convolutional neural network is to estimate the ker-\n",
    "nel of a set of filter banks in successive layers that will transform a multichannel image\n",
    "into another multichannel image, where different channels correspond to different\n",
    "features (such as one channel for the average, another channel for vertical edges, and\n",
    "so on). Figure 8.6 shows how the training automatically learns the kernels.\n",
    "\n",
    "![](images/8.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 Looking further with depth and pooling\n",
    "This is all well and good, but conceptually there’s an elephant in the room. We got all\n",
    "excited because by moving from fully connected layers to convolutions, we achieve\n",
    "locality and translation invariance. Then we recommended the use of small kernels,\n",
    "like 3 × 3, or 5 × 5: that’s peak locality, all right. What about the **big picture**? How do we\n",
    "know that all structures in our images are 3 pixels or 5 pixels wide? Well, we don’t,\n",
    "because they aren’t. And if they aren’t, how are our networks going to be equipped to\n",
    "see those patterns with larger scope? This is something we’ll really need if we want to solve our birds versus airplanes problem effectively, since although CIFAR-10 images\n",
    "are small, the objects still have a (wing-)span several pixels across.\n",
    "\n",
    "One possibility could be to use large convolution kernels. Well, sure, at the limit we\n",
    "could get a 32 × 32 kernel for a 32 × 32 image, but we would converge to the old fully\n",
    "connected, affine transformation and lose all the nice properties of convolution.\n",
    "Another option, which is used in convolutional neural networks, is stacking one con-\n",
    "volution after the other and at the same time downsampling the image between suc-\n",
    "cessive convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FROM LARGE TO SMALL : DOWNSAMPLING**\n",
    "\n",
    "Downsampling could in principle occur in different ways. Scaling an image by half is\n",
    "the equivalent of taking four neighboring pixels as input and producing one pixel as\n",
    "output. How we compute the value of the output based on the values of the input is\n",
    "up to us. We could\n",
    "\n",
    "* **Average the four pixels**. This **average pooling** was a common approach early on but has fallen out of favor somewhat.\n",
    "* **Take the maximum of the four pixels**. This approach, called **max pooling**, is currently the most commonly used approach, but it has a downside of discarding the other three-quarters of the data.\n",
    "* **Perform a strided convolution, where only every Nth pixel is calculated**. A 3 × 4 convolu- tion with stride 2 still incorporates input from all pixels from the previous layer. The literature shows promise for this approach, but it has not yet supplanted max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be focusing on max pooling, illustrated in figure 8.7, going forward. The fig-\n",
    "ure shows the most common setup of taking non-overlapping 2 x 2 tiles and taking the\n",
    "maximum over each of them as the new pixel at the reduced scale.\n",
    "\n",
    "![](images/8.7.png)\n",
    "Intuitively, the output images from a convolution layer, especially since they are fol-\n",
    "lowed by an activation just like any other linear layer, tend to have a high magnitude where certain features corresponding to the estimated kernel are detected (such as\n",
    "vertical lines). By keeping the highest value in the 2 × 2 neighborhood as the downs-\n",
    "ampled output, we ensure that the features that are found survive the downsampling,\n",
    "at the expense of the weaker responses.\n",
    "\n",
    "Max pooling is provided by the `nn.MaxPool2d` module (as with convolution, there are\n",
    "versions for 1D and 3D data). It takes as input the size of the neighborhood over which\n",
    "to operate the pooling operation. If we wish to downsample our image by half, we’ll want\n",
    "to use a size of 2. Let’s verify that it works as expected directly on our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD**\n",
    "Let’s now see how combining convolutions and downsampling can help us recognize\n",
    "larger structures. In figure 8.8, we start by applying a set of 3 × 3 kernels on our 8 × 8\n",
    "image, obtaining a multichannel output image of the same size. Then we scale down\n",
    "the output image by half, obtaining a 4 × 4 image, and apply another set of 3 × 3 ker-\n",
    "nels to it. This second set of kernels operates on a 3 × 3 neighborhood of something\n",
    "that has been scaled down by half, so it effectively maps back to 8 × 8 neighborhoods\n",
    "of the input. In addition, the second set of kernels takes the output of the first set of\n",
    "kernels (features like averages, edges, and so on) and extracts additional features on\n",
    "top of those.\n",
    "\n",
    "So, on one hand, the first set of kernels operates on small neighborhoods on first-\n",
    "order, low-level features, while the second set of kernels effectively operates on wider\n",
    "neighborhoods, producing features that are compositions of the previous features.\n",
    "This is a very powerful mechanism that provides convolutional neural networks with\n",
    "the ability to see into very complex scenes—much more complex than our 32 × 32\n",
    "images from the CIFAR-10 dataset.\n",
    "\n",
    "![](images/8.8.png)\n",
    "\n",
    ">**The receptive field of output pixels** \\\n",
    "When the second 3 × 3 convolution kernel produces 21 in its conv output in figure\n",
    "8.8, this is based on the top-left 3 × 3 pixels of the first max pool output. They, in turn,\n",
    "correspond to the 6 × 6 pixels in the top-left corner in the first conv output, which in\n",
    "turn are computed by the first convolution from the top-left 7 × 7 pixels. So the pixel\n",
    "in the second convolution output is influenced by a 7 × 7 input square. The first\n",
    "convolution also uses an implicitly “padded” column and row to produce the output in\n",
    "the corner; otherwise, we would have an 8 × 8 square of input pixels informing a given\n",
    "pixel (away from the boundary) in the second convolution’s output. In fancy language,\n",
    "we say that a given output neuron of the 3 × 3-conv, 2 × 2-max-pool, 3 × 3-conv\n",
    "construction has a receptive field of 8 × 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 Putting it all together for our network\n",
    "With these building blocks in our hands, we can now proceed to build our convolu-\n",
    "tional neural network for detecting birds and airplanes. Let’s take our previous fully\n",
    "connected model as a starting point and introduce nn.Conv2d and nn.MaxPool2d as\n",
    "described previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # ...\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first convolution takes us from 3 RGB channels to 16, thereby giving the network\n",
    "a chance to generate 16 independent features that operate to (hopefully) discriminate low-level features of birds and airplanes. Then we apply the `Tanh` activation function. The resulting 16-channel 32 × 32 image is pooled to a 16-channel 16 × 16 image\n",
    "by the first `MaxPool3d` . At this point, the downsampled image undergoes another convolution that generates an 8-channel 16 × 16 output. With any luck, this output will\n",
    "consist of higher-level features. Again, we apply a `Tanh` activation and then pool to an\n",
    "8-channel 8 × 8 output.\n",
    "\n",
    "Where does this end? After the input image has been reduced to a set of 8 × 8 features, we expect to be able to output some probabilities from the network that we can\n",
    "feed to our negative log likelihood. However, probabilities are a pair of numbers in a\n",
    "1D vector (one for airplane, one for bird), but here we’re still dealing with multichannel 2D features.\n",
    "\n",
    "Thinking back to the beginning of this chapter, we already know what we need to\n",
    "do: turn the 8-channel 8 × 8 image into a 1D vector and complete our network with a\n",
    "set of fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3,padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3,padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ...  Warning: Something important is missing here!\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code gives us a neural network as shown in figure 8.9.\n",
    "\n",
    "![](images/8.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the “something missing” comment for a minute. Let’s first notice that the size\n",
    "of the linear layer is dependent on the expected size of the output of `MaxPool2d`: $8 × 8\n",
    "× 8 = 512$. Let’s count the number of parameters for this small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s very reasonable for a limited dataset of such small images. In order to increase\n",
    "the capacity of the model, we could increase the number of output channels for the\n",
    "convolution layers (that is, the number of features each convolution layer generates),\n",
    "which would lead the linear layer to increase its size as well.\n",
    "\n",
    "We put the “Warning” note in the code for a reason. The model has zero chance of\n",
    "running without complaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9c784fd7714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, the error message is a bit obscure, but not too much so. We find references to `linear` in the traceback: looking back at the model, we see that only module\n",
    "that has to have a $512 × 32$ tensor is `nn.Linear(512, 32)` , the first linear module after\n",
    "the last convolution block.\n",
    "\n",
    "What’s missing there is the reshaping step from an 8-channel $8 × 8$ image to a $512-\n",
    "element$, 1D vector (1D if we ignore the batch dimension, that is). This could be\n",
    "achieved by calling view on the output of the last `nn.MaxPool2d` , but unfortunately, we\n",
    "don’t have any explicit visibility of the output of each module when we use\n",
    "`nn.Sequential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Subclassing nn.Module\n",
    "At some point in developing neural networks, we will find ourselves in a situation where\n",
    "we want to compute something that the premade modules do not cover. Here, it is some-\n",
    "thing very simple like reshaping, 5 ; but in section 8.5.3, we use the same construction to\n",
    "implement residual connections. So in this section, we learn how to make our own\n",
    "`nn.Module` subclasses that we can then use just like the prebuilt ones or `nn.Sequential`.\n",
    "\n",
    "When we want to build models that do more complex things than just applying\n",
    "one layer after another, we need to leave `nn.Sequential` for something that gives us\n",
    "added flexibility. PyTorch allows us to use any computation in our model by subclass-\n",
    "ing `nn.Module`.\n",
    "\n",
    "In order to subclass `nn.Module` , at a minimum we need to define a `forward` function\n",
    "that takes the inputs to the module and returns the output. This is where we define our\n",
    "module’s computation. The name `forward` here is reminiscent of a distant past, when\n",
    "modules needed to define both the forward and backward passes we met in section\n",
    "5.5.1. With PyTorch, if we use standard `torch` operations, autograd will take care of the\n",
    "backward pass automatically; and indeed, an `nn.Module` never comes with a `backward`.\n",
    "\n",
    "Typically, our computation will use other modules—premade like convolutions or\n",
    "customized. To include these submodules, we typically define them in the constructor\n",
    "`__init__` and assign them to `self` for use in the `forward` function. They will, at the\n",
    "same time, hold their parameters throughout the lifetime of our module. Note that you\n",
    "need to call `super().__init__()` before you can do that (or PyTorch will remind you)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 Our network as an nn.Module\n",
    "Let’s write our network as a submodule. To do so, we instantiate all the `nn.Conv2d` ,\n",
    "`nn.Linear` , and so on that we previously passed to `nn.Sequential` in the constructor,\n",
    "and then use their instances one after another in `forward` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8) #     This reshape is what we were missing earlier.\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Net` class is equivalent to the `nn.Sequential` model\n",
    "we built earlier in terms of submodules; but by writing\n",
    "the `forward` function explicitly, we can manipulate the\n",
    "output of `self.pool3` directly and call `view` on it to turn\n",
    "it into a $B × N$ vector. Note that we leave the batch\n",
    "dimension as $–1$ in the call to `view` , since in principle we\n",
    "don’t know how many samples will be in the batch.\n",
    "\n",
    "Here we use a subclass of `nn.Module` to contain\n",
    "our entire model. We could also use subclasses to\n",
    "define new building blocks for more complex networks. Picking up on the diagram style in chapter 6,\n",
    "our network looks like the one shown in figure 8.10.\n",
    "We are making some ad hoc choices about what information to present where.\n",
    "\n",
    "Recall that the goal of classification networks typically is to compress information in the sense that we\n",
    "start with an image with a sizable number of pixels\n",
    "and compress it into (a vector of probabilities of)\n",
    "classes. Two things about our architecture deserve\n",
    "some commentary with respect to this goal.\n",
    "\n",
    "![](images/8.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, our goal is reflected by the size of our intermediate values generally\n",
    "shrinking—this is done by reducing the number of channels in the convolutions, by\n",
    "reducing the number of pixels through pooling, and by having an output dimension\n",
    "lower than the input dimension in the linear layers. This is a common trait of\n",
    "classification networks. However, in many popular architectures like the ResNets we saw\n",
    "in chapter 2 and discuss more in section 8.5.3, the reduction is achieved by pooling in\n",
    "the spatial resolution, but the number of channels increases (still resulting in a\n",
    "reduction in size). It seems that our pattern of fast information reduction works well\n",
    "with networks of limited depth and small images; but for deeper networks, the decrease\n",
    "is typically slower.\n",
    "\n",
    "Second, in one layer, there is not a reduction of output size with regard to input\n",
    "size: the initial convolution. If we consider a single output pixel as a vector of 32 elements (the channels), it is a linear transformation of 27 elements (as a convolution of\n",
    "3 channels × 3 × 3 kernel size)—only a moderate increase. In ResNet, the initial convolution generates 64 channels from 147 elements (3 channels × 7 × 7 kernel size). 6\n",
    "So the first layer is exceptional in that it greatly increases the overall dimension (as in\n",
    "channels times pixels) of the data flowing through it, but the mapping for each output pixel considered in isolation still has approximately as many outputs as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 How PyTorch keeps track of parameters and submodules\n",
    "Interestingly, assigning an instance of `nn.Module` to an attribute in an `nn.Module` , as\n",
    "we did in the earlier constructor, automatically registers the module as a submodule.\n",
    "\n",
    "\n",
    ">**NOTE**: The submodules must be top-level attributes, not buried inside list or\n",
    "dict instances! Otherwise the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model\n",
    "requires a `list` or `dict` of submodules, PyTorch provides `nn.ModuleList` and\n",
    "`nn.ModuleDict` .\n",
    "\n",
    "We can call arbitrary methods of an `nn.Module` subclass. For example, for a model\n",
    "where training is substantially different than its use, say, for prediction, it may make\n",
    "sense to have a `predict` method. Be aware that calling such methods will be similar to\n",
    "calling `forward` instead of the module itself—they will be ignorant of hooks, and the\n",
    "JIT does not see the module structure when using them because we are missing the\n",
    "equivalent of the `__call__` bits shown in section 6.2.1.\n",
    "\n",
    "This allows Net to have access to the parameters of its submodules without further\n",
    "action by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that the `parameters()` call delves into all submodules assigned\n",
    "as attributes in the constructor and recursively calls `parameters()` on them. No matter how nested the submodule, any `nn.Module` can access the list of all child parameters. By accessing their `grad` attribute, which has been populated by `autograd` , the\n",
    "optimizer will know how to change parameters to minimize the loss. We know that\n",
    "story from chapter 5.\n",
    "\n",
    "We now know how to implement our own modules—and we will need this a lot for\n",
    "part 2. Looking back at the implementation of the `Net` class, and thinking about the\n",
    "utility of registering submodules in the constructor so that we can access their parameters, it appears a bit of a waste that we are also registering submodules that have no parameters, like `nn.Tanh` and `nn.MaxPool2d` . Wouldn’t it be easier to call these\n",
    "directly in the `forward` function, just as we called `view` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 The functional API\n",
    "It sure would! And that’s why PyTorch has *functional* counterparts for every nn module.\n",
    "By “functional” here we mean “having no internal state”—in other words, “whose output value is solely and fully determined by the value input arguments.” Indeed, `torch.nn.functional` provides many functions that work like the modules we find in `nn` .\n",
    "But instead of working on the input arguments and stored parameters like the module counterparts, they take inputs and parameters as arguments to the function call.\n",
    "For instance, the functional counterpart of `nn.Linear` is `nn.functional.linear `,\n",
    "which is a function that has signature `linear(input, weight, bias=None)` . The\n",
    "weight and bias parameters are arguments to the function.\n",
    "\n",
    "Back to our model, it makes sense to keep using `nn` modules for `nn.Linear` and\n",
    "`nn.Conv2d` so that `Net` will be able to manage their `Parameter` s during training. However, we can safely switch to the functional counterparts of pooling and activation,\n",
    "since they have no parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot more concise than and fully equivalent to our previous definition of Net\n",
    "in section 8.3.1. Note that it would still make sense to instantiate modules that require\n",
    "several parameters for their initialization in the constructor.\n",
    "\n",
    "> **TIP**: While general-purpose scientific functions like `tanh` still exist in\n",
    "`torch.nn.functional` in version 1.0, those entry points are deprecated in\n",
    "favor of functions in the top-level `torch` namespace. More niche functions\n",
    "like `max_pool2d` will remain in `torch.nn.functional` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can make our own `nn.Module` if we need to, and we also have the functional API for cases when instantiating and then calling an `nn.Module` is overkill. This\n",
    "has been the last bit missing to understand how the code organization works in just\n",
    "about any neural network implemented in PyTorch.\n",
    "\n",
    "Let’s double-check that our model runs, and then we’ll get to the training loop:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0157,  0.1143]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got two numbers! Information flows correctly. We might not realize it right now,\n",
    "but in more complex models, getting the size of the first linear layer right is some-\n",
    "times a source of frustration. We’ve heard stories of famous practitioners putting in\n",
    "arbitrary numbers and then relying on error messages from PyTorch to backtrack the\n",
    "correct sizes for their linear layers. Lame, eh? Nah, it’s all legit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Training our convnet\n",
    "We’re now at the point where we can assemble our complete training loop. We already\n",
    "developed the overall structure in chapter 5, and the training loop looks much like\n",
    "the one from chapter 6, but here we will revisit it to add some details like some track-\n",
    "ing for accuracy. After we run our model, we will also have an appetite for a little more\n",
    "speed, so we will learn how to run our models fast on a GPU. But first let’s look at the\n",
    "training loop.\n",
    "\n",
    "Recall that the core of our convnet is two nested loops: an outer one over the\n",
    "*epochs* and an inner one of the `DataLoader` that produces batches from our `Dataset` .\n",
    "In each loop, we then have to\n",
    "\n",
    "1. Feed the inputs through the model (the forward pass).\n",
    "2. Compute the loss (also part of the forward pass).\n",
    "3. Zero any old gradients.\n",
    "4. Call loss.backward() to compute the gradients of the loss with respect to all parameters (the backward pass).\n",
    "5. Have the optimizer take a step in toward lower loss.\n",
    "\n",
    "Also, we collect and print some information. So here is our training loop, looking\n",
    "almost as it does in the previous chapter—but it is good to remember what each thing\n",
    "is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # Uses the datetime module included with Python\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1): # Our loop over the epochs,\n",
    "                                         # numbered from 1 to n_epochs\n",
    "                                         # rather than starting at 0\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:   # Loops over our dataset in the batches \n",
    "                                            # the data loader creates for us\n",
    "            outputs = model(imgs) # Feeds a batch through our model ...\n",
    "            loss = loss_fn(outputs, labels) # ... and computes the loss we wish to minimize\n",
    "            optimizer.zero_grad()   # After getting rid of the gradients from \n",
    "                                    # the last round ...\n",
    "            \n",
    "            loss.backward() #... performs the backward step. That is, we\n",
    "                            # compute the gradients of all parameters we\n",
    "                            # want the network to learn.\n",
    "            optimizer.step() # Updates the model\n",
    "            loss_train += loss.item() # Sums the losses we saw over the epoch.\n",
    "                                      # Recall that it is important to transform the loss to a\n",
    "                                      # Python number with .item(), to escape the gradients.\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            # Divides by the length of the training data loader to get the average loss per batch.\n",
    "            # This is a much more intuitive measure than the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `Dataset` from chapter 7; wrap it into a `DataLoader` ; instantiate our net-\n",
    "work, an optimizer, and a loss function as before; and call our training loop.\n",
    "The substantial changes in our model from the last chapter are that now our\n",
    "model is a custom subclass of `nn.Module` and that we’re using convolutions. Let’s run\n",
    "training for 100 epochs while printing the loss. Depending on your hardware, this\n",
    "may take 20 minutes or more to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoai_tran/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-19 10:59:24.450933 Epoch 1, Training loss 0.5633942090022336\n",
      "2021-03-19 10:59:38.457467 Epoch 10, Training loss 0.3290676716596458\n",
      "2021-03-19 10:59:54.050335 Epoch 20, Training loss 0.30339384515574025\n",
      "2021-03-19 11:00:09.191498 Epoch 30, Training loss 0.2817165527943593\n",
      "2021-03-19 11:00:25.062456 Epoch 40, Training loss 0.26411616555444756\n",
      "2021-03-19 11:00:40.509177 Epoch 50, Training loss 0.2445237998654888\n",
      "2021-03-19 11:00:55.949954 Epoch 60, Training loss 0.22570058233608867\n",
      "2021-03-19 11:01:11.559702 Epoch 70, Training loss 0.20735514097532648\n",
      "2021-03-19 11:01:27.108955 Epoch 80, Training loss 0.19103906759221084\n",
      "2021-03-19 11:01:43.038171 Epoch 90, Training loss 0.17726916535075304\n",
      "2021-03-19 11:01:58.645120 Epoch 100, Training loss 0.16299435698017953\n"
     ]
    }
   ],
   "source": [
    "# The DataLoader batches up the examples of our cifar2 dataset.\n",
    "# Shuffling randomizes the order of the examples from the dataset.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                            shuffle=True)\n",
    "\n",
    "model = Net() # Instantiates our network ...\n",
    "# ... the stochastic gradient descent optimizer we have been working with ...\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss() #  ... and the cross entropy loss we met in 7.10\n",
    "\n",
    "# Calls the training loop we defined earlier\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can train our network. But again, our friend the bird watcher will likely not\n",
    "be impressed when we tell her that we trained to very low training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Measuring accuracy\n",
    "In order to have a measure that is more interpretable than the loss, we can take a look\n",
    "at our accuracies on the training and validation datasets. We use the same code as in\n",
    "chapter 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                                shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                                shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # We do not want gradients here, as we will not want to update the parameters.\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                # Gives us the index of the highest value as output\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                # Counts the number of examples, so total is increased by the batch size\n",
    "                total += labels.shape[0]\n",
    "                # Comparing the predicted class that had the maximum probability and the ground-truth\n",
    "                # labels, we first get a Boolean array. Taking the sum gives the number of items\n",
    "                # in the batch prediction and ground truth agree.\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        \n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a lot better than the fully connected model, which achieved only 79%\n",
    "accuracy. We about halved the number of errors on the validation set. Also, we used\n",
    "far fewer parameters. This is telling us that the model does a better job of generalizing\n",
    "its task of recognizing the subject of images from a new sample, through locality and\n",
    "translation invariance. We could now let it run for more epochs and see what perfor-\n",
    "mance we could squeeze out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Saving and loading our model\n",
    "Since we’re satisfied with our model so far, it would be nice to actually save it, right?\n",
    "It’s easy to do. Let’s save the model to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The birds_vs_airplanes.pt file now contains all the parameters of model : that is,\n",
    "weights and biases for the two convolution modules and the two linear modules. So, no structure—just the weights. This means when we deploy the model in production\n",
    "for our friend, we’ll need to keep the `model` class handy, create an instance, and then\n",
    "load the parameters back into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will have to make sure we don’t change the definition of Net between saving and\n",
    "# later loading the model state.\n",
    "loaded_model = Net()\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                            + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3 Training on the GPU\n",
    "We have a net and can train it! But it would be good to make it a bit faster. It is no surprise by now that we do so by moving our training onto the GPU. Using the `.to`\n",
    "method we saw in chapter 3, we can move the tensors we get from the data loader to\n",
    "the GPU, after which our computation will automatically take place there. But we also\n",
    "need to move our parameters to the GPU. Happily, `nn.Module` implements a `.to` function that moves all of its parameters to the GPU (or casts the type when you pass a\n",
    "`dtype` argument)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a somewhat subtle difference between `Module.to` and `Tensor.to` .\n",
    "`Module.to` is in place: the module instance is modified. But `Tensor.to` is out of place\n",
    "(in some ways computation, just like `Tensor.tanh` ), returning a new tensor. One\n",
    "implication is that it is good practice to create the `Optimizer` after moving the parameters to the appropriate device.\n",
    "\n",
    "It is considered good style to move things to the GPU if one is available. A good\n",
    "pattern is to set the a variable `device` depending on `torch.cuda.is_available` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can amend the training loop by moving the tensors we get from the data\n",
    "loader to the GPU by using the `Tensor.to` method. Note that the code is exactly like\n",
    "our first version at the beginning of this section except for the two lines moving the\n",
    "inputs to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            # These two lines that move imgs and labels to the device we are training\n",
    "            # on are the only difference from our previous version.\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same amendment must be made to the `validate` function. We can then instantiate our model, move it to `device` , and run it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                            shuffle=True)\n",
    "\n",
    "# Moves our model (all parameters) to the GPU. If you forget to move either the\n",
    "# model or the inputs to the GPU, you will get errors about tensors not being on the same\n",
    "# device, because the PyTorch operators do not support mixing GPU and CPU inputs.\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "        n_epochs = 100,\n",
    "        optimizer = optimizer,\n",
    "        model = model,\n",
    "        loss_fn = loss_fn,\n",
    "        train_loader = train_loader,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for our small network here, we do see a sizable increase in speed. The advantage\n",
    "of computing on GPUs is more visible for larger models.\n",
    "\n",
    "There is a slight complication when loading network weights: PyTorch will attempt\n",
    "to load the weight to the same device it was saved from—that is, weights on the GPU\n",
    "will be restored to the GPU. As we don’t know whether we want the same device, we\n",
    "have two options: we could move the network to the CPU before saving it, or move it\n",
    "back after restoring. It is a bit more concise to instruct PyTorch to override the device\n",
    "information when loading weights. This is done by passing the `map_location` keyword\n",
    "argument to `torch.load` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                    + 'birds_vs_airplanes.pt',\n",
    "                                    map_location=device))"
   ]
  },
  {
   "source": [
    "## 8.5 Model design\n",
    "We built our model as a subclass of `nn.Module`, the de facto standard for all but the\n",
    "simplest models. Then we trained it successfully and saw how to use the GPU to train\n",
    "our models. We’ve reached the point where we can build a feed-forward convolutional\n",
    "neural network and train it successfully to classify images. The natural question is,\n",
    "what now? What if we are presented with a more complicated problem? Admittedly,\n",
    "our birds versus airplanes dataset wasn’t that complicated: the images were very small,\n",
    "and the object under investigation was centered and took up most of the viewport.\n",
    "\n",
    "If we moved to, say, ImageNet, we would find larger, more complex images, where\n",
    "the right answer would depend on multiple visual clues, often hierarchically organized. For instance, when trying to predict whether a dark brick shape is a remote\n",
    "control or a cell phone, the network could be looking for something like a screen.\n",
    "\n",
    "Plus images may not be our sole focus in the real world, where we have tabular\n",
    "data, sequences, and text. The promise of neural networks is sufficient flexibility to\n",
    "solve problems on all these kinds of data given the proper architecture (that is, the\n",
    "interconnection of layers or modules) and the proper loss function.\n",
    "\n",
    "PyTorch ships with a very comprehensive collection of modules and loss functions\n",
    "to implement state-of-the-art architectures ranging from feed-forward components to\n",
    "long short-term memory (LSTM) modules and transformer networks (two very popular architectures for sequential data). Several models are available through PyTorch\n",
    "Hub or as part of torchvision and other vertical community efforts.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.5.1 Adding memory capacity: Width\n",
    "Given our feed-forward architecture, there are a couple of dimensions we’d likely\n",
    "want to explore before getting into further complications. The first dimension is the\n",
    "*width* of the network: the number of neurons per layer, or channels per convolution.\n",
    "We can make a model wider very easily in PyTorch. We just specify a larger number of\n",
    "output channels in the first convolution and increase the subsequent layers accordingly, taking care to change the `forward` function to reflect the fact that we’ll now\n",
    "have a longer vector once we switch to fully connected layers:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.Max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.Max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "If we want to avoid hardcoding numbers in the definition of the model, we can easily\n",
    "pass a parameter to init and parameterize the width, taking care to also parameterize\n",
    "the call to `view` in the `forward` function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "        padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "The numbers specifying channels and features for each layer are directly related to\n",
    "the number of parameters in a model; all other things being equal, they increase the\n",
    "*capacity* of the model. As we did previously, we can look at how many parameters our\n",
    "model has now:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "source": [
    "The greater the capacity, the more variability in the inputs the model will be able to\n",
    "manage; but at the same time, the more likely overfitting will be, since the model can\n",
    "use a greater number of parameters to memorize unessential aspects of the input. We\n",
    "already went into ways to combat overfitting, the best being increasing the sample size\n",
    "or, in the absence of new data, augmenting existing data through artificial modifications of the same data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}