{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Using convolutions to generalize\n",
    "This chapter covers\n",
    "* Understanding convolution\n",
    "* Building a convolutional neural network\n",
    "* Creating custom `nn.Module` subclasses\n",
    "* The difference between the module and functional APIs\n",
    "* Design choices for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 The case for convolutions\n",
    "In this section, we’ll see how convolutions deliver locality and translation invariance.\n",
    "We’ll do so by taking a close look at the formula defining convolutions and applying it\n",
    "using pen and paper—but don’t worry, the gist will be in pictures, not formulas.\n",
    "We said earlier that taking a 1D view of our input image and multiplying it by an\n",
    "`n_output_features × n_input_features` weight matrix, as is done in `nn.Linear` ,\n",
    "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature.\n",
    "\n",
    "We also said that, if we want to recognize patterns corresponding to objects, like an\n",
    "airplane in the sky, we will likely need to look at how nearby pixels are arranged, and\n",
    "we will be less interested in how pixels that are far from each other appear in combination. Essentially, it doesn’t matter if our image of a Spitfire has a tree or cloud or kite in the corner or not.\n",
    "\n",
    "In order to translate this intuition into mathematical form, we could compute the\n",
    "weighted sum of a pixel with its immediate neighbors, rather than with all other pixels\n",
    "in the image. This would be equivalent to building weight matrices, one per output\n",
    "feature and output pixel location, in which all weights beyond a certain distance from\n",
    "a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 What convolutions do\n",
    "We identified one more desired property earlier: we would like these localized patterns\n",
    "to have an effect on the output regardless of their location in the image: that is, to be\n",
    "translation invariant. To achieve this goal in a matrix applied to the image-as-a-vector we\n",
    "used in chapter 7 would require implementing a rather complicated pattern of weights\n",
    "(don’t worry if it is too complicated; it’ll get better shortly): most of the weight matrix\n",
    "would be zero (for entries corresponding to input pixels too far away from the output\n",
    "pixel to have an influence). For other weights, we would have to find a way to keep\n",
    "entries in sync that correspond to the same relative position of input and output pixels.\n",
    "This means we would need to initialize them to the same values and ensure that all these\n",
    "tied weights stayed the same while the network is updated during training. This way, we\n",
    "would ensure that weights operate in neighborhoods to respond to local patterns, and\n",
    "local patterns are identified no matter where they occur in the image.\n",
    "\n",
    "Of course, this approach is more than impractical. Fortunately, there is a readily\n",
    "available, local, translation-invariant linear operation on the image: a `convolution`. We\n",
    "can come up with a more compact description of a convolution, but what we are going\n",
    "to describe is exactly what we just delineated—only taken from a different angle.\n",
    "\n",
    "Convolution, or more precisely, `discrete convolution` (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the `kernel`, with every neighborhood in the input. Consider a\n",
    "3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as\n",
    "a 2D tensor\n",
    "\n",
    "```\n",
    "weight = torch.tensor([[w00, w01, w02],\n",
    "                       [w10, w11, w12],\n",
    "                       [w20, w21, w22]])\n",
    "```\n",
    "and a 1-channel, MxN image:\n",
    "```\n",
    "mage = torch.tensor([[i00,i01,i02,i03,...,i0N],\n",
    "                     [i10,i11,i12,i13,...,i1N],\n",
    "                     [i20,i21,i22,i23,...,i2N],\n",
    "                     [i30,i31,i32,i33,...,i3N],\n",
    "                     ...\n",
    "                     [iM0,iM1m iM2, iM3, ..., iMN]])\n",
    "```\n",
    "We can compute an element of the output image (without bias) as follows:\n",
    "```\n",
    "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\n",
    "      i21 * w10 + i22 * w11 + i23 * w12 +\n",
    "      i31 * w20 + i32 * w21 + i33 * w22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.1.png)\n",
    "\n",
    "Figure 8.1 shows this computation in action.\n",
    "That is, we “translate” the kernel on the `i11` location of the input image, and we\n",
    "multiply each weight by the value of the input image at the corresponding location.\n",
    "Thus, the output image is created by translating the kernel on all input locations and\n",
    "performing the weighted sum. For a multichannel image, like our RGB image, the\n",
    "weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\n",
    "\n",
    "Note that, just like the elements in the `weight` matrix of `nn.Linear` , the weights in\n",
    "the kernel are not known in advance, but they are initialized randomly and updated\n",
    "through backpropagation. Note also that the same kernel, and thus each weight in the\n",
    "kernel, is reused across the whole image. Thinking back to autograd, this means the use\n",
    "of each weight has a history spanning the entire image. Thus, the derivative of the loss\n",
    "with respect to a convolution weight includes contributions from the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s now possible to see the connection to what we were stating earlier: a convolution is\n",
    "equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.\n",
    "Summarizing, by switching to convolutions, we get\n",
    "* Local operations on neighborhoods\n",
    "* Translation invariance\n",
    "* Models with a lot fewer parameters\n",
    "\n",
    "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case\n",
    "in our fully connected model, but rather on the size of the convolution kernel (3 × 3,\n",
    "5 × 5, and so on) and on how many convolution filters (or output channels) we decide\n",
    "to use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutions in action\n",
    "Well, it looks like we’ve spent enough time down a rabbit hole! Let’s see some PyTorch\n",
    "in action on our birds versus airplanes challenge. The `torch.nn` module provides convolutions for 1, 2, and 3 dimensions: `nn.Conv1d` for time series, `nn.Conv2d` for images, and `nn.Conv3d` for volumes or videos.\n",
    "\n",
    "For our CIFAR-10 data, we’ll resort to `nn.Conv2d` . At a minimum, the arguments we\n",
    "provide to `nn.Conv2d` are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output\n",
    "features, and the size of the kernel. For instance, for our first convolutional module,\n",
    "we’ll have 3 input features per pixel (the RGB channels) and an arbitrary number of\n",
    "channels in the output—say, 16. The more channels in the output image, the more the\n",
    "capacity of the network. We need the channels to be able to detect many different types\n",
    "of features. Also, because we are randomly initializing them, some of the features we’ll\n",
    "get, even after training, will turn out to be useless. 2 Let’s stick to a kernel size of 3 × 3.\n",
    "\n",
    "It is very common to have kernel sizes that are the same in all directions, so\n",
    "PyTorch has a shortcut for this: whenever `kernel_size=3` is specified for a 2D convo-\n",
    "lution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it\n",
    "means 3 × 3 × 3. The CT scans we will see in part 2 of the book have a different voxel\n",
    "(volumetric pixel) resolution in one of the three axes. In such a case, it makes sense to\n",
    "consider kernels that have a different size for the exceptional dimension. But for now,\n",
    "we stick with having the same size of convolutions across all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f73793fedb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3) # Instead of the shortcut kernel_size=3, we\n",
    "                                       # could equivalently pass in the tuple that we\n",
    "                                       # see in the output: kernel_size=(3, 3).\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to be the shape of the weight tensor? The kernel is of size 3 × 3, so\n",
    "we want the weight to consist of 3 × 3 parts. For a single output pixel value, our kernel\n",
    "would consider, say, `in_ch = 3` input channels, so the weight component for a single\n",
    "output pixel value (and by translation the invariance for the entire output channel) is\n",
    "of shape `in_ch × 3 × 3`. Finally, we have as many of those as we have output channels,\n",
    "here `out_ch = 16`, so the complete weight tensor is `out_ch × in_ch × 3 × 3`, in our case\n",
    "16 × 3 × 3 × 3. The bias will have size 16 (we haven’t talked about bias for a while for\n",
    "simplicity, but just as in the linear module case, it’s a constant value we add to each\n",
    "channel of the output image). Let’s verify our assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how convolutions are a convenient choice for learning from images. We\n",
    "have smaller models looking for local patterns whose weights are optimized across the\n",
    "entire image.\n",
    "\n",
    "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\n",
    "sum over neighborhoods of the input image. In our case, both the kernel weights and the bias `conv.weight` are initialized randomly, so the output image will not be particularly meaningful. As usual, we need to add the zeroth batch dimension with\n",
    "`unsqueeze` if we want to call the `conv` module with one input image, since `nn.Conv2d`\n",
    "expects a B × C × H × W shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re curious, so we can display the output, shown in figure 8.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute. Let’s take a look a the size of `output` : it’s `torch.Size([1, 16, 30,\n",
    "30])` . Huh; we lost a few pixels in the process. How did that happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Padding the boundary\n",
    "The fact that our output image is smaller than the input is a side effect of deciding what\n",
    "to do at the boundary of the image. Applying a convolution kernel as a weighted sum\n",
    "of pixels in a 3 × 3 neighborhood requires that there are neighbors in all directions. If\n",
    "we are at i00, we only have pixels to the right of and below us. By default, PyTorch will\n",
    "slide the convolution kernel within the input picture, getting `width - kernel_width + 1`\n",
    "horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side.\n",
    "This explains why we’re missing two pixels in each dimension.\n",
    "\n",
    "However, PyTorch gives us the possibility of **padding** the image by creating **ghost** pixels around the border that have value zero as far as the convolution is concerned. Figure 8.3 shows padding in action.\n",
    "In our case, specifying `padding=1` when `kernel_size=3` means i00 has an extra set\n",
    "of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image. 3 The net result is that the output has\n",
    "now the exact same size as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.2.png)\n",
    "Note that the sizes of `weight` and `bias` don’t change, regardless of whether padding is\n",
    "used.\n",
    "\n",
    "There are two main reasons to pad convolutions. First, doing so helps us separate\n",
    "the matters of convolution and changing image sizes, so we have one less thing to\n",
    "remember. And second, when we have more elaborate structures such as skip con-\n",
    "nections (discussed in section 8.5.3) or the U-Nets we’ll cover in part 2, we want the\n",
    "tensors before and after a few convolutions to be of compatible size so that we can\n",
    "add them or take differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Detecting features with convolutions\n",
    "We said earlier that `weight` and `bias` are parameters that are learned through back-\n",
    "propagation, exactly as it happens for `weight` and `bias` in `nn.Linear` . However, we can\n",
    "play with convolution by setting weights by hand and see what happens.\n",
    "Let’s first zero out `bias` , just to remove any confounding factors, and then set\n",
    "`weights` to a constant value so that each pixel in the output gets the mean of its neighbors. For each 3 × 3 neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
