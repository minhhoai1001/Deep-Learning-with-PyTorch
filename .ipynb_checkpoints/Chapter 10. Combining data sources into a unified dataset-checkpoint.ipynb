{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10. Combining data sources into a unified dataset\n",
    "This chapter covers\n",
    "- Loading and processing raw data files\n",
    "- Implementing a Python class to represent our data\n",
    "- Converting our data into a format usable by PyTorch\n",
    "- Visualizing the training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to be able to produce a training sample given our inputs of raw CT\n",
    "scan data and a list of annotations for those CTs. This might sound simple, but\n",
    "quite a bit needs to happen before we can load, process, and extract the data we’re interested in. Figure 10.2 shows what we’ll need to do to turn our raw data into a training sample. Luckily, we got a head start on **understanding** our data in the last chapter,\n",
    "but we have more work to do on that front as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/9.1.png)\n",
    "\n",
    "![](images/9.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Raw CT data files\n",
    "Our CT data comes in two files: a **.mhd** file containing metadata header information,\n",
    "and a **.raw** file containing the raw bytes that make up the 3D array. Each file’s name starts\n",
    "with a unique identifier called the series UID (the name comes from the **Digital Imaging\n",
    "and Communications in Medicine** [DICOM] nomenclature) for the CT scan in question. For example, for series UID 1.2.3, there would be two files: 1.2.3.mhd and 1.2.3.raw.\n",
    "\n",
    "Our `Ct` class will consume those two files and produce the 3D array, as well as the\n",
    "transformation matrix to convert from the patient coordinate system (which we will\n",
    "discuss in more detail in section 10.6) to the index, row, column coordinates needed\n",
    "by the array (these coordinates are shown as (I,R,C) in the figures and are denoted\n",
    "with `_irc` variable suffixes in the code). Don’t sweat the details of all this right now;\n",
    "just remember that we’ve got some coordinate system conversion to do before we can\n",
    "apply these coordinates to our CT data. We’ll explore the details as we need them.\n",
    "We will also load the annotation data provided by LUNA, which will give us a list of\n",
    "nodule coordinates, each with a malignancy flag, along with the series UID of the relevant CT scan. By combining the nodule coordinate with coordinate system transformation information, we get the index, row, and column of the voxel at the center of\n",
    "our nodule.\n",
    "\n",
    "Using the (I,R,C) coordinates, we can crop a small 3D slice of our CT data to use as\n",
    "the input to our model. Along with this 3D sample array, we must construct the rest of\n",
    "our training sample tuple, which will have the sample array, nodule status flag, series\n",
    "UID, and the index of this sample in the CT list of nodule candidates. This sample\n",
    "tuple is exactly what PyTorch expects from our `Dataset` subclass and represents the\n",
    "last section of our bridge from our original raw data to the standard structure of\n",
    "PyTorch tensors.\n",
    "\n",
    "Limiting or cropping our data so as not to drown our model in noise is important,\n",
    "as is making sure we’re not so aggressive that our signal gets cropped out of our\n",
    "input. We want to make sure the range of our data is well behaved, especially after\n",
    "normalization. Clamping our data to remove outliers can be useful, especially if our\n",
    "data is prone to extreme outliers. We can also create handcrafted, algorithmic transformations of our input; this is known as **feature engineering**; and we discussed it briefly\n",
    "in chapter 1. We’ll usually want to let the model do most of the heavy lifting; feature\n",
    "engineering has its uses, but we won’t use it here in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Parsing LUNA’s annotation data\n",
    "The first thing we need to do is begin loading our data. When working on a new project, that’s often a good place to start. Making sure we know how to work with the raw\n",
    "input is required no matter what, and knowing how our data will look after it loads can help inform the structure of our early experiments. We could try loading individual CT scans, but we think it makes sense to parse the CSV files that LUNA provides,\n",
    "which contain information about the points of interest in each CT scan. As we can see\n",
    "in figure 10.3, we expect to get some coordinate information, an indication of\n",
    "whether the coordinate is a nodule, and a unique identifier for the CT scan. Since\n",
    "there are fewer types of information in the CSV files, and they’re easier to parse, we’re\n",
    "hoping they will give us some clues about what to look for once we start loading CTs. \n",
    "\n",
    "![](images/9.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `candidates.csv` file contains information about all lumps that potentially look like\n",
    "nodules, whether those lumps are malignant, benign tumors, or something else altogether. We’ll use this as the basis for building a complete list of candidates that can\n",
    "then be split into our training and validation datasets. The following Bash shell session shows what the file contains:\n",
    "\n",
    "```\n",
    "$ wc -l candidates.csv # Counts the number of lines in the file\n",
    "551066 candidates.csv\n",
    "\n",
    "$ head data/part2/luna/candidates.csv # Prints the first few lines of the file\n",
    "seriesuid,coordX,coordY,coordZ,class  # The first line of the .csv file defines the column headers.\n",
    "1.3...6860,-56.08,-67.85,-311.92,0\n",
    "1.3...6860,53.21,-244.41,-245.17,0\n",
    "1.3...6860,103.66,-121.8,-286.62,0\n",
    "1.3...6860,-33.66,-72.75,-308.41,0\n",
    "...\n",
    "\n",
    "$ grep ',1$' candidates.csv | wc -l # Counts the number of lines that end with 1, which indicates malignancy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** The values in the `seriesuid` column have been elided to better fit the\n",
    "printed page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 551,000 lines, each with a `seriesuid` (which we’ll call `series_uid` in the\n",
    "code), some (X,Y,Z) coordinates, and a class column that corresponds to the nodule\n",
    "status (it’s a Boolean value: 0 for a candidate that is not an actual nodule, and 1 for a\n",
    "candidate that is a nodule, either malignant or benign). We have 1,351 candidates\n",
    "flagged as actual nodules.\n",
    "\n",
    "The `annotations.csv` file contains information about some of the candidates that\n",
    "have been flagged as nodules. We are interested in the `diameter_mm` information in\n",
    "particular:\n",
    "\n",
    "```\n",
    "$ wc -l annotations.csv\n",
    "1187 annotations.csv # This is a different umber than in the umber than in the\n",
    "\n",
    "$ head data/part2/luna/annotations.csv\n",
    "seriesuid,coordX,coordY,coordZ,diameter_mm  # The last column is also different.\n",
    "1.3.6...6860,-128.6994211,-175.3192718,-298.3875064,5.651470635\n",
    "1.3.6...6860,103.7836509,-211.9251487,-227.12125,4.224708481\n",
    "1.3.6...5208,69.63901724,-140.9445859,876.3744957,5.786347814\n",
    "1.3.6...0405,-24.0138242,192.1024053,-391.0812764,8.143261683\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have size information for about 1,200 nodules. This is useful, since we can use it to\n",
    "make sure our training and validation data includes a representative spread of nodule\n",
    "sizes. Without this, it’s possible that our validation set could end up with only extreme\n",
    "values, making it seem as though our model is underperforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 Training and validation sets\n",
    "Let’s get back to our nodules. We’re going to sort them by size and take every Nth one for our validation set. That should give us the representative spread we’re looking  for. Unfortunately, the location information provided in `annotations.csv` doesn’t always precisely line up with the coordinates in `candidates.csv`:\n",
    "\n",
    "```\n",
    "$ grep 100225287222365663678666836860 annotations.csv\n",
    "1.3.6...6860,-128.6994211,-175.3192718,-298.3875064,5.651470635\n",
    "1.3.6...6860,103.7836509,-211.9251487,-227.12125,4.224708481\n",
    "\n",
    "$ grep '100225287222365663678666836860.*,1$' candidates.csv\n",
    "1.3.6...6860,104.16480444,-211.685591018,-227.011363746,1\n",
    "1.3.6...6860,-128.94,-175.04,-297.87,1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we truncate the corresponding coordinates from each file, we end up with (`–128.70,\n",
    "–175.32,–298.39`) versus (`–128.94,–175.04,–297.87`). Since the nodule in question has\n",
    "a diameter of 5 mm, both of these points are clearly meant to be the “center” of the\n",
    "nodule, but they don’t line up exactly. It would be a perfectly valid response to decide\n",
    "that dealing with this **data mismatch isn’t worth it**, and to **ignore the file**. We are going\n",
    "to do the legwork to make things line up, though, since real-world datasets are often\n",
    "imperfect this way, and this is a good example of the kind of work you will need to do\n",
    "to assemble data from disparate data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Unifying our annotation and candidate data\n",
    "Now that we know what our raw data files look like, let’s build a `getCandidateInfoList` function that will stitch it all together. We’ll use a named tuple that is defined at\n",
    "the top of the file to hold the information for each nodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.1 dsets.py:7\n",
    "```\n",
    "from collections import namedtuple\n",
    "# ... line 27\n",
    "CandidateInfoTuple = namedtuple(\n",
    "'CandidateInfoTuple',\n",
    "'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of candidate information will have the nodule status (what we’re going to be\n",
    "training the model to classify), diameter (useful for getting a good spread in training, since large and small nodules will not have the same features), series (to locate the\n",
    "correct CT scan), and candidate center (to find the candidate in the larger CT). The\n",
    "function that will build a list of these `NoduleInfoTuple` instances starts by using an inmemory caching decorator, followed by getting the list of files present on disk.\n",
    "\n",
    "```\n",
    "@functools.lru_cache(1)  # Standard library in memory caching\n",
    "def getCandidateInfoList(requireOnDisk_bool=True): # requireOnDisk_bool defaults to screening out series from data subsets \n",
    "                                                   # that aren’t in place yet\n",
    "    mhd_list = glob.glob('data-unversioned/part2/luna/subset*/*.mhd') \n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since parsing some of the data files can be slow, we’ll cache the results of this function call in memory. This will come in handy later, because we’ll be calling this function more often in future chapters. Speeding up our data pipeline by carefully applying **inmemory** or **on-disk caching** can result in some pretty impressive gains in training speed. Keep an eye out for these opportunities as you work on your projects.\n",
    "\n",
    "Earlier we said that we’ll support running our training program with less than the\n",
    "full set of training data, due to the long download times and high disk space requirements. The `requireOnDisk_bool` parameter is what makes good on that promise;\n",
    "we’re detecting which LUNA series UIDs are actually present and ready to be loaded\n",
    "from disk, and we’ll use that information to limit which entries we use from the CSV\n",
    "files we’re about to parse. Being able to run a subset of our data through the training\n",
    "loop can be useful to verify that the code is working as intended. Often a model’s\n",
    "training results are bad to useless when doing so, but exercising our logging, metrics,\n",
    "model check-pointing, and similar functionality is beneficial.\n",
    "\n",
    "After we get our candidate information, we want to merge in the diameter information from `annotations.csv`. First we need to group our annotations by `series_uid`,\n",
    "as that’s the first key we’ll use to cross-reference each row from the two files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 10.3 dsets.py:40, def getCandidateInfoList\n",
    "```\n",
    "candidateInfo_list = []\n",
    "with open('data/part2/luna/candidates.csv', \"r\") as f:\n",
    "    for row in list(csv.reader(f))[1:]:\n",
    "        series_uid = row[0]\n",
    "        if series_uid not in presentOnDisk_set and requireOnDisk_bool: # it’s in a subset we don’t have on disk so we should skip it.\n",
    "            continue\n",
    "        isNodule_bool = bool(int(row[4]))\n",
    "        candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "        candidateDiameter_mm = 0.0\n",
    "        for annotation_tup in diameter_dict.get(series_uid, []):\n",
    "            annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
    "            for i in range(3):\n",
    "                delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                if delta_mm > annotationDiameter_mm / 4:  # Divides the diameter by 2 to get the radius, and divides  \n",
    "                                                          # the radius by 2 to require that the two nodule center points\n",
    "                                                          # not be too far apart relative to the size of the nodule\n",
    "                break\n",
    "            else:\n",
    "                candidateDiameter_mm = annotationDiameter_mm\n",
    "            break\n",
    "        candidateInfo_list.append(CandidateInfoTuple(\n",
    "            isNodule_bool,\n",
    "            candidateDiameter_mm,\n",
    "            series_uid,\n",
    "            candidateCenter_xyz,\n",
    "            ))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the candidate entries for a given `series_uid`, we loop through the annotations we collected earlier for the same series_uid and see if the two coordinates are\n",
    "close enough to consider them the same nodule. If they are, great! Now we have diameter information for that nodule. If we don’t find a match, that’s fine; we’ll just treat\n",
    "the nodule as having a 0.0 diameter. Since we’re only using this information to get a\n",
    "good spread of nodule sizes in our training and validation sets, having incorrect diameter sizes for some nodules shouldn’t be a problem, but we should remember we’re\n",
    "doing this in case our assumption here is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.5 dsets.py:80, def getCandidateInfoList\n",
    "```\n",
    "# This means we have all of the actual nodule samples starting with the largest first, \n",
    "# followed by all of the non-nodule samples (which don’t have nodule size information).\n",
    "candidateInfo_list.sort(reverse=True)\n",
    "return candidateInfo_list\n",
    "```\n",
    "The ordering of the tuple members in `noduleInfo_list` is driven by this sort. We’re\n",
    "using this sorting approach to help ensure that when we take a slice of the data, that\n",
    "slice gets a representative chunk of the actual nodules with a good spread of nodule\n",
    "diameters. We’ll discuss this more in section 10.5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Loading individual CT scans\n",
    "We need to be able to take our `CT` data from a pile of bits on disk and turn it\n",
    "into a Python object from which we can extract 3D nodule density data. We can see this\n",
    "path from the `.mhd` and `.raw` files to `Ct` objects in figure 10.4. Our nodule annotation\n",
    "information acts like a map to the interesting parts of our raw data. Before we can follow\n",
    "that map to our data of interest, we need to get the data into an addressable form.\n",
    "\n",
    "![](images/10.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LUNA has converted the data we’re going to be using for this chapter into\n",
    "the MetaIO format, which is quite a bit easier to use (https://itk.org/Wiki/MetaIO/\n",
    "Documentation#Quick_Start). Don’t worry if you’ve never heard of the format\n",
    "before! We can treat the format of the data files as a black box and use `SimpleITK` to\n",
    "load them into more familiar `NumPy` arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.6 dsets.py:9\n",
    "```\n",
    "import SimpleITK as sitk\n",
    "# ... line 83\n",
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "        'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
    "        )[0]\n",
    "        # sitk.ReadImage implicitly consumes the .raw file in addition to the passed-in .mhd file.\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        # file in addition to the passed-in .mhd file. to convert the value type to np.float32\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For real projects, you’ll want to understand what types of information are contained in your raw data, but it’s perfectly fine to rely on third-party code like `SimpleITK` to parse the bits on disk. Finding the right balance of knowing everything about your inputs versus blindly accepting whatever your data-loading library hands you will probably take some experience. Just remember that we’re mostly concerned about **data**, not **bits**. It’s the information that matters, not how it’s represented.\n",
    "\n",
    "The 10 subsets we discussed earlier have about 90 CT scans each (888 in total),\n",
    "with every CT scan represented as two files: one with a `.mhd` extension and one with a\n",
    "`.raw` extension. The data being split between multiple files is hidden behind the `sitk`\n",
    "routines, however, and is not something we need to be directly concerned with.\n",
    "\n",
    "At this point, `ct_a` is a three-dimensional array. All three dimensions are spatial,\n",
    "and the single intensity channel is implicit. As we saw in chapter 4, in a PyTorch tensor, the channel information is represented as a fourth dimension with size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Hounsfield Units\n",
    "Recall that earlier, we said that we need to understand our data, not the bits that store\n",
    "it. Here, we have a perfect example of that in action. Without understanding the\n",
    "nuances of our data’s values and range, we’ll end up feeding values into our model\n",
    "that will hinder its ability to learn what we want it to.\n",
    "\n",
    "Continuing the `__init__` method, we need to do a bit of cleanup on the ct_a values. CT scan voxels are expressed in Hounsfield units (HU; https://en.wikipedia.org/\n",
    "wiki/Hounsfield_scale), which are odd units; air is –1,000 HU (close enough to 0 g/cc\n",
    "[grams per cubic centimeter] for our purposes), water is 0 HU (1 g/cc), and bone is\n",
    "at least +1,000 HU (2–3 g/cc).\n",
    "\n",
    "> **NOTE** HU values are typically stored on disk as signed 12-bit integers (shoved\n",
    "into 16-bit integers), which fits well with the level of precision CT scanners\n",
    "can provide. While this is perhaps interesting, it’s not particularly relevant to\n",
    "the project\n",
    "\n",
    "Some CT scanners use HU values that correspond to negative densities to indicate\n",
    "that those voxels are outside of the CT scanner’s field of view. For our purposes, everything outside of the patient should be air, so we discard that field-of-view information\n",
    "by setting a lower bound of the values to –1,000 HU. Similarly, the exact densities of\n",
    "bones, metal implants, and so on are not relevant to our use case, so we cap density at\n",
    "roughly 2 g/cc (1,000 HU) even though that’s not biologically accurate in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.7 dsets.py:96, Ct.__init__\n",
    "```\n",
    "ct_a.clip(-1000, 1000, ct_a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove all of these outlier values from our data: they aren’t directly relevant to our goal, and having those outliers can make the model’s job harder. This can\n",
    "happen in many ways, but a common example is when batch normalization is fed\n",
    "these outlier values and the statistics about how to best normalize the data are skewed.\n",
    "Always be on the lookout for ways to clean your data.\n",
    "\n",
    "All of the values we’ve built are now assigned to `self`.\n",
    "\n",
    "```\n",
    "self.series_uid = series_uid\n",
    "self.hu_a = ct_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Locating a nodule using the patient coordinate system\n",
    "Deep learning models typically need fixed-size inputs, due to having a fixed number\n",
    "of input neurons. We need to be able to produce a fixed-size array containing the candidate so that we can use it as input to our classifier. We’d like to train our model\n",
    "using a crop of the CT scan that has a candidate nicely centered, since then our\n",
    "model doesn’t have to learn how to notice nodules tucked away in the corner of the\n",
    "input. By reducing the variation in expected inputs, we make the model’s job easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.1 The patient coordinate system\n",
    "Unfortunately, all of the candidate center data we loaded in section 10.2 is expressed\n",
    "in millimeters, not voxels! We can’t just plug locations in millimeters into an array\n",
    "index and expect everything to work out the way we want. As we can see in figure 10.5,\n",
    "we need to transform our coordinates from the millimeter-based coordinate system\n",
    "(X,Y,Z) they’re expressed in, to the voxel-address-based coordinate system (I,R,C)\n",
    "used to take array slices from our CT scan data. This is a classic example of how it’s\n",
    "important to handle units consistently!\n",
    "\n",
    "![](images/10.5.png)\n",
    "\n",
    "As we have mentioned previously, when dealing with CT scans, we refer to the array\n",
    "dimensions as **index**, **row**, and **column**, because a separate meaning exists for X, Y, and Z, as illustrated in figure 10.6. The *patient coordinate system* defines positive X to be patientleft (left), positive Y to be patient-behind (*posterior*), and positive Z to be toward-patienthead (*superior*). Left-posterior-superior is sometimes abbreviated LPS.\n",
    "\n",
    "![](images/10.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patient coordinate system is measured in millimeters and has an arbitrarily positioned origin that does not correspond to the origin of the CT voxel array, as shown in\n",
    "figure 10.7.\n",
    "The patient coordinate system is often used to specify the locations of interesting\n",
    "anatomy in a way that is independent of any particular scan. The metadata that\n",
    "defines the relationship between the CT array and the patient coordinate system is\n",
    "stored in the header of DICOM files, and that meta-image format preserves the data\n",
    "in its header as well. This metadata allows us to construct the transformation from\n",
    "**(X,Y,Z)** to **(I,R,C)** that we saw in figure 10.5. The raw data contains many other fields\n",
    "of similar metadata, but since we don’t have a use for them right now, those unneeded\n",
    "fields will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
