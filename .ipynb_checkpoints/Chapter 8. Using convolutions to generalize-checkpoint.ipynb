{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Using convolutions to generalize\n",
    "This chapter covers\n",
    "* Understanding convolution\n",
    "* Building a convolutional neural network\n",
    "* Creating custom `nn.Module` subclasses\n",
    "* The difference between the module and functional APIs\n",
    "* Design choices for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 The case for convolutions\n",
    "In this section, we’ll see how convolutions deliver locality and translation invariance.\n",
    "We’ll do so by taking a close look at the formula defining convolutions and applying it\n",
    "using pen and paper—but don’t worry, the gist will be in pictures, not formulas.\n",
    "We said earlier that taking a 1D view of our input image and multiplying it by an\n",
    "`n_output_features × n_input_features` weight matrix, as is done in `nn.Linear` ,\n",
    "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature.\n",
    "\n",
    "We also said that, if we want to recognize patterns corresponding to objects, like an\n",
    "airplane in the sky, we will likely need to look at how nearby pixels are arranged, and\n",
    "we will be less interested in how pixels that are far from each other appear in combination. Essentially, it doesn’t matter if our image of a Spitfire has a tree or cloud or kite in the corner or not.\n",
    "\n",
    "In order to translate this intuition into mathematical form, we could compute the\n",
    "weighted sum of a pixel with its immediate neighbors, rather than with all other pixels\n",
    "in the image. This would be equivalent to building weight matrices, one per output\n",
    "feature and output pixel location, in which all weights beyond a certain distance from\n",
    "a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 What convolutions do\n",
    "We identified one more desired property earlier: we would like these localized patterns\n",
    "to have an effect on the output regardless of their location in the image: that is, to be\n",
    "translation invariant. To achieve this goal in a matrix applied to the image-as-a-vector we\n",
    "used in chapter 7 would require implementing a rather complicated pattern of weights\n",
    "(don’t worry if it is too complicated; it’ll get better shortly): most of the weight matrix\n",
    "would be zero (for entries corresponding to input pixels too far away from the output\n",
    "pixel to have an influence). For other weights, we would have to find a way to keep\n",
    "entries in sync that correspond to the same relative position of input and output pixels.\n",
    "This means we would need to initialize them to the same values and ensure that all these\n",
    "tied weights stayed the same while the network is updated during training. This way, we\n",
    "would ensure that weights operate in neighborhoods to respond to local patterns, and\n",
    "local patterns are identified no matter where they occur in the image.\n",
    "\n",
    "Of course, this approach is more than impractical. Fortunately, there is a readily\n",
    "available, local, translation-invariant linear operation on the image: a `convolution`. We\n",
    "can come up with a more compact description of a convolution, but what we are going\n",
    "to describe is exactly what we just delineated—only taken from a different angle.\n",
    "\n",
    "Convolution, or more precisely, `discrete convolution` (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the `kernel`, with every neighborhood in the input. Consider a\n",
    "3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as\n",
    "a 2D tensor\n",
    "\n",
    "```\n",
    "weight = torch.tensor([[w00, w01, w02],\n",
    "                       [w10, w11, w12],\n",
    "                       [w20, w21, w22]])\n",
    "```\n",
    "and a 1-channel, MxN image:\n",
    "```\n",
    "mage = torch.tensor([[i00,i01,i02,i03,...,i0N],\n",
    "                     [i10,i11,i12,i13,...,i1N],\n",
    "                     [i20,i21,i22,i23,...,i2N],\n",
    "                     [i30,i31,i32,i33,...,i3N],\n",
    "                     ...\n",
    "                     [iM0,iM1m iM2, iM3, ..., iMN]])\n",
    "```\n",
    "We can compute an element of the output image (without bias) as follows:\n",
    "```\n",
    "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\n",
    "      i21 * w10 + i22 * w11 + i23 * w12 +\n",
    "      i31 * w20 + i32 * w21 + i33 * w22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.1.png)\n",
    "\n",
    "Figure 8.1 shows this computation in action.\n",
    "That is, we “translate” the kernel on the `i11` location of the input image, and we\n",
    "multiply each weight by the value of the input image at the corresponding location.\n",
    "Thus, the output image is created by translating the kernel on all input locations and\n",
    "performing the weighted sum. For a multichannel image, like our RGB image, the\n",
    "weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\n",
    "\n",
    "Note that, just like the elements in the `weight` matrix of `nn.Linear` , the weights in\n",
    "the kernel are not known in advance, but they are initialized randomly and updated\n",
    "through backpropagation. Note also that the same kernel, and thus each weight in the\n",
    "kernel, is reused across the whole image. Thinking back to autograd, this means the use\n",
    "of each weight has a history spanning the entire image. Thus, the derivative of the loss\n",
    "with respect to a convolution weight includes contributions from the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s now possible to see the connection to what we were stating earlier: a convolution is\n",
    "equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.\n",
    "Summarizing, by switching to convolutions, we get\n",
    "* Local operations on neighborhoods\n",
    "* Translation invariance\n",
    "* Models with a lot fewer parameters\n",
    "\n",
    "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case\n",
    "in our fully connected model, but rather on the size of the convolution kernel (3 × 3,\n",
    "5 × 5, and so on) and on how many convolution filters (or output channels) we decide\n",
    "to use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutions in action\n",
    "Well, it looks like we’ve spent enough time down a rabbit hole! Let’s see some PyTorch\n",
    "in action on our birds versus airplanes challenge. The `torch.nn` module provides convolutions for 1, 2, and 3 dimensions: `nn.Conv1d` for time series, `nn.Conv2d` for images, and `nn.Conv3d` for volumes or videos.\n",
    "\n",
    "For our CIFAR-10 data, we’ll resort to `nn.Conv2d` . At a minimum, the arguments we\n",
    "provide to `nn.Conv2d` are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output\n",
    "features, and the size of the kernel. For instance, for our first convolutional module,\n",
    "we’ll have 3 input features per pixel (the RGB channels) and an arbitrary number of\n",
    "channels in the output—say, 16. The more channels in the output image, the more the\n",
    "capacity of the network. We need the channels to be able to detect many different types\n",
    "of features. Also, because we are randomly initializing them, some of the features we’ll\n",
    "get, even after training, will turn out to be useless. 2 Let’s stick to a kernel size of 3 × 3.\n",
    "\n",
    "It is very common to have kernel sizes that are the same in all directions, so\n",
    "PyTorch has a shortcut for this: whenever `kernel_size=3` is specified for a 2D convo-\n",
    "lution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it\n",
    "means 3 × 3 × 3. The CT scans we will see in part 2 of the book have a different voxel\n",
    "(volumetric pixel) resolution in one of the three axes. In such a case, it makes sense to\n",
    "consider kernels that have a different size for the exceptional dimension. But for now,\n",
    "we stick with having the same size of convolutions across all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5a649e4db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3) # Instead of the shortcut kernel_size=3, we\n",
    "                                       # could equivalently pass in the tuple that we\n",
    "                                       # see in the output: kernel_size=(3, 3).\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to be the shape of the weight tensor? The kernel is of size 3 × 3, so\n",
    "we want the weight to consist of 3 × 3 parts. For a single output pixel value, our kernel\n",
    "would consider, say, `in_ch = 3` input channels, so the weight component for a single\n",
    "output pixel value (and by translation the invariance for the entire output channel) is\n",
    "of shape `in_ch × 3 × 3`. Finally, we have as many of those as we have output channels,\n",
    "here `out_ch = 16`, so the complete weight tensor is `out_ch × in_ch × 3 × 3`, in our case\n",
    "16 × 3 × 3 × 3. The bias will have size 16 (we haven’t talked about bias for a while for\n",
    "simplicity, but just as in the linear module case, it’s a constant value we add to each\n",
    "channel of the output image). Let’s verify our assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how convolutions are a convenient choice for learning from images. We\n",
    "have smaller models looking for local patterns whose weights are optimized across the\n",
    "entire image.\n",
    "\n",
    "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\n",
    "sum over neighborhoods of the input image. In our case, both the kernel weights and the bias `conv.weight` are initialized randomly, so the output image will not be particularly meaningful. As usual, we need to add the zeroth batch dimension with\n",
    "`unsqueeze` if we want to call the `conv` module with one input image, since `nn.Conv2d`\n",
    "expects a B × C × H × W shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re curious, so we can display the output, shown in figure 8.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute. Let’s take a look a the size of `output` : it’s `torch.Size([1, 16, 30,\n",
    "30])` . Huh; we lost a few pixels in the process. How did that happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Padding the boundary\n",
    "The fact that our output image is smaller than the input is a side effect of deciding what\n",
    "to do at the boundary of the image. Applying a convolution kernel as a weighted sum\n",
    "of pixels in a 3 × 3 neighborhood requires that there are neighbors in all directions. If\n",
    "we are at i00, we only have pixels to the right of and below us. By default, PyTorch will\n",
    "slide the convolution kernel within the input picture, getting `width - kernel_width + 1`\n",
    "horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side.\n",
    "This explains why we’re missing two pixels in each dimension.\n",
    "\n",
    "However, PyTorch gives us the possibility of **padding** the image by creating **ghost** pixels around the border that have value zero as far as the convolution is concerned. Figure 8.3 shows padding in action.\n",
    "In our case, specifying `padding=1` when `kernel_size=3` means i00 has an extra set\n",
    "of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image. 3 The net result is that the output has\n",
    "now the exact same size as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.2.png)\n",
    "Note that the sizes of `weight` and `bias` don’t change, regardless of whether padding is\n",
    "used.\n",
    "\n",
    "There are two main reasons to pad convolutions. First, doing so helps us separate\n",
    "the matters of convolution and changing image sizes, so we have one less thing to\n",
    "remember. And second, when we have more elaborate structures such as skip con-\n",
    "nections (discussed in section 8.5.3) or the U-Nets we’ll cover in part 2, we want the\n",
    "tensors before and after a few convolutions to be of compatible size so that we can\n",
    "add them or take differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Detecting features with convolutions\n",
    "We said earlier that `weight` and `bias` are parameters that are learned through back-\n",
    "propagation, exactly as it happens for `weight` and `bias` in `nn.Linear` . However, we can\n",
    "play with convolution by setting weights by hand and see what happens.\n",
    "Let’s first zero out `bias` , just to remove any confounding factors, and then set\n",
    "`weights` to a constant value so that each pixel in the output gets the mean of its neighbors. For each 3 × 3 neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have gone with `conv.weight.one_()` —that would result in each pixel in the\n",
    "output being the sum of the pixels in the neighborhood. Not a big difference, except\n",
    "that the values in the output image would have been nine times larger.\n",
    "\n",
    "Anyway, let’s see the effect on our CIFAR image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have predicted, the filter produces a blurred version of the image, as shown\n",
    "in figure 8.4. After all, every pixel of the output is the average of a neighborhood of the\n",
    "input, so pixels in the output are correlated and change more smoothly.\n",
    "\n",
    "Next, let’s try something different. The following kernel may look a bit mysterious\n",
    "at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the weighted sum for an arbitrary pixel in position 2,2, as we did earlier\n",
    "for the generic convolution kernel, we get\n",
    "```\n",
    "o22 = i13 - i11 +\n",
    "      i23 - i21 +\n",
    "      i33 - i31\n",
    "```\n",
    "\n",
    "which performs the difference of all pixels on the right of i22 minus the pixels on the\n",
    "left of i22. If the kernel is applied on a vertical boundary between two adjacent regions\n",
    "of different intensity, o22 will have a high value. If the kernel is applied on a region of\n",
    "uniform intensity, o22 will be zero. It’s an **edge-detection** kernel: the kernel highlights the\n",
    "vertical edge between two horizontally adjacent regions.\n",
    "Applying the convolution kernel to our image, we see the result shown in figure\n",
    "8.5. \n",
    "\n",
    "![](images/8.5.png)\n",
    "\n",
    "As expected, the convolution kernel enhances the vertical edges. We could build lots more elaborate filters, such as for detecting horizontal or diagonal edges, or crosslike or checkerboard patterns, where “detecting” means the output has a high magnitude. In fact, the job of a computer vision expert has historically been to come up with the most effective combination of filters so that certain features are highlighted in images and objects can be recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With deep learning, we let kernels be estimated from data in whatever way the dis-\n",
    "crimination is most effective: for instance, in terms of minimizing the negative cross-\n",
    "entropy loss between the output and the ground truth that we introduced in section\n",
    "7.2.5. From this angle, the job of a convolutional neural network is to estimate the ker-\n",
    "nel of a set of filter banks in successive layers that will transform a multichannel image\n",
    "into another multichannel image, where different channels correspond to different\n",
    "features (such as one channel for the average, another channel for vertical edges, and\n",
    "so on). Figure 8.6 shows how the training automatically learns the kernels.\n",
    "\n",
    "![](images/8.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 Looking further with depth and pooling\n",
    "This is all well and good, but conceptually there’s an elephant in the room. We got all\n",
    "excited because by moving from fully connected layers to convolutions, we achieve\n",
    "locality and translation invariance. Then we recommended the use of small kernels,\n",
    "like 3 × 3, or 5 × 5: that’s peak locality, all right. What about the **big picture**? How do we\n",
    "know that all structures in our images are 3 pixels or 5 pixels wide? Well, we don’t,\n",
    "because they aren’t. And if they aren’t, how are our networks going to be equipped to\n",
    "see those patterns with larger scope? This is something we’ll really need if we want to solve our birds versus airplanes problem effectively, since although CIFAR-10 images\n",
    "are small, the objects still have a (wing-)span several pixels across.\n",
    "\n",
    "One possibility could be to use large convolution kernels. Well, sure, at the limit we\n",
    "could get a 32 × 32 kernel for a 32 × 32 image, but we would converge to the old fully\n",
    "connected, affine transformation and lose all the nice properties of convolution.\n",
    "Another option, which is used in convolutional neural networks, is stacking one con-\n",
    "volution after the other and at the same time downsampling the image between suc-\n",
    "cessive convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FROM LARGE TO SMALL : DOWNSAMPLING**\n",
    "\n",
    "Downsampling could in principle occur in different ways. Scaling an image by half is\n",
    "the equivalent of taking four neighboring pixels as input and producing one pixel as\n",
    "output. How we compute the value of the output based on the values of the input is\n",
    "up to us. We could\n",
    "\n",
    "* **Average the four pixels**. This **average pooling** was a common approach early on but has fallen out of favor somewhat.\n",
    "* **Take the maximum of the four pixels**. This approach, called **max pooling**, is currently the most commonly used approach, but it has a downside of discarding the other three-quarters of the data.\n",
    "* **Perform a strided convolution, where only every Nth pixel is calculated**. A 3 × 4 convolu- tion with stride 2 still incorporates input from all pixels from the previous layer. The literature shows promise for this approach, but it has not yet supplanted max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be focusing on max pooling, illustrated in figure 8.7, going forward. The fig-\n",
    "ure shows the most common setup of taking non-overlapping 2 x 2 tiles and taking the\n",
    "maximum over each of them as the new pixel at the reduced scale.\n",
    "\n",
    "![](images/8.7.png)\n",
    "Intuitively, the output images from a convolution layer, especially since they are fol-\n",
    "lowed by an activation just like any other linear layer, tend to have a high magnitude where certain features corresponding to the estimated kernel are detected (such as\n",
    "vertical lines). By keeping the highest value in the 2 × 2 neighborhood as the downs-\n",
    "ampled output, we ensure that the features that are found survive the downsampling,\n",
    "at the expense of the weaker responses.\n",
    "\n",
    "Max pooling is provided by the `nn.MaxPool2d` module (as with convolution, there are\n",
    "versions for 1D and 3D data). It takes as input the size of the neighborhood over which\n",
    "to operate the pooling operation. If we wish to downsample our image by half, we’ll want\n",
    "to use a size of 2. Let’s verify that it works as expected directly on our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD**\n",
    "Let’s now see how combining convolutions and downsampling can help us recognize\n",
    "larger structures. In figure 8.8, we start by applying a set of 3 × 3 kernels on our 8 × 8\n",
    "image, obtaining a multichannel output image of the same size. Then we scale down\n",
    "the output image by half, obtaining a 4 × 4 image, and apply another set of 3 × 3 ker-\n",
    "nels to it. This second set of kernels operates on a 3 × 3 neighborhood of something\n",
    "that has been scaled down by half, so it effectively maps back to 8 × 8 neighborhoods\n",
    "of the input. In addition, the second set of kernels takes the output of the first set of\n",
    "kernels (features like averages, edges, and so on) and extracts additional features on\n",
    "top of those.\n",
    "\n",
    "So, on one hand, the first set of kernels operates on small neighborhoods on first-\n",
    "order, low-level features, while the second set of kernels effectively operates on wider\n",
    "neighborhoods, producing features that are compositions of the previous features.\n",
    "This is a very powerful mechanism that provides convolutional neural networks with\n",
    "the ability to see into very complex scenes—much more complex than our 32 × 32\n",
    "images from the CIFAR-10 dataset.\n",
    "\n",
    "![](images/8.8.png)\n",
    "\n",
    ">**The receptive field of output pixels** \\\n",
    "When the second 3 × 3 convolution kernel produces 21 in its conv output in figure\n",
    "8.8, this is based on the top-left 3 × 3 pixels of the first max pool output. They, in turn,\n",
    "correspond to the 6 × 6 pixels in the top-left corner in the first conv output, which in\n",
    "turn are computed by the first convolution from the top-left 7 × 7 pixels. So the pixel\n",
    "in the second convolution output is influenced by a 7 × 7 input square. The first\n",
    "convolution also uses an implicitly “padded” column and row to produce the output in\n",
    "the corner; otherwise, we would have an 8 × 8 square of input pixels informing a given\n",
    "pixel (away from the boundary) in the second convolution’s output. In fancy language,\n",
    "we say that a given output neuron of the 3 × 3-conv, 2 × 2-max-pool, 3 × 3-conv\n",
    "construction has a receptive field of 8 × 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 Putting it all together for our network\n",
    "With these building blocks in our hands, we can now proceed to build our convolu-\n",
    "tional neural network for detecting birds and airplanes. Let’s take our previous fully\n",
    "connected model as a starting point and introduce nn.Conv2d and nn.MaxPool2d as\n",
    "described previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # ...\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first convolution takes us from 3 RGB channels to 16, thereby giving the network\n",
    "a chance to generate 16 independent features that operate to (hopefully) discriminate low-level features of birds and airplanes. Then we apply the `Tanh` activation function. The resulting 16-channel 32 × 32 image is pooled to a 16-channel 16 × 16 image\n",
    "by the first `MaxPool3d` . At this point, the downsampled image undergoes another convolution that generates an 8-channel 16 × 16 output. With any luck, this output will\n",
    "consist of higher-level features. Again, we apply a `Tanh` activation and then pool to an\n",
    "8-channel 8 × 8 output.\n",
    "\n",
    "Where does this end? After the input image has been reduced to a set of 8 × 8 features, we expect to be able to output some probabilities from the network that we can\n",
    "feed to our negative log likelihood. However, probabilities are a pair of numbers in a\n",
    "1D vector (one for airplane, one for bird), but here we’re still dealing with multichannel 2D features.\n",
    "\n",
    "Thinking back to the beginning of this chapter, we already know what we need to\n",
    "do: turn the 8-channel 8 × 8 image into a 1D vector and complete our network with a\n",
    "set of fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3,padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3,padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ...  Warning: Something important is missing here!\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code gives us a neural network as shown in figure 8.9.\n",
    "\n",
    "![](images/8.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the “something missing” comment for a minute. Let’s first notice that the size\n",
    "of the linear layer is dependent on the expected size of the output of `MaxPool2d`: $8 × 8\n",
    "× 8 = 512$. Let’s count the number of parameters for this small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s very reasonable for a limited dataset of such small images. In order to increase\n",
    "the capacity of the model, we could increase the number of output channels for the\n",
    "convolution layers (that is, the number of features each convolution layer generates),\n",
    "which would lead the linear layer to increase its size as well.\n",
    "\n",
    "We put the “Warning” note in the code for a reason. The model has zero chance of\n",
    "running without complaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9c784fd7714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, the error message is a bit obscure, but not too much so. We find references to `linear` in the traceback: looking back at the model, we see that only module\n",
    "that has to have a $512 × 32$ tensor is `nn.Linear(512, 32)` , the first linear module after\n",
    "the last convolution block.\n",
    "\n",
    "What’s missing there is the reshaping step from an 8-channel $8 × 8$ image to a $512-\n",
    "element$, 1D vector (1D if we ignore the batch dimension, that is). This could be\n",
    "achieved by calling view on the output of the last `nn.MaxPool2d` , but unfortunately, we\n",
    "don’t have any explicit visibility of the output of each module when we use\n",
    "`nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
