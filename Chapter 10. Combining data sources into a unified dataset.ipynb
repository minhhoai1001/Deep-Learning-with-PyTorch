{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10. Combining data sources into a unified dataset\n",
    "This chapter covers\n",
    "- Loading and processing raw data files\n",
    "- Implementing a Python class to represent our data\n",
    "- Converting our data into a format usable by PyTorch\n",
    "- Visualizing the training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to be able to produce a training sample given our inputs of raw CT\n",
    "scan data and a list of annotations for those CTs. This might sound simple, but\n",
    "quite a bit needs to happen before we can load, process, and extract the data we’re interested in. Figure 10.2 shows what we’ll need to do to turn our raw data into a training sample. Luckily, we got a head start on **understanding** our data in the last chapter,\n",
    "but we have more work to do on that front as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/9.1.png)\n",
    "\n",
    "![](images/9.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Raw CT data files\n",
    "Our CT data comes in two files: a **.mhd** file containing metadata header information,\n",
    "and a **.raw** file containing the raw bytes that make up the 3D array. Each file’s name starts\n",
    "with a unique identifier called the series UID (the name comes from the **Digital Imaging\n",
    "and Communications in Medicine** [DICOM] nomenclature) for the CT scan in question. For example, for series UID 1.2.3, there would be two files: 1.2.3.mhd and 1.2.3.raw.\n",
    "\n",
    "Our `Ct` class will consume those two files and produce the 3D array, as well as the\n",
    "transformation matrix to convert from the patient coordinate system (which we will\n",
    "discuss in more detail in section 10.6) to the index, row, column coordinates needed\n",
    "by the array (these coordinates are shown as (I,R,C) in the figures and are denoted\n",
    "with `_irc` variable suffixes in the code). Don’t sweat the details of all this right now;\n",
    "just remember that we’ve got some coordinate system conversion to do before we can\n",
    "apply these coordinates to our CT data. We’ll explore the details as we need them.\n",
    "We will also load the annotation data provided by LUNA, which will give us a list of\n",
    "nodule coordinates, each with a malignancy flag, along with the series UID of the relevant CT scan. By combining the nodule coordinate with coordinate system transformation information, we get the index, row, and column of the voxel at the center of\n",
    "our nodule.\n",
    "\n",
    "Using the (I,R,C) coordinates, we can crop a small 3D slice of our CT data to use as\n",
    "the input to our model. Along with this 3D sample array, we must construct the rest of\n",
    "our training sample tuple, which will have the sample array, nodule status flag, series\n",
    "UID, and the index of this sample in the CT list of nodule candidates. This sample\n",
    "tuple is exactly what PyTorch expects from our `Dataset` subclass and represents the\n",
    "last section of our bridge from our original raw data to the standard structure of\n",
    "PyTorch tensors.\n",
    "\n",
    "Limiting or cropping our data so as not to drown our model in noise is important,\n",
    "as is making sure we’re not so aggressive that our signal gets cropped out of our\n",
    "input. We want to make sure the range of our data is well behaved, especially after\n",
    "normalization. Clamping our data to remove outliers can be useful, especially if our\n",
    "data is prone to extreme outliers. We can also create handcrafted, algorithmic transformations of our input; this is known as **feature engineering**; and we discussed it briefly\n",
    "in chapter 1. We’ll usually want to let the model do most of the heavy lifting; feature\n",
    "engineering has its uses, but we won’t use it here in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Parsing LUNA’s annotation data\n",
    "The first thing we need to do is begin loading our data. When working on a new project, that’s often a good place to start. Making sure we know how to work with the raw\n",
    "input is required no matter what, and knowing how our data will look after it loads can help inform the structure of our early experiments. We could try loading individual CT scans, but we think it makes sense to parse the CSV files that LUNA provides,\n",
    "which contain information about the points of interest in each CT scan. As we can see\n",
    "in figure 10.3, we expect to get some coordinate information, an indication of\n",
    "whether the coordinate is a nodule, and a unique identifier for the CT scan. Since\n",
    "there are fewer types of information in the CSV files, and they’re easier to parse, we’re\n",
    "hoping they will give us some clues about what to look for once we start loading CTs. \n",
    "\n",
    "![](images/9.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `candidates.csv` file contains information about all lumps that potentially look like\n",
    "nodules, whether those lumps are malignant, benign tumors, or something else altogether. We’ll use this as the basis for building a complete list of candidates that can\n",
    "then be split into our training and validation datasets. The following Bash shell session shows what the file contains:\n",
    "\n",
    "```\n",
    "$ wc -l candidates.csv # Counts the number of lines in the file\n",
    "551066 candidates.csv\n",
    "\n",
    "$ head data/part2/luna/candidates.csv # Prints the first few lines of the file\n",
    "seriesuid,coordX,coordY,coordZ,class  # The first line of the .csv file defines the column headers.\n",
    "1.3...6860,-56.08,-67.85,-311.92,0\n",
    "1.3...6860,53.21,-244.41,-245.17,0\n",
    "1.3...6860,103.66,-121.8,-286.62,0\n",
    "1.3...6860,-33.66,-72.75,-308.41,0\n",
    "...\n",
    "\n",
    "$ grep ',1$' candidates.csv | wc -l # Counts the number of lines that end with 1, which indicates malignancy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** The values in the `seriesuid` column have been elided to better fit the\n",
    "printed page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 551,000 lines, each with a `seriesuid` (which we’ll call `series_uid` in the\n",
    "code), some (X,Y,Z) coordinates, and a class column that corresponds to the nodule\n",
    "status (it’s a Boolean value: 0 for a candidate that is not an actual nodule, and 1 for a\n",
    "candidate that is a nodule, either malignant or benign). We have 1,351 candidates\n",
    "flagged as actual nodules.\n",
    "\n",
    "The `annotations.csv` file contains information about some of the candidates that\n",
    "have been flagged as nodules. We are interested in the `diameter_mm` information in\n",
    "particular:\n",
    "\n",
    "```\n",
    "$ wc -l annotations.csv\n",
    "1187 annotations.csv # This is a different umber than in the umber than in the\n",
    "\n",
    "$ head data/part2/luna/annotations.csv\n",
    "seriesuid,coordX,coordY,coordZ,diameter_mm  # The last column is also different.\n",
    "1.3.6...6860,-128.6994211,-175.3192718,-298.3875064,5.651470635\n",
    "1.3.6...6860,103.7836509,-211.9251487,-227.12125,4.224708481\n",
    "1.3.6...5208,69.63901724,-140.9445859,876.3744957,5.786347814\n",
    "1.3.6...0405,-24.0138242,192.1024053,-391.0812764,8.143261683\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have size information for about 1,200 nodules. This is useful, since we can use it to\n",
    "make sure our training and validation data includes a representative spread of nodule\n",
    "sizes. Without this, it’s possible that our validation set could end up with only extreme\n",
    "values, making it seem as though our model is underperforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 Training and validation sets\n",
    "Let’s get back to our nodules. We’re going to sort them by size and take every Nth one for our validation set. That should give us the representative spread we’re looking  for. Unfortunately, the location information provided in `annotations.csv` doesn’t always precisely line up with the coordinates in `candidates.csv`:\n",
    "\n",
    "```\n",
    "$ grep 100225287222365663678666836860 annotations.csv\n",
    "1.3.6...6860,-128.6994211,-175.3192718,-298.3875064,5.651470635\n",
    "1.3.6...6860,103.7836509,-211.9251487,-227.12125,4.224708481\n",
    "\n",
    "$ grep '100225287222365663678666836860.*,1$' candidates.csv\n",
    "1.3.6...6860,104.16480444,-211.685591018,-227.011363746,1\n",
    "1.3.6...6860,-128.94,-175.04,-297.87,1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we truncate the corresponding coordinates from each file, we end up with (`–128.70,\n",
    "–175.32,–298.39`) versus (`–128.94,–175.04,–297.87`). Since the nodule in question has\n",
    "a diameter of 5 mm, both of these points are clearly meant to be the “center” of the\n",
    "nodule, but they don’t line up exactly. It would be a perfectly valid response to decide\n",
    "that dealing with this **data mismatch isn’t worth it**, and to **ignore the file**. We are going\n",
    "to do the legwork to make things line up, though, since real-world datasets are often\n",
    "imperfect this way, and this is a good example of the kind of work you will need to do\n",
    "to assemble data from disparate data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Unifying our annotation and candidate data\n",
    "Now that we know what our raw data files look like, let’s build a `getCandidateInfoList` function that will stitch it all together. We’ll use a named tuple that is defined at\n",
    "the top of the file to hold the information for each nodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.1 dsets.py:7\n",
    "```\n",
    "from collections import namedtuple\n",
    "# ... line 27\n",
    "CandidateInfoTuple = namedtuple(\n",
    "'CandidateInfoTuple',\n",
    "'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of candidate information will have the nodule status (what we’re going to be\n",
    "training the model to classify), diameter (useful for getting a good spread in training, since large and small nodules will not have the same features), series (to locate the\n",
    "correct CT scan), and candidate center (to find the candidate in the larger CT). The\n",
    "function that will build a list of these `NoduleInfoTuple` instances starts by using an inmemory caching decorator, followed by getting the list of files present on disk.\n",
    "\n",
    "```\n",
    "@functools.lru_cache(1)  # Standard library in memory caching\n",
    "def getCandidateInfoList(requireOnDisk_bool=True): # requireOnDisk_bool defaults to screening out series from data subsets \n",
    "                                                   # that aren’t in place yet\n",
    "    mhd_list = glob.glob('data-unversioned/part2/luna/subset*/*.mhd') \n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since parsing some of the data files can be slow, we’ll cache the results of this function call in memory. This will come in handy later, because we’ll be calling this function more often in future chapters. Speeding up our data pipeline by carefully applying **inmemory** or **on-disk caching** can result in some pretty impressive gains in training speed. Keep an eye out for these opportunities as you work on your projects.\n",
    "\n",
    "Earlier we said that we’ll support running our training program with less than the\n",
    "full set of training data, due to the long download times and high disk space requirements. The `requireOnDisk_bool` parameter is what makes good on that promise;\n",
    "we’re detecting which LUNA series UIDs are actually present and ready to be loaded\n",
    "from disk, and we’ll use that information to limit which entries we use from the CSV\n",
    "files we’re about to parse. Being able to run a subset of our data through the training\n",
    "loop can be useful to verify that the code is working as intended. Often a model’s\n",
    "training results are bad to useless when doing so, but exercising our logging, metrics,\n",
    "model check-pointing, and similar functionality is beneficial.\n",
    "\n",
    "After we get our candidate information, we want to merge in the diameter information from `annotations.csv`. First we need to group our annotations by `series_uid`,\n",
    "as that’s the first key we’ll use to cross-reference each row from the two files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 10.3 dsets.py:40, def getCandidateInfoList\n",
    "```\n",
    "candidateInfo_list = []\n",
    "with open('data/part2/luna/candidates.csv', \"r\") as f:\n",
    "    for row in list(csv.reader(f))[1:]:\n",
    "        series_uid = row[0]\n",
    "        if series_uid not in presentOnDisk_set and requireOnDisk_bool: # it’s in a subset we don’t have on disk so we should skip it.\n",
    "            continue\n",
    "        isNodule_bool = bool(int(row[4]))\n",
    "        candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "        candidateDiameter_mm = 0.0\n",
    "        for annotation_tup in diameter_dict.get(series_uid, []):\n",
    "            annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
    "            for i in range(3):\n",
    "                delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                if delta_mm > annotationDiameter_mm / 4:  # Divides the diameter by 2 to get the radius, and divides  \n",
    "                                                          # the radius by 2 to require that the two nodule center points\n",
    "                                                          # not be too far apart relative to the size of the nodule\n",
    "                break\n",
    "            else:\n",
    "                candidateDiameter_mm = annotationDiameter_mm\n",
    "            break\n",
    "        candidateInfo_list.append(CandidateInfoTuple(\n",
    "            isNodule_bool,\n",
    "            candidateDiameter_mm,\n",
    "            series_uid,\n",
    "            candidateCenter_xyz,\n",
    "            ))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the candidate entries for a given `series_uid`, we loop through the annotations we collected earlier for the same series_uid and see if the two coordinates are\n",
    "close enough to consider them the same nodule. If they are, great! Now we have diameter information for that nodule. If we don’t find a match, that’s fine; we’ll just treat\n",
    "the nodule as having a 0.0 diameter. Since we’re only using this information to get a\n",
    "good spread of nodule sizes in our training and validation sets, having incorrect diameter sizes for some nodules shouldn’t be a problem, but we should remember we’re\n",
    "doing this in case our assumption here is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.5 dsets.py:80, def getCandidateInfoList\n",
    "```\n",
    "# This means we have all of the actual nodule samples starting with the largest first, \n",
    "# followed by all of the non-nodule samples (which don’t have nodule size information).\n",
    "candidateInfo_list.sort(reverse=True)\n",
    "return candidateInfo_list\n",
    "```\n",
    "The ordering of the tuple members in `noduleInfo_list` is driven by this sort. We’re\n",
    "using this sorting approach to help ensure that when we take a slice of the data, that\n",
    "slice gets a representative chunk of the actual nodules with a good spread of nodule\n",
    "diameters. We’ll discuss this more in section 10.5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Loading individual CT scans\n",
    "We need to be able to take our `CT` data from a pile of bits on disk and turn it\n",
    "into a Python object from which we can extract 3D nodule density data. We can see this\n",
    "path from the `.mhd` and `.raw` files to `Ct` objects in figure 10.4. Our nodule annotation\n",
    "information acts like a map to the interesting parts of our raw data. Before we can follow\n",
    "that map to our data of interest, we need to get the data into an addressable form.\n",
    "\n",
    "![](images/10.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LUNA has converted the data we’re going to be using for this chapter into\n",
    "the MetaIO format, which is quite a bit easier to use (https://itk.org/Wiki/MetaIO/\n",
    "Documentation#Quick_Start). Don’t worry if you’ve never heard of the format\n",
    "before! We can treat the format of the data files as a black box and use `SimpleITK` to\n",
    "load them into more familiar `NumPy` arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.6 dsets.py:9\n",
    "```\n",
    "import SimpleITK as sitk\n",
    "# ... line 83\n",
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "        'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
    "        )[0]\n",
    "        # sitk.ReadImage implicitly consumes the .raw file in addition to the passed-in .mhd file.\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        # file in addition to the passed-in .mhd file. to convert the value type to np.float32\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For real projects, you’ll want to understand what types of information are contained in your raw data, but it’s perfectly fine to rely on third-party code like `SimpleITK` to parse the bits on disk. Finding the right balance of knowing everything about your inputs versus blindly accepting whatever your data-loading library hands you will probably take some experience. Just remember that we’re mostly concerned about **data**, not **bits**. It’s the information that matters, not how it’s represented.\n",
    "\n",
    "The 10 subsets we discussed earlier have about 90 CT scans each (888 in total),\n",
    "with every CT scan represented as two files: one with a `.mhd` extension and one with a\n",
    "`.raw` extension. The data being split between multiple files is hidden behind the `sitk`\n",
    "routines, however, and is not something we need to be directly concerned with.\n",
    "\n",
    "At this point, `ct_a` is a three-dimensional array. All three dimensions are spatial,\n",
    "and the single intensity channel is implicit. As we saw in chapter 4, in a PyTorch tensor, the channel information is represented as a fourth dimension with size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Hounsfield Units\n",
    "Recall that earlier, we said that we need to understand our data, not the bits that store\n",
    "it. Here, we have a perfect example of that in action. Without understanding the\n",
    "nuances of our data’s values and range, we’ll end up feeding values into our model\n",
    "that will hinder its ability to learn what we want it to.\n",
    "\n",
    "Continuing the `__init__` method, we need to do a bit of cleanup on the ct_a values. CT scan voxels are expressed in Hounsfield units (HU; https://en.wikipedia.org/\n",
    "wiki/Hounsfield_scale), which are odd units; air is –1,000 HU (close enough to 0 g/cc\n",
    "[grams per cubic centimeter] for our purposes), water is 0 HU (1 g/cc), and bone is\n",
    "at least +1,000 HU (2–3 g/cc).\n",
    "\n",
    "> **NOTE** HU values are typically stored on disk as signed 12-bit integers (shoved\n",
    "into 16-bit integers), which fits well with the level of precision CT scanners\n",
    "can provide. While this is perhaps interesting, it’s not particularly relevant to\n",
    "the project\n",
    "\n",
    "Some CT scanners use HU values that correspond to negative densities to indicate\n",
    "that those voxels are outside of the CT scanner’s field of view. For our purposes, everything outside of the patient should be air, so we discard that field-of-view information\n",
    "by setting a lower bound of the values to –1,000 HU. Similarly, the exact densities of\n",
    "bones, metal implants, and so on are not relevant to our use case, so we cap density at\n",
    "roughly 2 g/cc (1,000 HU) even though that’s not biologically accurate in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.7 dsets.py:96, Ct.__init__\n",
    "```\n",
    "ct_a.clip(-1000, 1000, ct_a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove all of these outlier values from our data: they aren’t directly relevant to our goal, and having those outliers can make the model’s job harder. This can\n",
    "happen in many ways, but a common example is when batch normalization is fed\n",
    "these outlier values and the statistics about how to best normalize the data are skewed.\n",
    "Always be on the lookout for ways to clean your data.\n",
    "\n",
    "All of the values we’ve built are now assigned to `self`.\n",
    "\n",
    "```\n",
    "self.series_uid = series_uid\n",
    "self.hu_a = ct_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Locating a nodule using the patient coordinate system\n",
    "Deep learning models typically need fixed-size inputs, due to having a fixed number\n",
    "of input neurons. We need to be able to produce a fixed-size array containing the candidate so that we can use it as input to our classifier. We’d like to train our model\n",
    "using a crop of the CT scan that has a candidate nicely centered, since then our\n",
    "model doesn’t have to learn how to notice nodules tucked away in the corner of the\n",
    "input. By reducing the variation in expected inputs, we make the model’s job easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.1 The patient coordinate system\n",
    "Unfortunately, all of the candidate center data we loaded in section 10.2 is expressed\n",
    "in millimeters, not voxels! We can’t just plug locations in millimeters into an array\n",
    "index and expect everything to work out the way we want. As we can see in figure 10.5,\n",
    "we need to transform our coordinates from the millimeter-based coordinate system\n",
    "(X,Y,Z) they’re expressed in, to the voxel-address-based coordinate system (I,R,C)\n",
    "used to take array slices from our CT scan data. This is a classic example of how it’s\n",
    "important to handle units consistently!\n",
    "\n",
    "![](images/10.5.png)\n",
    "\n",
    "As we have mentioned previously, when dealing with CT scans, we refer to the array\n",
    "dimensions as **index**, **row**, and **column**, because a separate meaning exists for X, Y, and Z, as illustrated in figure 10.6. The *patient coordinate system* defines positive X to be patientleft (left), positive Y to be patient-behind (*posterior*), and positive Z to be toward-patienthead (*superior*). Left-posterior-superior is sometimes abbreviated LPS.\n",
    "\n",
    "![](images/10.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patient coordinate system is measured in millimeters and has an arbitrarily positioned origin that does not correspond to the origin of the CT voxel array, as shown in\n",
    "figure 10.7.\n",
    "The patient coordinate system is often used to specify the locations of interesting\n",
    "anatomy in a way that is independent of any particular scan. The metadata that\n",
    "defines the relationship between the CT array and the patient coordinate system is\n",
    "stored in the header of DICOM files, and that meta-image format preserves the data\n",
    "in its header as well. This metadata allows us to construct the transformation from\n",
    "**(X,Y,Z)** to **(I,R,C)** that we saw in figure 10.5. The raw data contains many other fields\n",
    "of similar metadata, but since we don’t have a use for them right now, those unneeded\n",
    "fields will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.2 CT scan shape and voxel sizes\n",
    "One of the most common variations between CT scans is the size of the voxels; typically, they are not cubes. Instead, they can be 1.125 mm × 1.125 mm × 2.5 mm or similar. Usually the row and column dimensions have voxel sizes that are the same, and\n",
    "the index dimension has a larger value, but other ratios can exist.\n",
    "\n",
    "When plotted using square pixels, the non-cubic voxels can end up looking somewhat distorted, similar to the distortion near the north and south poles when using a\n",
    "Mercator projection map. That’s an imperfect analogy, since in this case the distortion\n",
    "is uniform and linear—the patient looks far more squat or barrel-chested in figure\n",
    "10.8 than they would in reality. We will need to apply a scaling factor if we want the\n",
    "images to depict realistic proportions.\n",
    "\n",
    "![](images/10.8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTs are commonly 512 rows by 512 columns, with the index dimension ranging from\n",
    "around 100 total slices up to perhaps 250 slices (250 slices times 2.5 millimeters is\n",
    "typically enough to contain the anatomical region of interest). This results in a lower\n",
    "bound of approximately 2 voxels, or about 32 million data points. Each CT specifies\n",
    "the voxel size in millimeters as part of the file metadata; for example, we’ll call `ct_mhd.GetSpacing()` in listing 10.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.3 Converting between millimeters and voxel addresses\n",
    "We will define some utility code to assist with the conversion between patient coordinates in millimeters (which we will denote in the code with an `_xyz` suffix on variables\n",
    "and the like) and (I,R,C) array coordinates (which we will denote in code with an\n",
    "`_irc` suffix).\n",
    "\n",
    "You might wonder whether the `SimpleITK` library comes with utility functions to\n",
    "convert these. And indeed, an Image instance does feature two methods—`TransformIndexToPhysicalPoint` and `TransformPhysicalPointToIndex`—to do just that\n",
    "(except shuffling from CRI [column,row,index] IRC). However, we want to be able to\n",
    "do this computation without keeping the Image object around, so we’ll perform the\n",
    "math manually here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping the axes (and potentially a rotation or other transforms) is encoded in a\n",
    "3 × 3 matrix returned as a tuple from `ct_mhd.GetDirections()`. To go from voxel\n",
    "indices to coordinates, we need to follow these four steps in order:\n",
    "\n",
    "1. Flip the coordinates from IRC to CRI, to align with XYZ.\n",
    "2. Scale the indices with the voxel sizes.\n",
    "3. Matrix-multiply with the directions matrix, using @ in Python.\n",
    "4. Add the offset for the origin.\n",
    "\n",
    "To go back from XYZ to IRC, we need to perform the inverse of each step in the\n",
    "reverse order.\n",
    "\n",
    "We keep the voxel sizes in named tuples, so we convert these into arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.9 util.py:16\n",
    "```\n",
    "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
    "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
    "\n",
    "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
    "    # Swaps the order while we convert to a NumPy array\n",
    "    cri_a = np.array(coord_irc)[::-1]\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    # The bottom three steps of our plan, all in one line\n",
    "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
    "    return XyzTuple(*coords_xyz)\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coord_a = np.array(coord_xyz)\n",
    "    # Inverse of the last three steps\n",
    "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
    "    # Sneaks in proper rounding before converting to integers\n",
    "    cri_a = np.round(cri_a)\n",
    "    # Shuffles and converts to integers\n",
    "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that was a bit heavy, don’t worry. Just remember that we need to convert and\n",
    "use the functions as a black box. The metadata we need to convert from patient coordinates (`_xyz`) to array coordinates (`_irc`) is contained in the MetaIO file alongside\n",
    "the CT data itself. We pull the voxel sizing and positioning metadata out of the `.mhd`\n",
    "file at the same time we get the `ct_a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.10 dsets.py:72, `class Ct`\n",
    "```\n",
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob('data\n",
    "        -unversioned/part2/luna/subset*/{}.mhd'.format(series_uid))[0]\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        # ... line 91\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        # Converts the directions to an array, and reshapes \n",
    "        # the nine-element array to its proper 3 × 3 matrix shape\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the inputs we need to pass into our `xyz2irc` conversion function, in addition\n",
    "to the individual point to covert. With these attributes, our CT object implementation now has all the data needed to convert a candidate center from patient coordinates to\n",
    "array coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.4 Extracting a nodule from a CT scan\n",
    "As we mentioned in chapter 9, up to 99.9999% of the voxels in a CT scan of a patient\n",
    "with a lung nodule won’t be part of the actual nodule (or cancer, for that matter).\n",
    "\n",
    "Instead, as we can see in figure 10.9, we will extract an area around each candidate\n",
    "and let the model focus on one candidate at a time. This is akin to letting you read\n",
    "individual paragraphs in that foreign language: still not an easy task, but far less\n",
    "daunting! Looking for ways to reduce the scope of the problem for our model can\n",
    "help, especially in the early stages of a project when we’re trying to get our first working implementation up and running.\n",
    "\n",
    "![](images/10.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `getRawNodule` function takes the center expressed in the patient coordinate system (X,Y,Z), just as it’s specified in the LUNA CSV data, as well as a width in voxels. It\n",
    "returns a cubic chunk of CT, as well as the center of the candidate converted to array\n",
    "coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.11 dsets.py:105, `Ct.getRawCandidate`\n",
    "```\n",
    "def getRawCandidate(self, center_xyz, width_irc):\n",
    "    center_irc = xyz2irc(\n",
    "        center_xyz,\n",
    "        self.origin_xyz,\n",
    "        self.vxSize_xyz,\n",
    "        self.direction_a,\n",
    "    )\n",
    "    \n",
    "    slice_list = []\n",
    "    for axis, center_val in enumerate(center_irc):\n",
    "        start_ndx = int(round(center_val - width_irc[axis]/2))\n",
    "        end_ndx = int(start_ndx + width_irc[axis])\n",
    "        slice_list.append(slice(start_ndx, end_ndx))\n",
    "        \n",
    "    ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "    \n",
    "    return ct_chunk, center_irc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 A straightforward dataset implementation\n",
    "By subclassing `Dataset`, we will take our arbitrary\n",
    "data and plug it into the rest of the PyTorch ecosystem. Each Ct instance represents\n",
    "hundreds of different samples that we can use to train our model or validate its effectiveness. Our `LunaDataset` class will normalize those samples, flattening each CT’s\n",
    "nodules into a single collection from which samples can be retrieved without regard\n",
    "for which Ct instance the sample originates from. This flattening is often how we want\n",
    "to process data, although as we’ll see in chapter 12, in some situations a simple flattening of the data isn’t enough to train a model well.\n",
    "\n",
    "PyTorch API only requires that\n",
    "any Dataset subclasses we want to implement must provide these two functions:\n",
    "\n",
    "- An implementation of `__len__` that must return a single, constant value after\n",
    "initialization (the value ends up being cached in some use cases)\n",
    "- The `__getitem__` method, which takes an index and returns a tuple with sample data to be used for training (or validation, as the case may be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.12 dsets.py:176, `LunaDataset.__len__`\n",
    "```\n",
    "def __len__(self):\n",
    "    return len(self.candidateInfo_list)\n",
    "\n",
    "def __getitem__(self, ndx):\n",
    "    # ... line 200\n",
    "    # This is our training sample.\n",
    "    return (\n",
    "        candidate_t, 1((CO10-1))\n",
    "        pos_t, 1((CO10-2))\n",
    "        candidateInfo_tup.series_uid,\n",
    "        torch.tensor(center_irc),\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `__len__` implementation is straightforward: we have a list of candidates, each candidate is a sample, and our dataset is as large as the number of samples we have. We don’t\n",
    "have to make the implementation as simple as it is here; in later chapters, we’ll see this\n",
    "change!4 The only rule is that if `__len__` returns a value of N, then `__getitem__` needs\n",
    "to return something valid for all inputs 0 to N – 1.\n",
    "\n",
    "For `__getitem__`, we take `ndx` (typically an integer, given the rule about supporting inputs 0 to N – 1) and return the four-item sample tuple as depicted in figure 10.2.\n",
    "Building this tuple is a bit more complicated than getting the length of our dataset,\n",
    "however, so let’s take a look.\n",
    "\n",
    "The first part of this method implies that we need to construct `self.candidateInfo_list` as well as provide the `getCtRawNodule` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.13 dsets.py:179, `LunaDataset.__getitem__`\n",
    "```\n",
    "def __getitem__(self, ndx):\n",
    "    candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "    width_irc = (32, 48, 48)\n",
    "    \n",
    "    # The return value candidate_a has shape (32,48,48); \n",
    "    # the axes are shape (32,48,48); the axes are\n",
    "    candidate_a, center_irc = getCtRawCandidate(\n",
    "        candidateInfo_tup.series_uid,\n",
    "        candidateInfo_tup.center_xyz,\n",
    "        width_irc,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to do in the `__getitem__` method is manipulate the data\n",
    "into the proper data types and required array dimensions that will be expected by\n",
    "downstream code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.14 dsets.py:189, `LunaDataset.__getitem__`\n",
    "```\n",
    "candidate_t = torch.from_numpy(candidate_a)\n",
    "candidate_t = candidate_t.to(torch.float32)\n",
    "# .unsqueeze(0) adds the ‘Channel’ dimension.\n",
    "candidate_t = candidate_t.unsqueeze(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t worry too much about exactly why we are manipulating dimensionality for now; the next chapter will contain the code that ends up consuming this output and imposing the constraints we’re proactively meeting here. This will be something you should expect for every custom Dataset you implement. These conversions are a key part of transforming your Wild West data into nice, orderly tensors. \n",
    "\n",
    "Finally, we need to build our classification tensor.\n",
    "\n",
    "#### Listing 10.15 dsets.py:193, `LunaDataset.__getitem__`\n",
    "```\n",
    "pos_t = torch.tensor([\n",
    "        not candidateInfo_tup.isNodule_bool,\n",
    "        candidateInfo_tup.isNodule_bool\n",
    "    ],\n",
    "    dtype=torch.long,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has two elements, one each for our possible candidate classes (nodule or nonnodule; or positive or negative, respectively). We could have a single output for the\n",
    "nodule status, but `nn.CrossEntropyLoss` expects one output value per class, so that’s\n",
    "what we provide here. The exact details of the tensors you construct will change based\n",
    "on the type of project you’re working on.\n",
    "\n",
    "Let’s take a look at our final sample tuple (the larger `nodule_t` output isn’t particularly readable, so we elide most of it in the listing).\n",
    "\n",
    "#### Listing 10.16 p2ch10_explore_data.ipynb\n",
    "```\n",
    "# In[10]:\n",
    "LunaDataset()[0]\n",
    "\n",
    "# Out[10]:\n",
    "(tensor([[[[-899., -903., -825., ..., -901., -898., -893.],\n",
    "    ...,\n",
    "    [ -92., -63., 4., ..., 63., 70., 52.]]]]),\n",
    "    tensor([0, 1]), # cls_t\n",
    "    '1.3.6...287966244644280690737019247886', # candidate_tup.series_uid (elided)\n",
    "    tensor([ 91, 360, 341])) # center_irc\n",
    "```\n",
    "\n",
    "Here we see the four items from our `__getitem__` return statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.1 Caching candidate arrays with the getCtRawCandidate function\n",
    "In order to get decent performance out of `LunaDataset`, we’ll need to invest in some\n",
    "on-disk caching. This will allow us to avoid having to read an entire CT scan from disk\n",
    "for every sample. Doing so would be prohibitively slow! Make sure you’re paying attention to bottlenecks in your project and doing what you can to optimize them once\n",
    "they start slowing you down. We’re kind of jumping the gun here since we haven’t\n",
    "demonstrated that we need caching here. Without caching, the `LunaDataset` is easily\n",
    "50 times slower! We’ll revisit this in the chapter’s exercises.\n",
    "\n",
    "The function itself is easy. It’s a file-cache-backed (https://pypi.python.org/pypi/\n",
    "diskcache) wrapper around the Ct.getRawCandidate method we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.17 dsets.py:139\n",
    "```\n",
    "@functools.lru_cache(1, typed=True)\n",
    "def getCt(series_uid):\n",
    "    return Ct(series_uid)\n",
    "@raw_cache.memoize(typed=True)\n",
    "\n",
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    return ct_chunk, center_irc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a few different caching methods here. First, we’re caching the getCt return\n",
    "value in memory so that we can repeatedly ask for the same `Ct` instance without having to reload all of the data from disk. That’s a huge speed increase in the case of\n",
    "repeated requests, but we’re only keeping one CT in memory, so cache misses will be\n",
    "frequent if we’re not careful about access order.\n",
    "\n",
    "The `getCtRawCandidate` function that calls `getCt` also has its outputs cached, however; so after our cache is populated, `getCt` won’t ever be called. These values are\n",
    "cached to disk using the Python library `diskcache`. We’ll discuss why we have this specific caching setup in chapter 11. For now, it’s enough to know that it’s much, much\n",
    "faster to read in 2 `float32` values from disk than it is to read in 2 `int16` values, convert to `float32`, and then select a 2 subset. From the second pass through the data\n",
    "forward, I/O times for input should drop to insignificance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.2 Constructing our dataset in `LunaDataset.__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just about every project will need to separate samples into a training set and a validation set. We are going to do that here by designating every tenth sample, specified by\n",
    "the `val_stride` parameter, as a member of the validation set. We will also accept an\n",
    "`isValSet_bool` parameter and use it to determine whether we should keep only the\n",
    "training data, the validation data, or everything.\n",
    "\n",
    "#### Listing 10.18 dsets.py:149, `class LunaDataset`\n",
    "```\n",
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "        val_stride=0,\n",
    "        isValSet_bool=None,\n",
    "        series_uid=None,\n",
    "        ):\n",
    "    # Copies the return value so the Copies the return value \n",
    "    # so the Copies the return value so the\n",
    "    self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
    "    \n",
    "    if series_uid:\n",
    "        self.candidateInfo_list = [\n",
    "        x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "    ]\n",
    "```\n",
    "\n",
    "If we pass in a truthy `series_uid`, then the instance will only have nodules from that\n",
    "series. This can be useful for visualization or debugging, by making it easier to look at,\n",
    "for instance, a single problematic CT scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.3 A training/validation split\n",
    "We allow for the `Dataset` to partition out 1/Nth of the data into a subset used for validating the model. How we will handle that subset is based on the value of the `isValSet_bool` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 10.19 dsets.py:162, `LunaDataset.__init__`\n",
    "```\n",
    "if isValSet_bool:\n",
    "    assert val_stride > 0, val_stride\n",
    "    self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "    assert self.candidateInfo_list\n",
    "elif val_stride > 0:\n",
    "    # Deletes the validation images (every val_stride-th item in the list) from\n",
    "    # self.candidateInfo_list. We made a self.candidateInfo_list. We made a the original list.\n",
    "    del self.candidateInfo_list[::val_stride]\n",
    "    assert self.candidateInfo_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can create two `Dataset` instances and be confident that there is strict\n",
    "segregation between our training data and our validation data. Of course, this\n",
    "depends on there being a consistent sorted order to `self.candidateInfo_list`,\n",
    "which we ensure by having there be a stable sorted order to the candidate info tuples,\n",
    "and by the `getCandidateInfoList` function sorting the list before returning it.\n",
    "\n",
    "The other caveat regarding separation of training and validation data is that,\n",
    "depending on the task at hand, we might need to ensure that data from a single\n",
    "patient is only present either in training or in testing but not both. Here this is not a\n",
    "problem; otherwise, we would have needed to split the list of patients and CT scans\n",
    "before going to the level of nodules.\n",
    "\n",
    "Let’s take a look at the data using `p2ch10_explore_data.ipynb`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# In[2]:\n",
    "from p2ch10.dsets import getCandidateInfoList, getCt, LunaDataset\n",
    "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
    "positiveInfo_list = [x for x in candidateInfo_list if x[0]]\n",
    "diameter_list = [x[1] for x in positiveInfo_list]\n",
    "\n",
    "# In[4]:\n",
    "for i in range(0, len(diameter_list), 100):\n",
    "    print('{:4} {:4.1f} mm'.format(i, diameter_list[i]))\n",
    "# Out[4]:\n",
    "0 32.3 mm\n",
    "100 17.7 mm\n",
    "200 13.0 mm\n",
    "300 10.0 mm\n",
    "400 8.2 mm\n",
    "500 7.0 mm\n",
    "600 6.3 mm\n",
    "700 5.7 mm\n",
    "800 5.1 mm\n",
    "900 4.7 mm\n",
    "1000 4.0 mm\n",
    "1100 0.0 mm\n",
    "1200 0.0 mm\n",
    "1300 0.0 mm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few very large candidates, starting at 32 mm, but they rapidly drop off to half that size. The bulk of the candidates are in the 4 to 10 mm range, and several hundred don’t have size information at all. This looks as expected; you might recall that we had more actual nodules than we had diameter annotations. Quick sanity checks on your data can be very helpful; catching a problem or mistaken assumption early may save hours of effort!\n",
    "\n",
    "The larger takeaway is that our training and validation splits should have a few\n",
    "properties in order to work well:\n",
    "- Both sets should include examples of all variations of expected inputs.\n",
    "- Neither set should have samples that aren’t representative of expected inputs\n",
    "unless they have a specific purpose like training the model to be robust to outliers.\n",
    "- The training set shouldn’t offer unfair hints about the validation set that\n",
    "wouldn’t be true for real-world data (for example, including the same sample in\n",
    "both sets; this is known as a leak in the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.4 Rendering the data\n",
    "Again, either use `p2ch10_explore_data.ipynb` directly or start Jupyter Notebook and\n",
    "enter\n",
    "\n",
    "```\n",
    "# In[7]:\n",
    "%matplotlib inline\n",
    "from p2ch10.vis import findNoduleSamples, showNodule\n",
    "noduleSample_list = findNoduleSamples()\n",
    "\n",
    "# In[8]:\n",
    "series_uid = positiveSample_list[11][2]\n",
    "showCandidate(series_uid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Conclusion\n",
    "In chapter 9, we got our heads wrapped around our data. In this chapter, we got\n",
    "PyTorch’s head wrapped around our data! By transforming our DICOM-via-meta-image\n",
    "raw data into tensors, we’ve set the stage to start implementing a model and a training\n",
    "loop, which we’ll see in the next chapter.\n",
    "\n",
    "It’s important not to underestimate the impact of the design decisions we’ve\n",
    "already made: the size of our inputs, the structure of our caching, and how we’re partitioning our training and validation sets will all make a difference to the success or failure of our overall project. Don’t hesitate to revisit these decisions later, especially\n",
    "once you’re working on your own projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Exercises\n",
    "1. Implement a program that iterates through a LunaDataset instance, and time how long it takes to do so. In the interest of time, it might make sense to have an option to limit the iterations to the first N=1000 samples. \\\n",
    "    a. How long does it take to run the first time? \\\n",
    "    b. How long does it take to run the second time? \\\n",
    "    c. What does clearing the cache do to the runtime? \\\n",
    "    d. What does using the last N=1000 samples do to the first/second runtime?\n",
    "2. Change the LunaDataset implementation to randomize the sample list during\n",
    "`__init__`. Clear the cache, and run the modified version. What does that do to\n",
    "the runtime of the first and second runs?\n",
    "3. Revert the randomization, and comment out the `@functools.lru_cache(1,typed=True)` decorator to getCt. Clear the cache, and run the modified\n",
    "version. How does the runtime change now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Summary\n",
    "- Often, the code required to parse and load raw data is nontrivial. For this project, we implement a `Ct` class that loads data from disk and provides access to\n",
    "cropped regions around points of interest.\n",
    "- Caching can be useful if the parsing and loading routines are expensive. Keep\n",
    "in mind that some caching can be done in memory, and some is best performed on disk. Each can have its place in a data-loading pipeline.\n",
    "- PyTorch Dataset subclasses are used to convert data from its native form into\n",
    "tensors suitable to pass in to the model. We can use this functionality to integrate our real-world data with PyTorch APIs.\n",
    "- Subclasses of `Dataset` need to provide implementations for two methods:\n",
    "`__len__` and `__getitem__`. Other helper methods are allowed but not required.\n",
    "- Splitting our data into a sensible training set and a validation set requires that\n",
    "we make sure no sample is in both sets. We accomplish this here by using a consistent sort order and taking every tenth sample for our validation set.\n",
    "- Data visualization is important; being able to investigate data visually can provide important clues about errors or problems. We are using Jupyter Notebooks\n",
    "and Matplotlib to render our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
