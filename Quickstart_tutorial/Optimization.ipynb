{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd095c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TỐI ƯU HÓA THÔNG SỐ MÔ HÌNH\n",
    "Bây giờ chúng tôi đã có mô hình và dữ liệu, đã đến lúc đào tạo, xác thực và kiểm tra mô hình của mình bằng cách tối ưu hóa các thông số của nó trên dữ liệu của chúng tôi. Đào tạo một mô hình là một quá trình lặp đi lặp lại; trong mỗi lần lặp (được gọi là một epoch), mô hình đưa ra phỏng đoán về kết quả đầu ra, tính toán lỗi trong dự đoán của nó (tổn thất), thu thập các đạo hàm của lỗi đối với các tham số của nó (như chúng ta đã thấy trong [phần trước](https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html)) và tối ưu hóa các tham số này bằng cách sử dụng gradient descent. Để có hướng dẫn chi tiết hơn về quy trình này, hãy xem video này về nhân giống ngược từ [3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Mã tiên quyết\n",
    "Chúng tôi tải mã từ các phần trước trên Datasets & DataLoaders và Build Model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "source": [
    "## Siêu tham số (Hyperparameters)\n",
    "Hyperparameters là các tham số có thể điều chỉnh cho phép bạn kiểm soát quá trình tối ưu hóa mô hình. Các giá trị siêu tham số khác nhau có thể ảnh hưởng đến việc đào tạo mô hình và tốc độ hội tụ (đọc thêm về điều chỉnh siêu tham số)\n",
    "\n",
    "Chúng tôi xác định các siêu tham số sau để đào tạo:\n",
    "\n",
    "- **Number of Epochs** - số lần lặp lại trên tập dữ liệu\n",
    "- **Batch Size** - số lượng mẫu dữ liệu được truyền qua mạng trước khi các tham số được cập nhật\n",
    "- **Learning Rate** - mức độ cập nhật các thông số mô hình tại batch/epoch. Giá trị nhỏ hơn mang lại tốc độ học tập chậm, trong khi giá trị lớn có thể dẫn đến hành vi không thể đoán trước trong quá trình đào tạo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "source": [
    "## Vòng lặp tối ưu hóa\n",
    "Khi chúng tôi đặt siêu tham số, chúng tôi có thể đào tạo và tối ưu hóa mô hình của mình bằng một vòng lặp tối ưu hóa. Mỗi lần lặp của vòng lặp tối ưu hóa được gọi là một epoch.\n",
    "\n",
    "Mỗi epoch bao gồm hai phần chính:\n",
    "- The Train Loop - lặp qua tập dữ liệu huấn luyện và cố gắng hội tụ đến các tham số tối ưu.\n",
    "- The Validation/Test Loop - lặp lại tập dữ liệu thử nghiệm để kiểm tra xem hiệu suất của mô hình có được cải thiện hay không.\n",
    "\n",
    "Chúng ta hãy làm quen với một số khái niệm được sử dụng trong vòng đào tạo. Hãy tiếp tục để xem Triển khai đầy đủ của vòng lặp tối ưu hóa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss Function\n",
    "Khi được trình bày với một số dữ liệu đào tạo, mạng chưa được đào tạo của chúng tôi có thể không đưa ra câu trả lời chính xác. Hàm tổn thất đo lường mức độ không giống nhau của kết quả thu được với giá trị mục tiêu và đó là hàm tổn thất mà chúng tôi muốn giảm thiểu trong quá trình đào tạo. Để tính toán tổn thất, chúng tôi đưa ra dự đoán bằng cách sử dụng đầu vào của mẫu dữ liệu đã cho của chúng tôi và so sánh nó với giá trị nhãn dữ liệu thực.\n",
    "\n",
    "Các hàm mất mát phổ biến bao gồm `nn.MSELoss` (Mean Square Error) cho các tác vụ hồi quy và `nn.NLLLoss` (Negative Log Likelihood) để phân loại. `nn.CrossEntropyLoss` kết hợp `nn.LogSoftmax` và `nn.NLLLoss`.\n",
    "\n",
    "Chúng tôi chuyển nhật ký đầu ra của mô hình của chúng tôi tới `nn.CrossEntropyLoss`, sẽ chuẩn hóa nhật ký và tính toán lỗi dự đoán."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "source": [
    "## Optimizer (Trình tối ưu hóa)\n",
    "Tối ưu hóa là quá trình điều chỉnh các tham số của mô hình để giảm lỗi mô hình trong mỗi bước huấn luyện. Các thuật toán tối ưu hóa xác định cách quá trình này được thực hiện (trong ví dụ này, chúng tôi sử dụng Stochastic Gradient Descent). Tất cả logic tối ưu hóa được gói gọn trong đối tượng trình `optimizer`. Ở đây, chúng tôi sử dụng trình tối ưu hóa SGD; Ngoài ra, có nhiều trình tối ưu hóa khác nhau có sẵn trong PyTorch như ADAM và RMSProp, hoạt động tốt hơn cho các loại mô hình và dữ liệu khác nhau.\n",
    "\n",
    "Chúng tôi khởi chạy trình tối ưu hóa bằng cách đăng ký các tham số của mô hình cần được đào tạo và chuyển vào siêu thông số tốc độ học."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "Bên trong vòng lặp đào tạo, việc tối ưu hóa diễn ra theo ba bước:\n",
    "- Gọi `optimizer.zero_grad()` để đặt lại gradient của các thông số mô hình. Gradients theo mặc định cộng lên; để ngăn việc đếm hai lần, chúng tôi rõ ràng bằng không chúng ở mỗi lần lặp.\n",
    "- Đảo ngược sự mất mát dự đoán bằng lời gọi `loss.backwards ()`. PyTorch ghi nhận mức độ lỗ w.r.t. mỗi tham số.\n",
    "- Khi chúng ta đã có các gradient của mình, chúng ta gọi `Optimizer.step()` để điều chỉnh các tham số theo các gradient được thu thập trong đường chuyền ngược."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Full Implementation\n",
    "Chúng tôi xác định `train_loop` lặp qua mã tối ưu hóa của chúng tôi và `test_loop` đánh giá hiệu suất của mô hình dựa trên dữ liệu thử nghiệm của chúng tôi."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "source": [
    "Chúng tôi khởi tạo hàm mất mát và trình tối ưu hóa, và chuyển nó vào `train_loop` và `test_loop`. Vui lòng tăng số lượng kỷ nguyên để theo dõi hiệu suất đang cải thiện của mô hình."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298615  [    0/60000]\n",
      "loss: 2.289773  [ 6400/60000]\n",
      "loss: 2.286231  [12800/60000]\n",
      "loss: 2.297304  [19200/60000]\n",
      "loss: 2.272071  [25600/60000]\n",
      "loss: 2.272357  [32000/60000]\n",
      "loss: 2.273503  [38400/60000]\n",
      "loss: 2.256310  [44800/60000]\n",
      "loss: 2.260671  [51200/60000]\n",
      "loss: 2.268083  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 0.035359 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.229062  [    0/60000]\n",
      "loss: 2.226611  [ 6400/60000]\n",
      "loss: 2.211932  [12800/60000]\n",
      "loss: 2.266988  [19200/60000]\n",
      "loss: 2.197991  [25600/60000]\n",
      "loss: 2.183466  [32000/60000]\n",
      "loss: 2.204887  [38400/60000]\n",
      "loss: 2.151039  [44800/60000]\n",
      "loss: 2.185158  [51200/60000]\n",
      "loss: 2.209997  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 0.034137 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.120913  [    0/60000]\n",
      "loss: 2.132128  [ 6400/60000]\n",
      "loss: 2.098882  [12800/60000]\n",
      "loss: 2.221612  [19200/60000]\n",
      "loss: 2.081829  [25600/60000]\n",
      "loss: 2.053265  [32000/60000]\n",
      "loss: 2.110592  [38400/60000]\n",
      "loss: 2.007039  [44800/60000]\n",
      "loss: 2.090524  [51200/60000]\n",
      "loss: 2.141356  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 0.032590 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.981597  [    0/60000]\n",
      "loss: 2.011241  [ 6400/60000]\n",
      "loss: 1.960606  [12800/60000]\n",
      "loss: 2.168586  [19200/60000]\n",
      "loss: 1.958167  [25600/60000]\n",
      "loss: 1.920905  [32000/60000]\n",
      "loss: 2.018664  [38400/60000]\n",
      "loss: 1.866194  [44800/60000]\n",
      "loss: 1.991749  [51200/60000]\n",
      "loss: 2.086291  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 0.031181 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.843351  [    0/60000]\n",
      "loss: 1.903609  [ 6400/60000]\n",
      "loss: 1.835470  [12800/60000]\n",
      "loss: 2.121151  [19200/60000]\n",
      "loss: 1.855919  [25600/60000]\n",
      "loss: 1.818761  [32000/60000]\n",
      "loss: 1.935893  [38400/60000]\n",
      "loss: 1.755337  [44800/60000]\n",
      "loss: 1.894397  [51200/60000]\n",
      "loss: 2.044186  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 0.029975 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.721439  [    0/60000]\n",
      "loss: 1.812006  [ 6400/60000]\n",
      "loss: 1.735356  [12800/60000]\n",
      "loss: 2.080289  [19200/60000]\n",
      "loss: 1.772928  [25600/60000]\n",
      "loss: 1.746134  [32000/60000]\n",
      "loss: 1.871559  [38400/60000]\n",
      "loss: 1.676470  [44800/60000]\n",
      "loss: 1.817940  [51200/60000]\n",
      "loss: 1.992873  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 0.028720 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.620778  [    0/60000]\n",
      "loss: 1.729661  [ 6400/60000]\n",
      "loss: 1.623239  [12800/60000]\n",
      "loss: 2.000310  [19200/60000]\n",
      "loss: 1.682303  [25600/60000]\n",
      "loss: 1.566586  [32000/60000]\n",
      "loss: 1.832251  [38400/60000]\n",
      "loss: 1.566649  [44800/60000]\n",
      "loss: 1.671846  [51200/60000]\n",
      "loss: 1.844288  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 0.026975 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.507124  [    0/60000]\n",
      "loss: 1.644305  [ 6400/60000]\n",
      "loss: 1.515357  [12800/60000]\n",
      "loss: 1.945119  [19200/60000]\n",
      "loss: 1.612287  [25600/60000]\n",
      "loss: 1.470961  [32000/60000]\n",
      "loss: 1.788875  [38400/60000]\n",
      "loss: 1.495527  [44800/60000]\n",
      "loss: 1.596291  [51200/60000]\n",
      "loss: 1.786154  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 0.025961 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.429223  [    0/60000]\n",
      "loss: 1.579597  [ 6400/60000]\n",
      "loss: 1.439455  [12800/60000]\n",
      "loss: 1.904731  [19200/60000]\n",
      "loss: 1.558903  [25600/60000]\n",
      "loss: 1.411497  [32000/60000]\n",
      "loss: 1.756108  [38400/60000]\n",
      "loss: 1.445326  [44800/60000]\n",
      "loss: 1.543032  [51200/60000]\n",
      "loss: 1.745509  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss: 0.025207 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.373255  [    0/60000]\n",
      "loss: 1.533212  [ 6400/60000]\n",
      "loss: 1.380516  [12800/60000]\n",
      "loss: 1.869853  [19200/60000]\n",
      "loss: 1.520396  [25600/60000]\n",
      "loss: 1.368519  [32000/60000]\n",
      "loss: 1.730166  [38400/60000]\n",
      "loss: 1.407560  [44800/60000]\n",
      "loss: 1.501865  [51200/60000]\n",
      "loss: 1.713453  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 0.024607 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "## Further Reading\n",
    "- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- [`torch.optim`](https://pytorch.org/docs/stable/optim.html)\n",
    "- [Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}