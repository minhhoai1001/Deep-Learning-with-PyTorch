{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH `TORCH.AUTOGRAD`\n",
    "Khi đào tạo mạng nơ-ron, thuật toán thường được sử dụng nhất là lan truyền ngược. Trong thuật toán này, các tham số (trọng số mô hình) được điều chỉnh theo độ dốc của hàm mất mát đối với tham số đã cho.\n",
    "\n",
    "Để tính toán các độ dốc đó, PyTorch có một công cụ phân biệt tích hợp có tên là torch.autograd. Nó hỗ trợ tính toán tự động gradient cho bất kỳ đồ thị tính toán nào.\n",
    "\n",
    "Hãy xem xét mạng nơ-ron một lớp đơn giản nhất, với đầu vào `x`, các tham số `w` và `b`, và một số hàm mất mát. Nó có thể được định nghĩa trong PyTorch theo cách sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor, chức năng và đồ thị tính toán \n",
    "Đoạn mã này xác định đồ thị tính toán sau:\n",
    "![](comp-graph.png)\n",
    "\n",
    "Trong mạng này, `w` và `b` là các tham số mà chúng ta cần tối ưu hóa. Do đó, chúng ta cần có khả năng tính toán các bậc của hàm mất mát liên quan đến các biến đó. Để làm điều đó, chúng tôi đặt thuộc tính `requi_grad` của các tensor đó.\n",
    "\n",
    ">**Note** \\\n",
    "Bạn có thể đặt giá trị của `required_grad` khi tạo tensor hoặc sau này bằng cách sử dụng phương thức `x.requires_grad_(True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một hàm mà chúng ta áp dụng cho tensors để xây dựng đồ thị tính toán trên thực tế là một đối tượng của lớp `Function`. Đối tượng này biết cách tính hàm theo chiều thuận, và cũng biết cách tính đạo hàm của nó trong bước truyền ngược. Tham chiếu đến hàm truyền ngược được lưu trữ trong thuộc tính `grad_fn` của tensor. Bạn có thể tìm thêm thông tin của `Function` trong [tài liệu](https://pytorch.org/docs/stable/autograd.html#function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x00000255404E4BE0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x00000255404E4970>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tính Gradients\n",
    "Để tối ưu hóa trọng số của các tham số trong mạng nơ-ron, chúng ta cần tính toán các dẫn xuất của hàm mất mát của chúng ta đối với các tham số, cụ thể là chúng ta cần $\\frac{\\partial loss}{\\partial w} $ và $\\frac{\\partial loss}{\\partial b}$ dưới một số giá trị cố định của `x` và `y`. Để tính toán các dẫn xuất đó, chúng tôi gọi `loss.backward()`, sau đó truy xuất các giá trị từ `w.grad` và `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0325, 0.0007, 0.0164],\n",
      "        [0.0325, 0.0007, 0.0164],\n",
      "        [0.0325, 0.0007, 0.0164],\n",
      "        [0.0325, 0.0007, 0.0164],\n",
      "        [0.0325, 0.0007, 0.0164]])\n",
      "tensor([0.0325, 0.0007, 0.0164])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tắt theo dõi Gradient\n",
    "Theo mặc định, tất cả các tensor có `requi_grad = True` đang theo dõi lịch sử tính toán của chúng và hỗ trợ tính toán gradient. Tuy nhiên, có một số trường hợp chúng ta không cần phải làm điều đó, ví dụ như khi chúng ta đã đào tạo mô hình và chỉ muốn áp dụng nó cho một số dữ liệu đầu vào, tức là chúng ta chỉ muốn thực hiện các phép tính chuyển tiếp thông qua mạng. Chúng tôi có thể ngừng theo dõi các phép tính bằng cách bao quanh mã tính toán của chúng tôi với khối `torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một cách khác để đạt được kết quả tương tự là sử dụng phương thức `detach()` trên tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có những lý do bạn có thể muốn tắt theo dõi gradient:\n",
    "- Để đánh dấu một số tham số trong mạng nơ-ron của bạn tại các tham số được đóng băng (**frozen parameters**). Đây là một tình huống rất phổ biến để tinh chỉnh một mạng được đào tạo trước\n",
    "- Để tăng tốc độ tính toán khi bạn chỉ thực hiện chuyển tiếp, bởi vì tính toán trên tensors không theo dõi gradient sẽ hiệu quả hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thông tin thêm về Đồ thị tính toán\n",
    "Về mặt khái niệm, autograd lưu giữ bản ghi dữ liệu (tensors) và tất cả các hoạt động đã thực thi (cùng với các tensor mới thu được) trong một đồ thị xoay chiều có hướng (**directed acyclic graph-DAG**) bao gồm các đối tượng [Funtion](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function). Trong DAG này, lá là tensor đầu vào, rễ là tensor đầu ra. Bằng cách theo dõi biểu đồ này từ gốc đến lá, bạn có thể tự động tính toán độ dốc bằng cách sử dụng quy tắc chuỗi.\n",
    "\n",
    "Trong một lần chuyển tiếp, autograd thực hiện đồng thời hai việc:\n",
    "\n",
    "- Chạy hoạt động được yêu cầu để tính toán kết quả một tensor.\n",
    "- Duy trì chức năng gradient của hoạt động trong DAG. \n",
    "\n",
    "Đường lùi bắt đầu khi `.backward()` được gọi trên gốc DAG `.autograd` sau đó:\n",
    "\n",
    "- Tính toán các gradient từ mỗi `.grad_fn`,\n",
    "- Tích lũy chúng trong thuộc tính tensor’s `.grad` tương ứng\n",
    "- Sử dụng quy tắc chuỗi, lan truyền tất cả các cách để tensor lá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note** \\\n",
    "Các DAG rất linh hoạt trong PyTorch Một điều quan trọng cần lưu ý là biểu đồ được tạo lại từ đầu; sau mỗi lần gọi `.backward()`, autograd bắt đầu điền vào một biểu đồ mới. Đây chính xác là những gì cho phép bạn sử dụng các câu lệnh luồng điều khiển trong mô hình của mình; bạn có thể thay đổi hình dạng, kích thước và hoạt động ở mỗi lần lặp nếu cần."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc tùy chọn: Tensor Gradients và các sản phẩm Jacobian\n",
    "Trong nhiều trường hợp, chúng ta có một hàm mất mát vô hướng và chúng ta cần tính toán gradient đối với một số tham số. Tuy nhiên, có những trường hợp khi hàm đầu ra là một tensor tùy ý. Trong trường hợp này, PyTorch cho phép bạn tính cái gọi là **sản phẩm Jacobian**, chứ không phải gradient thực tế.\n",
    "\n",
    "Đối với một hàm vectơ $\\vec{y}=f(\\vec{x})$ với $\\vec{x}=(x_1, ..., x_n)$ và $\\vec{y} = (y_1, ..., y_n)$ một gradient của $\\vec{y}$ đối với $\\vec{x}$ được cung cấp bởi ma trận Jacobian:\\\n",
    "![](Jacobian_matrix.png)\n",
    "\n",
    "Thay vì tính toán ma trận Jacobian, PyTorch cho phép bạn tính toán Sản phẩm Jacobian $v^T.J$ cho một vectơ đầu vào nhất định $v = (v_1, ..., v_m)$. Điều này đạt được bằng cách gọi `backward` với $v$ làm đối số. Kích thước của $v$ phải giống với kích thước của tensor ban đầu, đối với mà chúng tôi muốn tính toán sản phẩm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng khi chúng ta gọi `backward` lần thứ hai với cùng một đối số, giá trị của gradient sẽ khác. Điều này xảy ra bởi vì khi thực hiện truyền ngược, PyTorch **tích lũy các gradient**, tức là giá trị của các gradient được tính toán được thêm vào thuộc tính grad của tất cả các nút lá của đồ thị tính toán. Nếu bạn muốn tính toán các gradient thích hợp, bạn cần phải loại bỏ thuộc tính `grad` trước đó. Trong quá trình đào tạo thực tế, một trình tối ưu hóa sẽ giúp chúng tôi thực hiện điều này.\n",
    "\n",
    ">**Note**\\\n",
    "Trước đây chúng ta đã gọi hàm `backward()` mà không có tham số. Điều này về cơ bản tương đương với việc gọi `backward(torch.tensor(1.0))`, đây là một cách hữu ích để tính toán độ dốc trong trường hợp hàm có giá trị vô hướng, chẳng hạn như mất mát trong quá trình đào tạo mạng nơ-ron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readin\n",
    "- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
