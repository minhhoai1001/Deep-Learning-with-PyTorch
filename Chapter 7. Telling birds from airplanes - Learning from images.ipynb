{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Telling birds from airplanes: Learning from images\n",
    "This chapter covers\n",
    "* Building a feed-forward neural network\n",
    "* Loading data using Dataset s and DataLoader s\n",
    "* Understanding classification loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we’ll keep moving ahead with building our neural network foundations. This time, we’ll turn our attention to images. Image recognition is arguably the task that made the world realize the potential of deep learning.\n",
    "\n",
    "We will approach a simple image recognition problem step by step, building from\n",
    "a simple neural network like the one we defined in the last chapter. This time, instead\n",
    "of a tiny dataset of numbers, we’ll use a more extensive dataset of tiny images. Let’s\n",
    "download the dataset first and get to work preparing it for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 A dataset of tiny images\n",
    "There is nothing like an intuitive understanding of a subject, and there is nothing to\n",
    "achieve that like working on simple data. One of the most basic datasets for image\n",
    "recognition is the handwritten digit-recognition dataset known as **MNIST**. Here\n",
    "we will use another dataset that is similarly simple and a bit more fun. It’s called\n",
    "CIFAR-10, and, like its sibling **CIFAR-100**, it has been a computer vision classic for\n",
    "a decade.\n",
    "\n",
    "CIFAR-10 consists of 60,000 tiny 32 × 32 color (RGB) images, labeled with an inte-\n",
    "ger corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2), cat (3),\n",
    "deer (4), dog (5), frog (6), horse (7), ship (8), and truck (9). Nowadays, **CIFAR-10** is\n",
    "considered too simple for developing or validating new research, but it serves our\n",
    "learning purposes just fine. We will use the `torchvision` module to automatically\n",
    "download the dataset and load it as a collection of PyTorch tensors. Figure 7.1 gives us\n",
    "a taste of **CIFAR-10**.\n",
    "\n",
    "![](images/7.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 Downloading CIFAR-10\n",
    "As we anticipated, let’s import `torchvision` and use the `datasets` module to download the CIFAR-10 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "data_path = './data-unversioned/p1ch7/'\n",
    "# Instantiates a dataset for the training data; \n",
    "# TorchVision downloads the data if it is not present.\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "# With train=False, this gets us a dataset for the validation data, again downloading as necessary.\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument we provide to the `CIFAR10` function is the location from which the\n",
    "data will be downloaded; the second specifies whether we’re interested in the training\n",
    "set or the validation set; and the third says whether we allow PyTorch to download the\n",
    "data if it is not found in the location specified in the first argument.\n",
    "\n",
    "Just like `CIFAR10` , the `datasets` submodule gives us precanned access to the most\n",
    "popular computer vision datasets, such as MNIST, Fashion-MNIST, CIFAR-100,\n",
    "SVHN, Coco, and Omniglot. In each case, the dataset is returned as a subclass of\n",
    "`torch.utils.data.Dataset` . We can see that the method-resolution order of our\n",
    "`cifar10` instance includes it as a base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " typing.Generic,\n",
       " object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 The Dataset class\n",
    "It’s a good time to discover what being a subclass of torch.utils.data.Dataset\n",
    "means in practice. Looking at figure 7.2, we see what PyTorch Dataset is all about. It\n",
    "is an object that is required to implement two methods: `__len__` and `__getitem__` .\n",
    "The former should return the number of items in the dataset; the latter should return\n",
    "the item, consisting of a sample and its corresponding label (an integer index).\n",
    "\n",
    "In practice, when a Python object is equipped with the `__len__` method, we can\n",
    "pass it as an argument to the len Python built-in function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/7.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, since the dataset is equipped with the `__getitem__` method, we can use the\n",
    "standard subscript for indexing tuples and lists to access individual items. Here, we get\n",
    "a **PIL** (Python Imaging Library, the PIL package) image with our desired output—an\n",
    "integer with the value 1 , corresponding to “automobile”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F56A6AFC070>, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the sample in the `data.CIFAR10` dataset is an instance of an RGB PIL image. We\n",
    "can plot it right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Dataset transforms\n",
    "That’s all very nice, but we’ll likely need a way to convert the PIL image to a PyTorch\n",
    "tensor before we can do anything with it. That’s where `torchvision.transforms`\n",
    "comes in. This module defines a set of composable, function-like objects that can be\n",
    "passed as an argument to a `torchvision` dataset such as `datasets.CIFAR10(...)` , and\n",
    "that perform transformations on the data after it is loaded but before it is returned by\n",
    "`__getitem__` . We can see the list of available objects as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSizedCrop',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'Scale',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'functional',\n",
       " 'functional_pil',\n",
       " 'functional_tensor',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among those transforms, we can spot `ToTensor` , which turns NumPy arrays and PIL\n",
    "images to tensors. It also takes care to lay out the dimensions of the output tensor as\n",
    "C × H × W (channel, height, width; just as we covered in chapter 4).\n",
    "\n",
    "Let’s try out the `ToTensor` transform. Once instantiated, it can be called like a\n",
    "function with the PIL image as the argument, returning a tensor as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we anticipated, we can pass the transform directly as an argument to `dataset.CIFAR10` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, accessing an element of the dataset will return a tensor, rather than a\n",
    "PIL image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the shape has the channel as the first dimension, while the scalar type is `float32` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per channel), the `ToTensor` transform turns the data into a 32-bit floating-point per channel,\n",
    "scaling the values down from 0.0 to 1.0. Let’s verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let’s verify that we’re getting the same image out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_t.permute(1,2,0)) #Changes the order of the axes from C × H × W to H × W × C\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It checks. Note how we have to use `permute` to change the order of the axes from\n",
    "C × H × W to H × W × C to match what Matplotlib expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 Normalizing data\n",
    "Transforms are really handy because we can chain them using transforms.Compose ,\n",
    "and they can handle normalization and data augmentation transparently, directly in\n",
    "the data loader. For instance, it’s good practice to normalize the dataset so that each\n",
    "channel has zero mean and unitary standard deviation. We mentioned this in chapter\n",
    "4, but now, after going through chapter 5, we also have an intuition for why: by choosing\n",
    "activation functions that are linear around 0 plus or minus 1 (or 2), keeping the data\n",
    "in the same range means it’s more likely that neurons have nonzero gradients and, hence, will learn sooner. Also, normalizing each channel so that it has the same\n",
    "distribution will ensure that channel information can be mixed and updated through\n",
    "gradient descent using the same learning rate. This is just like the situation in section\n",
    "5.4.4 when we rescaled the weight to be of the same magnitude as the bias in our\n",
    "temperature-conversion model.\n",
    "\n",
    "In order to make it so that each channel has zero mean and unitary standard devi-\n",
    "ation, we can compute the mean value and the standard deviation of each channel\n",
    "across the dataset and apply the following transform: `v_n[c] = (v[c] - mean[c]) / stdev[c]` . This is what `transforms.Normalize` does. The values of `mean` and `stdev`\n",
    "must be computed offline (they are not computed by the transform). Let’s compute\n",
    "them for the CIFAR-10 training set.\n",
    "\n",
    "Since the CIFAR-10 dataset is small, we’ll be able to manipulate it entirely in memory. Let’s stack all the tensors returned by the dataset along an extra dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily compute the mean per channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).mean(dim=1)\n",
    "# Recall that view(3, -1) keeps the three channels and\n",
    "# merges all the remaining dimensions into one, figuring\n",
    "# out the appropriate size. Here our 3 × 32 × 32 image is\n",
    "# transformed into a 3 × 1,024 vector, and then the mean\n",
    "# is taken over the 1,024 elements of each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these numbers in our hands, we can initialize the `Normalize` transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the standard deviation is similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these numbers in our hands, we can initialize the `Normalize` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and concatenate it after the `ToTensor` transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, at this point, plotting an image drawn from the dataset won’t provide us\n",
    "with a faithful representation of the actual image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_t, _ = cifar10[99]\n",
    "\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Distinguishing birds from airplanes\n",
    "Jane, our friend at the bird-watching club, has set up a fleet of cameras in the woods\n",
    "south of the airport. The cameras are supposed to save a shot when something enters\n",
    "the frame and upload it to the club’s real-time bird-watching blog. The problem is\n",
    "that a lot of planes coming and going from the airport end up triggering the camera, so Jane spends a lot of time deleting pictures of airplanes from the blog. What she\n",
    "needs is an automated system like that shown in figure 7.6. Instead of manually deleting, she needs a neural network—an AI if we’re into fancy marketing speak—to throw\n",
    "away the airplanes right away.\n",
    "\n",
    "![](images/7.3.png)\n",
    "\n",
    "No worries! We’ll take care of that, no problem—we just got the perfect dataset for\n",
    "it (what a coincidence, right?). We’ll pick out all the birds and airplanes from our\n",
    "CIFAR-10 dataset and build a neural network that can tell birds and airplanes apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Building the dataset\n",
    "The first step is to get the data in the right shape. We could create a `Dataset` subclass\n",
    "that only includes birds and airplanes. However, the dataset is small, and we only need\n",
    "indexing and `len` to work on our dataset. It doesn’t actually have to be a subclass of\n",
    "`torch.utils.data.dataset.Dataset` ! Well, why not take a shortcut and just filter the\n",
    "data in `cifar10` and remap the labels so they are contiguous? Here’s how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [(img, label_map[label])\n",
    "    for img, label in cifar10\n",
    "    if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "    for img, label in cifar10_val\n",
    "    if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cifar2` object satisfies the basic requirements for a `Dataset` —that is, `__len__` and `__getitem__` are defined—so we’re going to use that. We should be aware, however,\n",
    "that this is a clever shortcut and we might wish to implement a proper `Dataset` if we\n",
    "hit limitations with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 A fully connected model\n",
    "We learned how to build a neural network in chapter 5. We know that it’s a tensor of\n",
    "features in, a tensor of features out. After all, an image is just a set of numbers laid out\n",
    "in a spatial configuration. OK, we don’t know how to handle the spatial configuration\n",
    "part just yet, but in theory if we just take the image pixels and straighten them into a\n",
    "long 1D vector, we could consider those numbers as input features, right? This is what\n",
    "figure 7.7 illustrates.\n",
    "\n",
    "![](images/7.4.png)\n",
    "\n",
    "How many features per sample? Well, 32 × 32 × 3: that is, 3,072 input\n",
    "features per sample. Starting from the model we built in chapter 5, our new model\n",
    "would be an `nn.Linear` with 3,072 input features and some number of hidden features, followed by an activation, and then another `nn.Linear` that tapers the network down to\n",
    "an appropriate output number of features (2, for this use case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        3072,  # Input features\n",
    "        512,   # hidden layer size\n",
    "    ),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(\n",
    "        512,   # Hidden layer size\n",
    "        n_out, # Output classes\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We somewhat arbitrarily pick 512 hidden features. A neural network needs at least\n",
    "one hidden layer (of activations, so two modules) with a nonlinearity in between in\n",
    "order to be able to learn arbitrary functions in the way we discussed in section 6.3—\n",
    "otherwise, it would just be a linear model. The hidden features represent (learned)\n",
    "relations between the inputs encoded through the weight matrix. As such, the model\n",
    "might learn to “compare” vector elements 176 and 208, but it does not a priori focus\n",
    "on them because it is structurally unaware that these are, indeed (row 5, pixel 16) and\n",
    "(row 6, pixel 16), and thus adjacent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Output of a classifier\n",
    "In chapter 6, the network produced the predicted temperature (a number with a quantitative meaning) as output. We could do something similar here: make our network output a single scalar value (so n_out = 1 ), cast the labels to floats (0.0 for airplane and 1.0 for bird), and use those as a target for MSELoss (the average of squared differences in the batch). Doing so, we would cast the problem into a regression problem. However, looking more closely, we are now dealing with something a bit different in nature.\n",
    "\n",
    "We need to recognize that the output is categorical: it’s either a bird or an airplane (or something else if we had all 10 of the original classes). As we learned in\n",
    "chapter 4, when we have to represent a categorical variable, we should switch to a\n",
    "one-hot-encoding representation of that variable, such as [1, 0] for airplane or [0, 1] for bird (the order is arbitrary). This will still work if we have 10 classes, as in the full\n",
    "CIFAR-10 dataset; we’ll just have a vector of length 10.\n",
    "\n",
    "In the ideal case, the network would output torch.tensor([1.0, 0.0]) for an airplane and torch.tensor([0.0, 1.0]) for a bird. Practically speaking, since our classifier will not be perfect, we can expect the network to output something in between. The key realization in this case is that we can interpret our output as probabilities: the first entry is the probability of “airplane,” and the second is the probability of “bird.”\n",
    "\n",
    "Casting the problem in terms of probabilities imposes a few extra constraints on\n",
    "the outputs of our network:\n",
    "* Each element of the output must be in the [0.0, 1.0] range (a probability of an outcome cannot be less than 0 or greater than 1).\n",
    "* The elements of the output must add up to 1.0 (we’re certain that one of the two outcomes will occur).\n",
    "\n",
    "It sounds like a tough constraint to enforce in a differentiable way on a vector of numbers. Yet there’s a very smart trick that does exactly that, and it’s differentiable: it’s called `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Representing the output as probabilities\n",
    "Softmax is a function that takes a vector of values and produces another vector of the\n",
    "same dimension, where the values satisfy the constraints we just listed to represent\n",
    "probabilities. The expression for softmax is shown in figure 7.8.\n",
    "\n",
    "That is, we take the elements of the vector, compute the elementwise exponential,\n",
    "and divide each element by the sum of exponentials. In code, it’s something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test it on an input vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/7.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it satisfies the constraints on probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is a monotone function, in that lower values in the input will correspond to lower values in the output. However, it’s not scale invariant, in that the ratio between values is not preserved. In fact, the ratio between the first and second elements of the\n",
    "input is 0.5, while the ratio between the same elements in the output is 0.3678. This is not a real issue, since the learning process will drive the parameters of the model in a way that values have appropriate ratios.\n",
    "\n",
    "The `nn` module makes softmax available as a module. Since, as usual, input tensors\n",
    "may have an additional batch 0th dimension, or have dimensions along which they\n",
    "encode probabilities and others in which they don’t, `nn.Softmax` requires us to specify\n",
    "the dimension along which the softmax function is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have two input vectors in two rows (just like when we work with\n",
    "batches), so we initialize `nn.Softmax` to operate along dimension 1.\n",
    "Excellent! We can now add a softmax at the end of our model, and our network\n",
    "will be equipped to produce probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually try running the model before even training it. Let’s do it, just to see\n",
    "what comes out. We first build a batch of one image, our bird (figure 7.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, hello there. In order to call the model, we need to make the input have the right\n",
    "dimensions. We recall that our model expects 3,072 features in the input, and that nn\n",
    "works with data organized into batches along the zeroth dimension. So we need to\n",
    "turn our 3 × 32 × 32 image into a 1D tensor and then add an extra dimension in the\n",
    "zeroth position. We learned how to do this in chapter 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to invoke our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4507, 0.5493]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got probabilities! Well, we know we shouldn’t get too excited: the weights and\n",
    "biases of our linear layers have not been trained at all. Their elements are initialized\n",
    "randomly by PyTorch between –1.0 and 1.0. Interestingly, we also see `grad_fn` for the\n",
    "output, which is the tip of the backward computation graph (it will be used as soon as\n",
    "we need to backpropagate).\n",
    "\n",
    "In addition, while we know which output probability is supposed to be which\n",
    "(recall our `class_names` ), our network has no indication of that. Is the first entry “air-\n",
    "plane” and the second “bird,” or the other way around? The network can’t even tell\n",
    "that at this point. It’s the loss function that associates a meaning with these two num-\n",
    "bers, after backpropagation. If the labels are provided as index 0 for “airplane” and\n",
    "index 1 for “bird,” then that’s the order the outputs will be induced to take. Thus,\n",
    "after training, we will be able to get the label as an index by computing the argmax of\n",
    "the output probabilities: that is, the index at which we get the maximum probability.\n",
    "Conveniently, when supplied with a dimension, `torch.max` returns the maximum ele-\n",
    "ment along that dimension as well as the index at which that value occurs. In our case,\n",
    "we need to take the max along the probability vector (not across batches), therefore,\n",
    "dimension 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 A loss for classifying\n",
    "We just mentioned that the loss is what gives probabilities meaning. In chapters 5 and\n",
    "6, we used mean square error (MSE) as our loss. We could still use MSE and make our\n",
    "output probabilities converge to `[0.0, 1.0]` and `[1.0, 0.0]` . However, thinking about\n",
    "it, we’re not really interested in reproducing these values exactly. Looking back at the\n",
    "argmax operation we used to extract the index of the predicted class, what we’re really\n",
    "interested in is that the first probability is higher than the second for airplanes and vice versa for birds. In other words, we want to penalize misclassifications rather than painstakingly penalize everything that doesn’t look exactly like a 0.0 or 1.0.\n",
    "\n",
    "What we need to maximize in this case is the probability associated with the correct class, `out[class_index]` , where out is the output of softmax and `class_index` is a vector containing 0 for “airplane” and 1 for “bird” for each sample. This quantity—that is, the probability associated with the correct class—is referred to as the **likelihood** (of our model’s parameters, given the data). In other words, we want a loss function that is very high when the likelihood is low: so low that the alternatives have a higher probability. Conversely, the loss should be low when the likelihood is higher than the alternatives, and we’re not really fixated on driving the probability up to 1.\n",
    "\n",
    "There’s a loss function that behaves that way, and it’s called **negative log likelihood**\n",
    "(**NLL**). It has the expression `NLL = - sum(log(out_i[c_i]))` , where the sum is taken\n",
    "over N samples and `c_i` is the correct class for sample `i`. Let’s take a look at figure 7.10, which shows the NLL as a function of predicted probability.\n",
    "\n",
    "![](images/7.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that when low probabilities are assigned to the data, the NLL grows to infinity, whereas it decreases at a rather shallow rate when probabilities are greater than 0.5. Remember that the NLL takes probabilities as input; so, as the likelihood grows, the other probabilities will necessarily decrease.\n",
    "\n",
    "Summing up, our loss for classification can be computed as follows. For each sam-\n",
    "ple in the batch:\n",
    "1. Run the forward pass, and obtain the output values from the last (linear) layer.\n",
    "2. Compute their softmax, and obtain probabilities.\n",
    "3. Take the predicted probability corresponding to the correct class (the likelihood of the parameters). Note that we know what the correct class is because it’s a supervised problem—it’s our ground truth.\n",
    "4. Compute its logarithm, slap a minus sign in front of it, and add it to the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we do this in PyTorch? PyTorch has an `nn.NLLLoss` class. However (gotcha\n",
    "ahead), as opposed to what you might expect, it does not take probabilities but rather\n",
    "takes a tensor of log probabilities as input. It then computes the NLL of our model\n",
    "given the batch of data. There’s a good reason behind the input convention: taking\n",
    "the logarithm of a probability is tricky when the probability gets close to zero. The\n",
    "workaround is to use `nn.LogSoftmax` instead of `nn.Softmax` , which takes care to make\n",
    "the calculation numerically stable.\n",
    "\n",
    "We can now modify our model to use `nn.LogSoftmax` as the output module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we instantiate our NLL loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss takes the output of `nn.LogSoftmax` for a batch as the first argument and a\n",
    "tensor of class indices (zeros and ones, in our case) as the second argument. We can\n",
    "now test it with our birdie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9774, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ending our investigation of losses, we can look at how using cross-entropy loss\n",
    "improves over MSE. In figure 7.11, we see that the cross-entropy loss has some slope when the prediction is off target (in the low-loss corner, the correct class is assigned a\n",
    "predicted probability of 99.97%), while the MSE we dismissed at the beginning saturates much earlier and—crucially—also for very wrong predictions. The underlying\n",
    "reason is that the slope of the MSE is too low to compensate for the flatness of the softmax function for wrong predictions. This is why the MSE for probabilities is not a\n",
    "good fit for classification work.\n",
    "\n",
    "![](images/7.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 Training the classifier\n",
    "We’re ready to bring back the training loop we wrote in chapter 5 and see\n",
    "how it trains (the process is illustrated in figure 7.12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoai_tran/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0c53e6b4b568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcifar2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Prints the loss for the last image. In the next chapter, we will improve our output to give an average over the entire epoch.        \n",
    "print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/7.8.png)\n",
    "\n",
    "Looking more closely, we made a small change to the training loop. In chapter 5, we\n",
    "had just one loop: over the epochs (recall that an epoch ends when all samples in the\n",
    "training set have been evaluated). We figured that evaluating all 10,000 images in a\n",
    "single batch would be too much, so we decided to have an inner loop where we evalu-\n",
    "ate one sample at a time and backpropagate over that single sample.\n",
    "\n",
    "By shuffling samples at each epoch and estimating the gradient on one or (preferably, for stability) a few samples at a time, we are effectively introducing randomness in our gradient descent. Remember **SGD**? It stands for stochastic gradient descent, and this is what the S is about: working on small batches (aka minibatches) of shuffled data.\n",
    "\n",
    "Typically, minibatches are a constant size that we need to set prior to training, just\n",
    "like the learning rate. These are called hyperparameters, to distinguish them from the\n",
    "parameters of a model.\n",
    "\n",
    "![](images/7.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training code, we chose minibatches of size 1 by picking one item at a time from\n",
    "the dataset. The `torch.utils.data module` has a class that helps with shuffling and\n",
    "organizing the data in minibatches: `DataLoader` . The job of a data loader is to sample\n",
    "minibatches from a dataset, giving us the flexibility to choose from different sampling\n",
    "strategies. A very common strategy is uniform sampling after shuffling the data at each\n",
    "epoch. Figure 7.14 shows the data loader shuffling the indices it gets from the `Dataset`.\n",
    "\n",
    "![](images/7.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how this is done. At a minimum, the `DataLoader` constructor takes a `Dataset`\n",
    "object as input, along with `batch_size` and a `shuffle` Boolean that indicates whether\n",
    "the data needs to be shuffled at the beginning of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataLoader` can be iterated over, so we can use it directly in the inner loop of our\n",
    "new training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\\\n",
    "from torch import optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "    # Due to the shuffling, this now prints the loss for a random batch—clearly something we want to improve in chapter 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each inner iteration, imgs is a tensor of size `64 × 3 × 32 × 32`—that is, a minibatch of `64 (32 × 32)` RGB images—while labels is a tensor of size 64 containing label indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreases somehow, but we have no idea whether it’s low enough. Since our goal here is to correctly assign classes to images, and preferably do that on an independent dataset, we can compute the accuracy of our model on the validation set in terms of the number of correct classifications over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a great performance, but quite a lot better than random. In our defense, our\n",
    "model was quite a shallow classifier; it’s a miracle that it worked at all. It did because\n",
    "our dataset is really simple—a lot of the samples in the two classes likely have system-\n",
    "atic differences (such as the color of the background) that help the model tell birds\n",
    "from airplanes, based on a few pixels.\n",
    "\n",
    "We can certainly add some bling to our model by including more layers, which will\n",
    "increase the model’s depth and capacity. One rather arbitrary possibility is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are trying to taper the number of features more gently toward the output, in\n",
    "the hope that intermediate layers will do a better job of squeezing information in\n",
    "increasingly shorter intermediate outputs.\n",
    "\n",
    "The combination of `nn.LogSoftmax` and `nn.NLLLoss` is equivalent to using `nn.CrossEntropyLoss` . This terminology is a particularity of PyTorch, as the\n",
    "nn.NLLoss computes, in fact, the cross entropy but with log probability predictions as inputs where `nn.CrossEntropyLoss` takes scores (sometimes called logits). Technically, `nn.NLLLoss` is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs.\n",
    "\n",
    "It is quite common to drop the last `nn.LogSoftmax` layer from the network and use\n",
    "`nn.CrossEntropyLoss` as a loss. Let us try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numbers will be exactly the same as with `nn.LogSoftmax` and `nn.NLLLoss` . It’s just more convenient to do it all in one pass, with the only gotcha being that the output of our model will not be interpretable as probabilities (or log probabilities). We’ll need to explicitly pass the output through a softmax to obtain those.\n",
    "\n",
    "Training this model and evaluating the accuracy on the validation set (0.802000)\n",
    "lets us appreciate that a larger model bought us an increase in accuracy, but not that\n",
    "much. The accuracy on the training set is practically perfect (0.998100). What is this\n",
    "telling us? That we are overfitting our model in both cases. Our fully connected\n",
    "model is finding a way to discriminate birds and airplanes on the training set by mem-\n",
    "orizing the training set, but performance on the validation set is not all that great,\n",
    "even if we choose a larger model.\n",
    "\n",
    "PyTorch offers a quick way to determine how many parameters a model has\n",
    "through the `parameters()` method of `nn.Model` (the same method we use to provide\n",
    "the parameters to the optimizer). To find out how many elements are in each tensor\n",
    "instance, we can call the `numel` method. Summing those gives us our total count.\n",
    "Depending on our use case, counting parameters might require us to check whether a\n",
    "parameter has `requires_grad` set to `True` , as well. We might want to differentiate the\n",
    "number of trainable parameters from the overall model size. Let’s take a look at what\n",
    "we have right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connected_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f3fd247a8900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m numel_list = [p.numel()\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconnected_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m             if p.requires_grad == True]\n\u001b[1;32m      4\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connected_model' is not defined"
     ]
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "            for p in model.parameters()\n",
    "            if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 3.7 million parameters! Not a small network for such a small input image, is it?\n",
    "Even our first network was pretty large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters in our first model is roughly half that in our latest model.\n",
    "Well, from the list of individual parameter sizes, we start having an idea what’s\n",
    "responsible: the first module, which has 1.5 million parameters. In our full network,\n",
    "we had 1,024 output features, which led the first linear module to have 3 million\n",
    "parameters. This shouldn’t be unexpected: we know that a linear layer computes `y =\n",
    "weight * x + bias` , and if `x` has length 3,072 (disregarding the batch dimension for\n",
    "simplicity) and `y` must have length 1,024, then the `weight` tensor needs to be of size\n",
    "1,024 × 3,072 and the bias size must be 1,024. And 1,024 * 3,072 + 1,024 = 3,146,752,\n",
    "as we found earlier. We can verify these quantities directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this telling us? That our neural network won’t scale very well with the number\n",
    "of pixels. What if we had a 1,024 × 1,024 RGB image? That’s 3.1 million input values.\n",
    "Even abruptly going to 1,024 hidden features (which is not going to work for our clas-\n",
    "sifier), we would have over **3 billion parameters**. Using 32-bit floats, we’re already at 12 GB of RAM, and we haven’t even hit the second layer, much less computed and stored\n",
    "the gradients. That’s just not going to fit on most present-day GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 The limits of going fully connected\n",
    "\n",
    "Let’s reason about what using a linear module on a 1D view of our image entails—figure\n",
    "7.15 shows what is going on. It’s like taking every single input value—that is, every single\n",
    "component in our RGB image—and computing a linear combination of it with all the\n",
    "other values for every output feature. On one hand, we are allowing for the combina-\n",
    "tion of any pixel with every other pixel in the image being potentially relevant for our\n",
    "task. On the other hand, we aren’t utilizing the relative position of neighboring or far-\n",
    "away pixels, since we are treating the image as one big vector of numbers.\n",
    "\n",
    "![](images/7.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An airplane flying in the sky captured in a 32 × 32 image will be very roughly similar to\n",
    "a dark, cross-like shape on a blue background. A fully connected network as in figure\n",
    "7.15 would need to learn that when pixel 0,1 is dark, pixel 1,1 is also dark, and so on,\n",
    "that’s a good indication of an airplane. This is illustrated in the top half of figure 7.16.\n",
    "However, shift the same airplane by one pixel or more as in the bottom half of the fig-\n",
    "ure, and the relationships between pixels will have to be relearned from scratch: this\n",
    "time, an airplane is likely when pixel 0,2 is dark, pixel 1,2 is dark, and so on. In more\n",
    "technical terms, a fully connected network is not **translation invariant**. This means a\n",
    "network that has been trained to recognize a Spitfire starting at position 4,4 will not\n",
    "be able to recognize the exact same Spitfire starting at position 8,8. We would then have\n",
    "to **augment** the dataset—that is, apply random translations to images during training—\n",
    "so the network would have a chance to see Spitfires all over the image, and we would\n",
    "need to do this for every image in the dataset (for the record, we could concatenate a transform from `torchvision.transforms` to do this transparently). However, this data\n",
    "augmentation strategy comes at a cost: the number of hidden features—that is, of\n",
    "parameters—must be large enough to store the information about all of these translated replicas.\n",
    "\n",
    "![](images/7.12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at the end of this chapter, we have a dataset, a model, and a training loop, and\n",
    "our model learns. However, due to a mismatch between our problem and our network\n",
    "structure, we end up overfitting our training data, rather than learning the general-\n",
    "ized features of what we want the model to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Conclusion\n",
    "In this chapter, we have solved a simple classification problem from dataset, to model,\n",
    "to minimizing an appropriate loss in a training loop. All of these things will be stan-\n",
    "dard tools for your PyTorch toolbelt, and the skills needed to use them will be useful\n",
    "throughout your PyTorch tenure.\n",
    "\n",
    "We’ve also found a severe shortcoming of our model: we have been treating 2D\n",
    "images as 1D data. Also, we do not have a natural way to incorporate the translation\n",
    "invariance of our problem. In the next chapter, you’ll learn how to exploit the 2D\n",
    "nature of image data to get much better results.\n",
    "\n",
    "We could use what we have learned right away to process data without this translation\n",
    "invariance. For example, using it on tabular data or the time-series data we met in chap-\n",
    "ter 4, we can probably do great things already. To some extent, it would also be possible\n",
    "to use it on text data that is appropriately represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Exercises\n",
    "1. Use torchvision to implement random cropping of the data.\n",
    "    * How are the resulting images different from the uncropped originals?\n",
    "    * What happens when you request the same image a second time?\n",
    "    * What is the result of training using randomly cropped images?\n",
    "2. Switch loss functions (perhaps MSE).\n",
    "    * Does the training behavior change?\n",
    "3. Is it possible to reduce the capacity of the network enough that it stops overfitting?\n",
    "    * How does the model perform on the validation set when doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Summary\n",
    "* Computer vision is one of the most extensive applications of deep learning.\n",
    "* Several datasets of annotated images are publicly available; many of them can be accessed via `torchvision` .\n",
    "* `Datasets` and `DataLoaders` provide a simple yet effective abstraction for loading and sampling datasets.\n",
    "* For a classification task, using the softmax function on the output of a network produces values that satisfy the requirements for being interpreted as probabilities. The ideal loss function for classification in this case is obtained by using the output of softmax as the input of a non-negative log likelihood function. The combination of softmax and such loss is called cross entropy in PyTorch.\n",
    "* Nothing prevents us from treating images as vectors of pixel values, dealing with them using a fully connected network, just like any other numerical data. However, doing so makes it much harder to take advantage of the spatial relationships in the data.\n",
    "* Simple models can be created using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
