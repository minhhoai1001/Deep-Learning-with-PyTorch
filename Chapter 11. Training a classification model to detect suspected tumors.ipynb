{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training a classification model to detect suspected tumors\n",
    "This chapter covers\n",
    "- Using PyTorch DataLoaders to load data\n",
    "- Implementing a model that performs classification on our CT data\n",
    "- Setting up the basic skeleton for our application\n",
    "- Logging and displaying metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 A foundational model and training loop\n",
    "We’ll start by building the nodule\n",
    "classification model and training loop that will be the foundation that the rest of part 2\n",
    "uses to explore the larger project. To do that, we’ll use the `Ct` and `LunaDataset` classes\n",
    "we implemented in chapter 10 to feed `DataLoader` instances. Those instances, in turn,\n",
    "will feed our classification model with data via training and validation loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we’ll work on producing a model capable of performing step 4: **classification**. As a reminder, we will classify candidates as **nodules** or **non-nodules** (we’ll build\n",
    "another classifier to attempt to tell malignant nodules from benign ones in chapter\n",
    "14). That means we’re going to assign a single, specific label to each sample that we\n",
    "present to the model. In this case, those labels are “nodule” and “non-nodule,” since\n",
    "each sample represents a single candidate.\n",
    "\n",
    "![](images/11.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/11.2.png)\n",
    "\n",
    "The basic structure of what we’re going to implement is as follows:\n",
    "- Initialize our model and data loading.\n",
    "- Loop over a semi-arbitrarily chosen number of epochs.\n",
    "    - Loop over each batch of training data returned by LunaDataset.\n",
    "    - The data-loader worker process loads the relevant batch of data in the background.\n",
    "    - Pass the batch into our classification model to get results.\n",
    "    - Calculate our loss based on the difference between our predicted results and our ground-truth data.\n",
    "    - Record metrics about our model’s performance into a temporary data structure.\n",
    "    - Update the model weights via backpropagation of the error.\n",
    "    - The basic structure of what we’re going to implement is as follows:\n",
    "    - Initialize our model and data loading.\n",
    "    - Loop over a semi-arbitrarily chosen number of epochs.\n",
    "    - Loop over each batch of training data returned by LunaDataset.\n",
    "    - The data-loader worker process loads the relevant batch of data in the background.\n",
    "    - Pass the batch into our classification model to get results.\n",
    "    - Calculate our loss based on the difference between our predicted results and our ground-truth data.\n",
    "    - Record metrics about our model’s performance into a temporary data structure.\n",
    "    - Update the model weights via backpropagation of the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 The main entry point for our application\n",
    "One of the big structural differences from earlier training work we’ve done in this\n",
    "book is that part 2 wraps our work in a fully fledged command-line application. It will parse command-line arguments, have a full-featured --help command, and be easy to run in a wide variety of environments. All this will allow us to easily invoke the training routines from both Jupyter and a Bash shell.\n",
    "\n",
    "Our application’s functionality will be implemented via a class so that we can\n",
    "instantiate the application and pass it around if we feel the need. This can make testing, debugging, or invocation from other Python programs easier. We can invoke the\n",
    "application without needing to spin up a second OS-level process (we won’t do\n",
    "explicit unit testing in this book, but the structure we create can be helpful for real\n",
    "projects where that kind of testing is appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 11.1 code/p2_run_everything.ipynb\n",
    "\n",
    "```\n",
    "# In[2]:w\n",
    "def run(app, *argv):\n",
    "    argv = list(argv)\n",
    "    argv.insert(0, '--num-workers=4') # We assume you have a four-core, \n",
    "                                      # eight thread CPU. Change the 4 if needed.\n",
    "    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
    "    app_cls = importstr(*app.rsplit('.', 1)) # This is a slightly cleaner call to __import__.\n",
    "    app_cls(argv).main()\n",
    "    log.info(\"Finished: {}.{!r}).main()\".format(app, argv))\n",
    "    \n",
    "# In[6]:\n",
    "run('p2ch11.training.LunaTrainingApp', '--epochs=1')\n",
    "```\n",
    "> **NOTE** The training here assumes that you’re on a workstation that has a fourcore, eight-thread CPU, 16 GB of RAM, and a GPU with 8 GB of RAM. Reduce\n",
    "`--batch-size` if your GPU has less RAM, and `--num-workers` if you have fewer\n",
    "CPU cores, or less CPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get some semistandard boilerplate code out of the way. We’ll start at the end of\n",
    "the file with a pretty standard `if main` stanza that instantiates the application object\n",
    "and invokes the `main` method.\n",
    "\n",
    "#### Listing 11.2 training.py:386\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "    LunaTrainingApp().main()\n",
    "```\n",
    "\n",
    "From there, we can jump back to the top of the file and have a look at the application class\n",
    "and the two functions we just called, `__init__` and `main`. We’ll want to be able to accept\n",
    "command-line arguments, so we’ll use the standard `argparse` library (https://docs\n",
    ".python.org/3/library/argparse.html) in the application’s `__init__` function. Note that\n",
    "we can pass in custom arguments to the initializer, should we wish to do so. The `main`\n",
    "method will be the primary entry point for the core logic of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 11.3 training.py:31, `class LunaTrainingApp`\n",
    "```\n",
    "class LunaTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "        # ... line 63\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "\n",
    "    # ... line 137\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "```\n",
    "\n",
    "This structure is pretty general and could be reused for future projects. In particular, parsing arguments in `__init__` allows us to configure the application separately from\n",
    "invoking it. \n",
    "\n",
    "If you check the code for this chapter on the book’s website or GitHub, you might notice some extra lines mentioning `TensorBoard`. Ignore those for now; we’ll discuss them in detail later in the chapter, in section 11.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Pretraining setup and initialization\n",
    "Before we can begin iterating over each batch in our epoch, some initialization work needs to happen. After all, we can’t train a model if we haven’t even instantiated one yet! We need to do two main things, as we can see in figure 11.3. The first, as we just mentioned, is to initialize our model and optimizer; and the second is to initialize our Dataset and DataLoader instances. LunaDataset will define the randomized set of samples that will make up our training epoch, and our DataLoader instance will perform the work of loading the data out of our dataset and providing it to our application.\n",
    "\n",
    "![](images/11.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1 Initializing the model and optimizer\n",
    "For this section, we are treating the details of `LunaModel` as a black box. In section 11.4,\n",
    "we will detail the internal workings. You are welcome to explore changes to the implementation to better meet our goals for the model, although that’s probably best done\n",
    "after finishing at least chapter 12.\n",
    "\n",
    "#### Listing 11.4 training.py:31, `class LunaTrainingApp`\n",
    "```\n",
    "class LunaTrainingApp:\n",
    "def __init__(self, sys_argv=None):\n",
    "    # ... line 70\n",
    "    self.use_cuda = torch.cuda.is_available()\n",
    "    self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "    self.model = self.initModel()\n",
    "    self.optimizer = self.initOptimizer()\n",
    "\n",
    "def initModel(self):\n",
    "    model = LunaModel()\n",
    "    if self.use_cuda:\n",
    "        log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model) # Wraps the model\n",
    "        model = model.to(self.device) # Wraps the model parameters to the GPU\n",
    "    return model\n",
    "\n",
    "def initOptimizer(self):\n",
    "    return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "```\n",
    "If the system used for training has more than one GPU, we will use the `nn.DataParallel`\n",
    "class to distribute the work between all of the GPUs in the system and then collect and\n",
    "resync parameter updates and so on. This is almost entirely transparent in terms of both\n",
    "the model implementation and the code that uses that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DataParallel vs. DistributedDataParallel** \\\n",
    "In this book, we use `DataParallel` to handle utilizing multiple GPUs. We chose `DataParallel` because it’s a simple drop-in wrapper around our existing models. It is not\n",
    "the best-performing solution for using multiple GPUs, however, and it is limited to working with the hardware available in a single machine.\\\n",
    "PyTorch also provides `DistributedDataParallel`, which is the recommended wrapper class to use when you need to spread work between more than one GPU or\n",
    "machine. Since the proper setup and configuration are nontrivial, and we suspect that\n",
    "the vast majority of our readers won’t see any benefit from the complexity, we won’t\n",
    "cover `DistributedDataParallel` in this book. If you wish to learn more, we suggest\n",
    "reading the official documentation: https://pytorch.org/tutorials/intermediate/\n",
    "ddp_tutorial.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that `self.use_cuda` is true, the call `self.model.to(device)` moves the\n",
    "model parameters to the GPU, setting up the various convolutions and other calculations to use the GPU for the heavy numerical lifting. It’s important to do so before\n",
    "constructing the optimizer, since, otherwise, the optimizer would be left looking at\n",
    "the CPU-based parameter objects rather than those copied to the GPU.\n",
    "\n",
    "For our optimizer, we’ll use basic stochastic gradient descent (SGD;\n",
    "https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) with momentum.\n",
    "We first saw this optimizer in chapter 5. Recall from part 1 that many different optimizers are available in PyTorch; while we won’t cover most of them in any detail, the\n",
    "official documentation (https://pytorch.org/docs/stable/optim.html#algorithms)\n",
    "does a good job of linking to the relevant papers.\n",
    "\n",
    "Using SGD is generally considered a safe place to start when it comes to picking an optimizer; there are some problems that might not work well with SGD, but they’re relatively rare. Similarly, a **learning rate of 0.001** and a **momentum of 0.9** are pretty\n",
    "safe choices. Empirically, SGD with those values has worked reasonably well for a wide range of projects, and it’s easy to try a learning rate of 0.01 or 0.0001 if things aren’t working well right out of the box.\n",
    "\n",
    "That’s not to say any of those values is the best for our use case, but trying to find better ones is getting ahead of ourselves. Systematically trying different values for learning\n",
    "rate, momentum, network size, and other similar configuration settings is called a **hyperparameter search**. There are other, more glaring issues we need to address first in the coming chapters. Once we address those, we can begin to fine-tune these values. As we\n",
    "mentioned in the section “Testing other optimizers” in chapter 5, there are also other,\n",
    "more exotic optimizers we might choose; but other than perhaps swapping\n",
    "`torch.optim.SGD` for `torch.optim.Adam`, understanding the trade-offs involved is a\n",
    "topic too advanced for this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2 Care and feeding of data loaders\n",
    "The LunaDataset class that we built in the last chapter acts as the bridge between\n",
    "whatever Wild West data we have and the somewhat more structured world of tensors\n",
    "that the PyTorch building blocks expect. For example, `torch.nn.Conv3d` (https://\n",
    "pytorch.org/docs/stable/nn.html#conv3d) expects five-dimensional input: (N, C, D,\n",
    "H, W): number of samples, channels per sample, depth, height, and width. Quite different from the native 3D our CT provides!\n",
    "\n",
    "You may recall the `ct_t.unsqueeze(0)` call in `LunaDataset.__getitem__` from the\n",
    "last chapter; it provides the fourth dimension, a “channel” for our data. Recall from\n",
    "chapter 4 that an RGB image has three channels, one each for red, green, and blue.\n",
    "Astronomical data could have dozens, one each for various slices of the electromagnetic spectrum—gamma rays, X-rays, ultraviolet light, visible light, infrared, microwaves, and/or radio waves. Since CT scans are single-intensity, our channel dimension\n",
    "is only size 1.\n",
    "\n",
    "Also recall from part 1 that training on single samples at a time is typically an inefficient use of computing resources, because most processing platforms are capable of\n",
    "more parallel calculations than are required by a model to process a single training or\n",
    "validation sample. The solution is to group sample tuples together into a batch tuple,\n",
    "as in figure 11.4, allowing multiple samples to be processed at the same time. The fifth\n",
    "dimension (N) differentiates multiple samples in the same batch.\n",
    "\n",
    "![](images/11.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, we don’t have to implement any of this batching: the PyTorch `DataLoader` class will handle all of the collation work for us. We’ve already built the bridge\n",
    "from the CT scans to PyTorch tensors with our `LunaDataset` class, so all that remains\n",
    "is to plug our dataset into a data loader.\n",
    "\n",
    "#### Listing 11.5 training.py:89, `LunaTrainingApp.initTrainDl`\n",
    "```\n",
    "def initTrainDl(self):\n",
    "    train_ds = LunaDataset(  # Our custom dataset\n",
    "        val_stride=10,\n",
    "        isValSet_bool=False,\n",
    "    )\n",
    "    \n",
    "    batch_size = self.cli_args.batch_size\n",
    "    if self.use_cuda:\n",
    "        batch_size *= torch.cuda.device_count()\n",
    "        \n",
    "    train_dl = DataLoader(  # An off-the-shelf class\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=self.cli_args.num_workers,\n",
    "        pin_memory=self.use_cuda,\n",
    "    )\n",
    "    return train_dl\n",
    "    \n",
    "# ... line 137\n",
    "def main(self):\n",
    "    train_dl = self.initTrainDl()\n",
    "    val_dl = self.initValDl() # The validation data loader is very similar to training.\n",
    "```\n",
    "\n",
    "In addition to batching individual samples, data loaders can also provide parallel\n",
    "loading of data by using separate processes and shared memory. All we need to do is\n",
    "specify `num_workers=…` when instantiating the data loader, and the rest is taken care of\n",
    "behind the scenes. Each worker process produces complete batches as in figure 11.4.\n",
    "This helps make sure hungry GPUs are well fed with data. Our `validation_ds` and\n",
    "`validation_dl` instances look similar, except for the obvious `isValSet_bool=True`.\n",
    "\n",
    "When we iterate, like `for batch_tup in self.train_dl`:, we won’t have to wait\n",
    "for each `Ct` to be loaded, samples to be taken and batched, and so on. Instead, we’ll\n",
    "get the already loaded `batch_tup` immediately, and a worker process will be freed up\n",
    "in the background to begin loading another batch to use on a later iteration. Using\n",
    "the data-loading features of PyTorch can help speed up most projects, because we can\n",
    "overlap data loading and processing with GPU calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Our first-pass neural network design\n",
    "The possible design space for a convolutional neural network capable of detecting\n",
    "tumors is effectively infinite. Luckily, considerable effort has been spent over the past\n",
    "decade or so investigating effective models for image recognition. While these have\n",
    "largely focused on 2D images, the general architecture ideas transfer well to 3D, so\n",
    "there are many tested designs that we can use as a starting point. This helps because\n",
    "although our first network architecture is unlikely to be our best option, right now we\n",
    "are only aiming for “**good enough to get us going.**”\n",
    "\n",
    "We will base the network design on what we used in chapter 8. We will have to\n",
    "update the model somewhat because our input data is 3D, and we will add some complicating details, but the overall structure shown in figure 11.5 should feel familiar.\n",
    "Similarly, the work we do for this project will be a good base for your future projects,\n",
    "although the further you get from classification or segmentation projects, the more\n",
    "you’ll have to adapt this base to fit. Let’s dissect this architecture, starting with the four\n",
    "repeated blocks that make up the bulk of the network.\n",
    "\n",
    "![](images/11.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 The core convolutions\n",
    "Classification models often have a structure that consists of a **tail**, a **backbone** (or\n",
    "body), and a **head**. The **tail** is the first few layers that process the input to the network.\n",
    "These early layers often have a different structure or organization than the rest of the\n",
    "network, as they must adapt the input to the form expected by the backbone. Here we\n",
    "use a simple batch normalization layer, though often the tail contains convolutional\n",
    "layers as well. Such convolutional layers are often used to aggressively downsample the\n",
    "size of the image; since our image size is already small, we don’t need to do that here.\n",
    "\n",
    "Next, the **backbone** of the network typically contains the bulk of the layers, which\n",
    "are usually arranged in series of **blocks**. Each block has the same (or at least a similar)\n",
    "set of layers, though often the size of the expected input and the number of filters\n",
    "changes from block to block. We will use a block that consists of two 3 × 3 convolutions, each followed by an activation, with a max-pooling operation at the end of the\n",
    "block. We can see this in the expanded view of figure 11.5 labeled `Block[block1]`.\n",
    "Here’s what the implementation of the block looks like in code.\n",
    "\n",
    "#### Listing 11.6 model.py:67, `class LunaBlock`\n",
    "```\n",
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace=True) 1((CO5-1))\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        \n",
    "        return self.maxpool(block_out)\n",
    "```\n",
    "\n",
    "Finally, the **head** of the network takes the output from the backbone and converts it\n",
    "into the desired output form. For convolutional networks, this often involves flattening the intermediate output and passing it to a fully connected layer. For some networks, it makes sense to also include a second fully connected layer, although that is\n",
    "usually more appropriate for classification problems in which the imaged objects have\n",
    "more structure (think about cars versus trucks having wheels, lights, grill, doors, and\n",
    "so on) and for projects with a large number of classes. Since we are only doing binary\n",
    "classification, and we don’t seem to need the additional complexity, we have only a\n",
    "single flattening layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a structure like this can be a good first building block for a convolutional network. There are more complicated designs out there, but for many projects they’re overkill in terms of both implementation complexity and computational demands. It’s a good idea to start simple and add complexity only when there’s a demonstrable need for it.\n",
    "\n",
    "We’re using $3 × 3 × 3$ convolutions in our block. A single $3 × 3 × 3$ convolution has\n",
    "a receptive field of $3 × 3 × 3$, which is almost tautological. Twenty-seven voxels are fed\n",
    "in, and one comes out.\n",
    "\n",
    "![](images/11.6.png)\n",
    "\n",
    "It gets interesting when we use two $3 × 3 × 3$ convolutions stacked back to back. Stacking convolutional layers allows the final output voxel (or pixel) to be influenced by an\n",
    "input further away than the size of the convolutional kernel suggests. If that output voxel is fed into another $3 × 3 × 3$ kernel as one of the edge voxels, then some of the\n",
    "inputs to the first layer will be outside of the $3 × 3 × 3$ area of input to the second. The\n",
    "final output of those two stacked layers has an **effective receptive field** of $5 × 5 × 5$. That\n",
    "means that when taken together, the stacked layers act as similar to a single convolutional layer with a larger size.\n",
    "\n",
    "Put another way, each $3 × 3 × 3$ convolutional layer adds an additional one-voxelper-edge border to the receptive field. We can see this if we trace the arrows in figure 11.6 backward; our $2 × 2$ output has a receptive field of $4 × 4$, which in turn has a\n",
    "receptive field of $6 × 6$. Two stacked $3 × 3 × 3$ layers uses fewer parameters than a full\n",
    "$5 × 5 × 5$ convolution would (and so is also faster to compute).\n",
    "\n",
    "The output of our two stacked convolutions is fed into a $2 × 2 × 2$ max pool, which\n",
    "means we’re taking a $6 × 6 × 6$ effective field, throwing away seven-eighths of the data,\n",
    "and going with the one $5 × 5 × 5$ field that produced the largest value.2 Now, those\n",
    "“discarded” input voxels still have a chance to contribute, since the max pool that’s\n",
    "one output voxel over has an overlapping input field, so it’s possible they’ll influence\n",
    "the final output that way.\n",
    "\n",
    "Note that while we show the receptive field shrinking with each convolutional\n",
    "layer, we’re using padded convolutions, which add a virtual one-pixel border around\n",
    "the image. Doing so keeps our input and output image sizes the same.\n",
    "The nn.ReLU layers are the same as the ones we looked at in chapter 6. Outputs\n",
    "greater than 0.0 will be left unchanged, and outputs less than 0.0 will be clamped to\n",
    "zero.\n",
    "\n",
    "This block will be repeated multiple times to form our model’s backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 The full model\n",
    "Let’s take a look at the full model implementation. We’ll skip the block definition,\n",
    "since we just saw that in listing 11.6\n",
    "\n",
    "#### Listing 11.7 model.py:13, `class LunaModel`\n",
    "```\n",
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1) # tail\n",
    "        \n",
    "        self.block1 = LunaBlock(in_channels, conv_channels) # backbone\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels * 2) # backbone\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4) # backbone\n",
    "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8) # backbone\n",
    "        \n",
    "        self.head_linear = nn.Linear(1152, 2) # head\n",
    "        self.head_softmax = nn.Softmax(dim=1) # head\n",
    "```\n",
    "Here, our tail is relatively simple. We are going to normalize our input using\n",
    "`nn.BatchNorm3d`, which, as we saw in chapter 8, will shift and scale our input so that it\n",
    "has a mean of 0 and a standard deviation of 1. Thus, the somewhat odd Hounsfield\n",
    "unit (HU) scale that our input is in won’t really be visible to the rest of the network.\n",
    "This is a somewhat arbitrary choice; we know what our input units are, and we know\n",
    "the expected values of the relevant tissues, so we could probably implement a fixed\n",
    "normalization scheme pretty easily. It’s not clear which approach would be better.\n",
    "\n",
    "Our backbone is four repeated blocks, with the block implementation pulled out into\n",
    "the separate `nn.Module` subclass we saw earlier in listing 11.6. Since each block ends with\n",
    "a $2 × 2 × 2$ max-pool operation, after 4 layers we will have decreased the resolution of the\n",
    "image 16 times in each dimension. Recall from chapter 10 that our data is returned in\n",
    "chunks that are 32 × 48 × 48, which will become $2 × 3 × 3$ by the end of the backbone.\n",
    "\n",
    "Finally, our tail is just a fully connected layer followed by a call to `nn.Softmax`. Softmax is a useful function for single-label classification tasks and has a few nice properties: it bounds the output between 0 and 1, it’s relatively insensitive to the absolute\n",
    "range of the inputs (only the relative values of the inputs matter), and it allows our\n",
    "model to express the degree of certainty it has in an answer.\n",
    "\n",
    "The function itself is relatively simple. Every value from the input is used to exponentiate $e$, and the resulting series of values is then divided by the sum of all the\n",
    "results of exponentiation. Here’s what it looks like implemented in a simple fashion as\n",
    "a nonoptimized softmax implementation in pure Python:\n",
    "\n",
    "```\n",
    ">>> logits = [1, -2, 3]\n",
    ">>> exp = [e ** x for x in logits]\n",
    ">>> exp\n",
    "[2.718, 0.135, 20.086]\n",
    ">>> softmax = [x / sum(exp) for x in exp]\n",
    ">>> softmax\n",
    "[0.118, 0.006, 0.876]\n",
    "```\n",
    "Of course, we use the PyTorch version of `nn.Softmax` for our model, as it natively\n",
    "understands batches and tensors and will perform autograd quickly and as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPLICATION: CONVERTING FROM CONVOLUTION TO LINEAR\n",
    "Continuing on with our model definition, we come to a complication. We can’t just\n",
    "feed the output of `self.block4` into a fully connected layer, since that output is a persample $2 × 3 × 3$ image with 64 channels, and fully connected layers expect a 1D vector\n",
    "as input (well, technically they expect a batch of 1D vectors, which is a 2D array, but the\n",
    "mismatch remains either way). Let’s take a look at the `forward` method.\n",
    "\n",
    "#### Listing 11.8 model.py:50, `LunaModel.forward`\n",
    "```\n",
    "def forward(self, input_batch):\n",
    "    bn_output = self.tail_batchnorm(input_batch)\n",
    "    \n",
    "    block_out = self.block1(bn_output)\n",
    "    block_out = self.block2(block_out)\n",
    "    block_out = self.block3(block_out)\n",
    "    block_out = self.block4(block_out)\n",
    "    conv_flat = block_out.view(\n",
    "        block_out.size(0), # The batch size\n",
    "        -1,\n",
    "    )\n",
    "    linear_output = self.head_linear(conv_flat)\n",
    "    \n",
    "    return linear_output, self.head_softmax(linear_output)\n",
    "```\n",
    "Note that before we pass data into a fully connected layer, we must **flatten** it using the\n",
    "`view` function. Since that operation is stateless (it has no parameters that govern its\n",
    "behavior), we can simply perform the operation in the forward function. This is\n",
    "somewhat similar to the functional interfaces we discussed in chapter 8. Almost every\n",
    "model that uses convolution and produces classifications, regressions, or other nonimage outputs will have a similar component in the head of the network.\n",
    "\n",
    "For the return value of the forward method, we return both the raw **logits** and the\n",
    "softmax-produced probabilities. We first hinted at logits in section 7.2.6: they are the\n",
    "numerical values produced by the network prior to being normalized into probabilities by the softmax layer. That might sound a bit complicated, but logits are really just\n",
    "the raw input to the softmax layer. They can have any real-valued input, and the softmax will squash them to the range 0–1.\n",
    "\n",
    "We’ll use the logits when we calculate the `nn.CrossEntropyLoss` during training,4\n",
    "and we’ll use the probabilities for when we want to actually classify the samples. This\n",
    "kind of slight difference between what’s used for training and what’s used in production is fairly common, especially when the difference between the two outputs is a simple, stateless function like softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION\n",
    "Finally, let’s talk about initializing our network’s parameters. In order to get wellbehaved performance out of our model, the network’s weights, biases, and other\n",
    "parameters need to exhibit certain properties. Let’s imagine a degenerate case, where\n",
    "all of the network’s weights are greater than 1 (and we do not have residual connections). In that case, repeated multiplication by those weights would result in layer outputs that became very large as data flowed through the layers of the network.\n",
    "Similarly, weights less than 1 would cause all layer outputs to become smaller and vanish. Similar considerations apply to the gradients in the backward pass.\n",
    "\n",
    "Many normalization techniques can be used to keep layer outputs well behaved, but\n",
    "one of the simplest is to just make sure the network’s weights are initialized such that\n",
    "intermediate values and gradients become neither unreasonably small nor unreasonably\n",
    "large. As we discussed in chapter 8, PyTorch does not help us as much as it should here,\n",
    "so we need to do some initialization ourselves. We can treat the following `_init_weights`\n",
    "function as boilerplate, as the exact details aren’t particularly important.\n",
    "\n",
    "#### Listing 11.9 model.py:30, `LunaModel._init_weights`\n",
    "```\n",
    "def _init_weights(self):\n",
    "    for m in self.modules():\n",
    "        if type(m) in { nn.Linear, nn.Conv3d,}:\n",
    "            nn.init.kaiming_normal_(\n",
    "                m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
    "            )\n",
    "        if m.bias is not None:\n",
    "            fan_in, fan_out = \\\n",
    "                nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "            bound = 1 / math.sqrt(fan_out)\n",
    "            nn.init.normal_(m.bias, -bound, bound)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Training and validating the model\n",
    "Now it’s time to take the various pieces we’ve been working with and assemble them\n",
    "into something we can actually execute. This training loop should be familiar—we saw\n",
    "loops like figure 11.7 in chapter 5.\n",
    "\n",
    "![](images/11.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is relatively compact (the `doTraining` function is only 12 statements; it’s longer here due to line-length limitations).\n",
    "\n",
    "#### Listing 11.10 training.py:137, `LunaTrainingApp.main`\n",
    "```\n",
    "def main(self):\n",
    "    # ... line 143\n",
    "    for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "        trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "        self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "# ... line 165\n",
    "def doTraining(self, epoch_ndx, train_dl):\n",
    "    self.model.train()\n",
    "    trnMetrics_g = torch.zeros(  # Initializes an empty\n",
    "        METRICS_SIZE,            # metrics array\n",
    "        len(train_dl.dataset),\n",
    "        device=self.device,\n",
    "    )\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(   # Sets up our batch looping\n",
    "        train_dl,                         # with time estimate\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    \n",
    "    for batch_ndx, batch_tup in batch_iter:\n",
    "        self.optimizer.zero_grad()  # Frees any leftover gradient tensors\n",
    "        loss_var = self.computeBatchLoss(  # We’ll discuss this method in\n",
    "            batch_ndx,                     # detail in the next section.\n",
    "            batch_tup,\n",
    "            train_dl.batch_size,\n",
    "            trnMetrics_g\n",
    "        )\n",
    "        \n",
    "        loss_var.backward()   # Actually updates\n",
    "        self.optimizer.step() # the model weights\n",
    "        \n",
    "    self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "    return trnMetrics_g.to('cpu')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences that we see from the training loops in earlier chapters are as\n",
    "follows:\n",
    "- The `trnMetrics_g` tensor collects detailed per-class metrics during training. For larger projects like ours, this kind of insight can be very nice to have.\n",
    "- We don’t directly iterate over the `train_dl` data loader. We use `enumerateWithEstimate` to provide an estimated time of completion. This isn’t crucial; it’s just a stylistic choice.\n",
    "- The actual loss computation is pushed into the `computeBatchLoss` method. Again, this isn’t strictly necessary, but code reuse is typically a plus.\n",
    "\n",
    "We’ll discuss why we’ve wrapped `enumerate` with additional functionality in section\n",
    "11.7.2; for now, assume it’s the same as enumerate(train_dl).\n",
    "\n",
    "The purpose of the `trnMetrics_g` tensor is to transport information about how\n",
    "the model is behaving on a per-sample basis from the `computeBatchLoss` function to\n",
    "the `logMetrics` function. Let’s take a look at `computeBatchLoss` next. We’ll cover\n",
    "`logMetrics` after we’re done with the rest of the main training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1 The computeBatchLoss function\n",
    "The `computeBatchLoss` function is called by both the **training** and **validation** loops. As the name suggests, it computes the loss over a batch of samples. In addition, the function also computes and records per-sample information about the output the model is\n",
    "producing. This lets us compute things like the percentage of correct answers per\n",
    "class, which allows us to hone in on areas where our model is having difficulty.\n",
    "\n",
    "Of course, the function’s core functionality is around feeding the batch into the\n",
    "model and computing the per-batch loss. We’re using `CrossEntropyLoss` (https://\n",
    "pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss), just like in chapter 7.\n",
    "Unpacking the batch tuple, moving the tensors to the GPU, and invoking the model\n",
    "should all feel familiar after that earlier training work.\n",
    "\n",
    "#### Listing 11.11 training.py:225, `.computeBatchLoss`\n",
    "```\n",
    "def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "    input_t, label_t, _series_list, _center_list = batch_tup\n",
    "    \n",
    "    input_g = input_t.to(self.device, non_blocking=True)\n",
    "    label_g = label_t.to(self.device, non_blocking=True)\n",
    "    \n",
    "    logits_g, probability_g = self.model(input_g)\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss(reduction='none') # reduction=‘none’ gives\n",
    "                                                      # the loss per sample.\n",
    "    loss_g = loss_func(\n",
    "        logits_g,\n",
    "        label_g[:,1], # Index of the one hot-encoded class\n",
    "    )\n",
    "    # ... line 238\n",
    "    return loss_g.mean() # Recombines the loss per sample into a single value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are **not** using the default behavior to get a loss value averaged over the batch.\n",
    "Instead, we get a tensor of loss values, one per sample. This lets us track the individual\n",
    "losses, which means we can aggregate them as we wish (per class, for example). We’ll\n",
    "see that in action in just a moment. For now, we’ll return the mean of those per-sample\n",
    "losses, which is equivalent to the batch loss. In situations where you don’t want to keep\n",
    "statistics per sample, using the loss averaged over the batch is perfectly fine. Whether\n",
    "that’s the case is highly dependent on your project and goals.\n",
    "\n",
    "Once that’s done, we’ve fulfilled our obligations to the calling function in terms of\n",
    "what’s required to do backpropagation and weight updates. Before we do that, however, we also want to record our per-sample stats for posterity (and later analysis).\n",
    "We’ll use the `metrics_g` parameter passed in to accomplish this.\n",
    "\n",
    "#### Listing 11.12 training.py:26\n",
    "```\n",
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "\n",
    "    # ... line 225\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        # ... line 238\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "        \n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n",
    "            label_g[:,1].detach()                            # We use detach since\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = \\   # none of our metrics\n",
    "            probability_g[:,1].detach()                      # need to hold on to\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = \\   # gradients.\n",
    "            loss_g.detach()\n",
    "                                 # Again, this is the loss\n",
    "        return loss_g.mean()     # over the entire batch.\n",
    "```\n",
    "By recording the label, prediction, and loss for each and every training (and later, validation) sample, we have a wealth of detailed information we can use to investigate\n",
    "the behavior of our model. For now, we’re going to focus on compiling per-class statistics, but we could easily use this information to find the sample that is classified the\n",
    "most wrongly and start to investigate why. Again, for some projects, this kind of information will be less interesting, but it’s good to remember that you have these kinds of\n",
    "options available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2 The validation loop is similar\n",
    "The validation loop in figure 11.8 looks very similar to training but is somewhat simplified. The key difference is that validation is read-only. Specifically, the loss value\n",
    "returned is not used, and the weights are not updated.\n",
    "\n",
    "![](images/11.8.png)\n",
    "\n",
    "Nothing about the model should have changed between the start and end of the function call. In addition, it’s quite a bit faster due to the `with torch.no_grad()` context\n",
    "manager explicitly informing PyTorch that no gradients need to be computed.\n",
    "\n",
    "#### Listing 11.13 training.py:137, `LunaTrainingApp.main`\n",
    "```\n",
    "def main(self):\n",
    "    for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "        # ... line 157\n",
    "        valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "        self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "        \n",
    "# ... line 203\n",
    "def doValidation(self, epoch_ndx, val_dl):\n",
    "    with torch.no_grad():\n",
    "        self.model.eval()  # Turns off training-time behavior\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.computeBatchLoss(\n",
    "                batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "        \n",
    "    return valMetrics_g.to('cpu')\n",
    "```\n",
    "Without needing to update network weights (recall that doing so would violate the\n",
    "entire premise of the validation set; something we never want to do!), we don’t need\n",
    "to use the loss returned from `computeBatchLoss`, nor do we need to reference the\n",
    "optimizer. All that’s left inside the loop is the call to `computeBatchLoss`. Note that we\n",
    "are still collecting metrics in `valMetrics_g` as a side effect of the call, even though we\n",
    "aren’t using the overall per-batch loss returned by `computeBatchLoss` for anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Outputting performance metrics\n",
    "we were collecting results in `trnMetrics_g` and `valMetrics_g` for logging progress per epoch. Each of these two tensors now contains everything we need to compute our percent correct and average loss per class for our training and validation runs. Doing this per epoch is a common choice, though somewhat arbitrary. In future chapters, we’ll see how to manipulate the size of our epochs such that we get feedback about training progress at a reasonable rate.\n",
    "\n",
    "![](images/11.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.1 The logMetrics function\n",
    "Let’s talk about the high-level structure of the `logMetrics` function. The signature\n",
    "looks like this.\n",
    "\n",
    "#### Listing 11.14 training.py:251, `LunaTrainingApp.logMetrics`\n",
    "```\n",
    "def logMetrics(\n",
    "    self,\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5,\n",
    "):\n",
    "```\n",
    "We use `epoch_ndx` purely for display while logging our results. The `mode_str` argument tells us whether the metrics are for training or validation.\n",
    "\n",
    "We consume either `trnMetrics_t` or `valMetrics_t`, which is passed in as the `metrics_t` parameter. Recall that both of those inputs are tensors of floating-point values that we\n",
    "filled with data during `computeBatchLoss` and then transferred back to the CPU right\n",
    "before we returned them from `doTraining` and `doValidation`. Both tensors have three\n",
    "rows and as many columns as we have samples (training samples or validation samples,\n",
    "depending). As a reminder, those three rows correspond to the following constants.\n",
    "\n",
    "#### Listing 11.15 training.py:26\n",
    "```\n",
    "METRICS_LABEL_NDX=0   # These are declared at\n",
    "METRICS_PRED_NDX=1    # module-level scope.\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTRUCTING MASKS\n",
    "Next, we’re going to construct masks that will let us limit our metrics to only the nodule or non-nodule (aka positive or negative) samples. We will also count the total samples per class, as well as the number of samples we classified correctly.\n",
    "\n",
    "#### Listing 11.16 training.py:264, `LunaTrainingApp.logMetrics`\n",
    "```\n",
    "negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
    "negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
    "posLabel_mask = ~negLabel_mask\n",
    "posPred_mask = ~negPred_mask\n",
    "```\n",
    "While we don’t `assert` it here, we know that all of the values stored in `metrics_t[METRICS_LABEL_NDX]` belong to the set `{0.0, 1.0}` since we know that our nodule\n",
    "status labels are simply `True` or `False`. By comparing to `classificationThreshold`,\n",
    "which defaults to 0.5, we get an array of binary values where a `True` value corresponds\n",
    "to a non-nodule (aka negative) label for the sample in question.\n",
    "\n",
    "We do a similar comparison to create the `negPred_mask`, but we must remember\n",
    "that the `METRICS_PRED_NDX` values are the positive predictions produced by our model\n",
    "and can be any floating-point value between 0.0 and 1.0, inclusive. That doesn’t\n",
    "change our comparison, but it does mean the actual value can be close to 0.5. The\n",
    "positive masks are simply the inverse of the negative masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use those masks to compute some per-label statistics and store them in a dictionary, `metrics_dict`.\n",
    "\n",
    "#### Listing 11.17 training.py:270, `LunaTrainingApp.logMetrics`\n",
    "```\n",
    "neg_count = int(negLabel_mask.sum()) # Converts to a normal Python integer\n",
    "pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "\n",
    "metrics_dict = {}\n",
    "metrics_dict['loss/all'] = \\\n",
    "    metrics_t[METRICS_LOSS_NDX].mean()\n",
    "metrics_dict['loss/neg'] = \\\n",
    "    metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "metrics_dict['loss/pos'] = \\\n",
    "    metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "    \n",
    "metrics_dict['correct/all'] = (pos_correct + neg_correct) \\\n",
    "    / np.float32(metrics_t.shape[1]) * 100  # Avoids integer division by division by np.float32\n",
    "metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
    "metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the average loss over the entire epoch. Since the loss is the single\n",
    "metric that is being minimized during training, we always want to be able to keep\n",
    "track of it. Then we limit the loss averaging to only those samples with a negative label\n",
    "using the `negLabel_mask` we just made. We do the same with the positive loss. Computing a per-class loss like this can be useful if one class is persistently harder to classify\n",
    "than another, since that knowledge can help drive investigation and improvements.\n",
    "\n",
    "We’ll close out the calculations with determining the fraction of samples we classified correctly, as well as the fraction correct from each label. Since we will display\n",
    "these numbers as percentages in a moment, we also multiply the values by 100. Similar\n",
    "to the loss, we can use these numbers to help guide our efforts when making improvements. After the calculations, we then log our results with three calls to `log.info`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 11.18 training.py:289, `LunaTrainingApp.logMetrics`\n",
    "```\n",
    "log.info(\n",
    "    (\"E{} {:8} {loss/all:.4f} loss, \"\n",
    "        + \"{correct/all:-5.1f}% correct, \"\n",
    "    ).format(\n",
    "        epoch_ndx,\n",
    "        mode_str,\n",
    "        **metrics_dict,\n",
    "    )\n",
    ")\n",
    "log.info(\n",
    "    (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
    "        + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "    ).format(\n",
    "        epoch_ndx,\n",
    "        mode_str + '_neg',\n",
    "        neg_correct=neg_correct,\n",
    "        neg_count=neg_count,\n",
    "        **metrics_dict,\n",
    "    )\n",
    ")\n",
    "log.info(  # The ‘pos’ logging is similar to the ‘neg’ logging earlier.\n",
    "    # ... line 319\n",
    ")\n",
    "```\n",
    "The first log has values computed from all of our samples and is tagged `/all`, while\n",
    "the negative (non-nodule) and positive (nodule) values are tagged `/neg` and `/pos`,\n",
    "respectively. We don’t show the third logging statement for positive values here; it’s\n",
    "identical to the second except for swapping **neg** for **pos** in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Running the training script\n",
    "Now that we’ve completed the core of the `training.py` script, we’ll actually start running it. This will initialize and train our model and print statistics about how well the\n",
    "training is going. The idea is to get this kicked off to run in the background while\n",
    "we’re covering the model implementation in detail. Hopefully we’ll have results to\n",
    "look at once we’re done.\n",
    "\n",
    "We’re running this script from the main code directory; it should have subdirectories called p2ch11, util, and so on. The **python** environment used should have all the\n",
    "libraries listed in requirements.txt installed. Once those libraries are ready, we can run:\n",
    "```\n",
    "$ python -m p2ch11.training  # This is the command line for Linux/Bash\n",
    "Starting LunaTrainingApp,\n",
    "    Namespace(batch_size=256, channels=8, epochs=20, layers=3, num_workers=8)\n",
    "<p2ch11.dsets.LunaDataset object at 0x7fa53a128710>: 495958 training samples\n",
    "<p2ch11.dsets.LunaDataset object at 0x7fa537325198>: 55107 validation samples\n",
    "Epoch 1 of 20, 1938/216 batches of size 256\n",
    "E1 Training ----/1938, starting\n",
    "E1 Training 16/1938, done at 2018-02-28 20:52:54, 0:02:57\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, we also provide a **Jupyter Notebook** that contains invocations of the\n",
    "training application.\n",
    "#### Listing 11.19 code/p2_run_everything.ipynb\n",
    "```\n",
    "# In[5]:\n",
    "run('p2ch11.prepcache.LunaPrepCacheApp')\n",
    "\n",
    "# In[6]:\n",
    "run('p2ch11.training.LunaTrainingApp', '--epochs=1')\n",
    "```\n",
    "If the first epoch seems to be taking a very long time (more than 10 or 20 minutes), it\n",
    "might be related to needing to prepare the cached data required by `LunaDataset`. See\n",
    "section 10.5.1 for details about the caching. The exercises for chapter 10 included\n",
    "writing a script to pre-stuff the cache in an efficient manner. We also provide the\n",
    "`prepcache.py` file to do the same thing; it can be invoked with `python -m p2ch11.prepcache`. Since we repeat our `dsets.py` files per chapter, the caching will need to be\n",
    "repeated for every chapter. This is somewhat space and time inefficient, but it means we\n",
    "can keep the code for each chapter much more well contained. For your future projects, we recommend reusing your cache more heavily.\n",
    "\n",
    "Once training is underway, we want to make sure we’re using the computing\n",
    "resources at hand the way we expect. An easy way to tell if the bottleneck is data loading\n",
    "or computation is to wait a few moments after the script starts to train (look for output\n",
    "like `E1 Training 16/7750, done at…`) and then check both `top` and `nvidia-smi`:\n",
    "- If the eight Python worker processes are consuming >80% CPU, then the cache probably needs to be prepared (we know this here because the authors have made sure there aren’t CPU bottlenecks in this project’s implementation; this won’t be generally true).\n",
    "- If `nvidia-smi` reports that `GPU-Util` is >80%, then you’re saturating your GPU. We’ll discuss some strategies for efficient waiting in section 11.7.2.\n",
    "\n",
    "The intent is that the GPU is saturated; we want to use as much of that computing power as we can to complete epochs quickly. A single NVIDIA GTX 1080 Ti should complete an epoch in under 15 minutes. Since our model is relatively simple, it doesn’t take a lot of CPU preprocessing for the CPU to be the bottleneck. When working with models with greater depth (or more needed calculations in general), processing each batch will take longer, which will increase the amount of CPU processing we can do before the GPU runs out of work before the next batch of input is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.1 Needed data for training\n",
    "If the number of samples is less than 495,958 for training or 55,107 for validation, it\n",
    "might make sense to do some sanity checking to be sure the full data is present and\n",
    "accounted for. For your future projects, make sure your dataset returns the number of\n",
    "samples that you expect.\n",
    "\n",
    "First, let’s take a look at the basic directory structure of our data-unversioned/\n",
    "part2/luna directory:\n",
    "\n",
    "```\n",
    "$ ls -1p data-unversioned/part2/luna/\n",
    "subset0/\n",
    "subset1/\n",
    "...\n",
    "subset9/\n",
    "```\n",
    "\n",
    "Next, let’s make sure we have one `.mhd` file and one `.raw` file for each series UID\n",
    "\n",
    "```\n",
    "$ ls -1p data-unversioned/part2/luna/subset0/\n",
    "1.3.6.1.4.1.14519.5.2.1.6279.6001.105756658031515062000744821260.mhd\n",
    "1.3.6.1.4.1.14519.5.2.1.6279.6001.105756658031515062000744821260.raw\n",
    "1.3.6.1.4.1.14519.5.2.1.6279.6001.108197895896446896160048741492.mhd\n",
    "1.3.6.1.4.1.14519.5.2.1.6279.6001.108197895896446896160048741492.raw\n",
    "...\n",
    "```\n",
    "and that we have the overall correct number of files:\n",
    "```\n",
    "$ ls -1 data-unversioned/part2/luna/subset?/* | wc -l\n",
    "1776\n",
    "$ ls -1 data-unversioned/part2/luna/subset0/* | wc -l\n",
    "178\n",
    "...\n",
    "$ ls -1 data-unversioned/part2/luna/subset9/* | wc -l\n",
    "176\n",
    "```\n",
    "If all of these seem right but things still aren’t working, ask on Manning LiveBook\n",
    "(https://livebook.manning.com/book/deep-learning-with-pytorch/chapter-11) and\n",
    "hopefully someone can help get things sorted out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Interlude: The enumerateWithEstimate function\n",
    "Working with deep learning involves a lot of waiting. We’re talking about real-world,\n",
    "sitting around, glancing at the clock on the wall, a watched pot never boils (but you\n",
    "could fry an egg on the GPU), straight up **boredom**.\n",
    "\n",
    "The only thing worse than sitting and staring at a blinking cursor that hasn’t\n",
    "moved for over an hour is flooding your screen with this:\n",
    "```\n",
    "2020-01-01 10:00:00,056 INFO training batch 1234\n",
    "2020-01-01 10:00:00,067 INFO training batch 1235\n",
    "2020-01-01 10:00:00,077 INFO training batch 1236\n",
    "2020-01-01 10:00:00,087 INFO training batch 1237\n",
    "...etc...\n",
    "```\n",
    "\n",
    "At least the quietly blinking cursor doesn’t blow out your scrollback buffer!\\\n",
    "Fundamentally, while doing all this waiting, we want to answer the question “Do I\n",
    "have time to go refill my water glass?” along with follow-up questions about having\n",
    "time to\n",
    "- Brew a cup of coffee\n",
    "- Grab dinner\n",
    "- Grab dinner in Paris\n",
    "\n",
    "To answer these pressing questions, we’re going to use our enumerateWithEstimate\n",
    "function. Usage looks like the following:\n",
    "\n",
    "```\n",
    ">>> for i, _ in enumerateWithEstimate(list(range(234)), \"sleeping\"):\n",
    "... time.sleep(random.random())\n",
    "...\n",
    "11:12:41,892 WARNING sleeping ----/234, starting\n",
    "11:12:44,542 WARNING sleeping 4/234, done at 2020-01-01 11:15:16, 0:02:35\n",
    "11:12:46,599 WARNING sleeping 8/234, done at 2020-01-01 11:14:59, 0:02:17\n",
    "11:12:49,534 WARNING sleeping 16/234, done at 2020-01-01 11:14:33, 0:01:51\n",
    "11:12:58,219 WARNING sleeping 32/234, done at 2020-01-01 11:14:41, 0:01:59\n",
    "11:13:15,216 WARNING sleeping 64/234, done at 2020-01-01 11:14:43, 0:02:01\n",
    "11:13:44,233 WARNING sleeping 128/234, done at 2020-01-01 11:14:35, 0:01:53\n",
    "11:14:40,083 WARNING sleeping ----/234, done at 2020-01-01 11:14:40\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s 8 lines of output for over 200 iterations lasting about 2 minutes. Even given the\n",
    "wide variance of `random.random()`, the function had a pretty decent estimate after 16\n",
    "iterations (in less than 10 seconds). For loop bodies with more constant timing, the\n",
    "estimates stabilize even more quickly.\n",
    "\n",
    "In terms of behavior, `enumerateWithEstimate` is almost identical to the standard\n",
    "enumerate (the differences are things like the fact that our function returns a generator, whereas `enumerate` returns a specialized `<enumerate object at 0x…>`)\n",
    "\n",
    "#### Listing 11.20 util.py:143, `def enumerateWithEstimate`\n",
    "```\n",
    "def enumerateWithEstimate(\n",
    "    iter,\n",
    "    desc_str,\n",
    "    start_ndx=0,\n",
    "    print_ndx=4,\n",
    "    backoff=None,\n",
    "    iter_len=None,\n",
    "):\n",
    "    for (current_ndx, item) in enumerate(iter):\n",
    "        yield (current_ndx, item)\n",
    "```\n",
    "However, the side effects (logging, specifically) are what make the function interesting. Rather than get lost in the weeds trying to cover every detail of the implementation, if you’re interested, you can consult the function docstring (https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/util/util.py#L143) to get\n",
    "information about the function parameters and desk-check the implementation.\n",
    "\n",
    "Deep learning projects can be very time intensive. Knowing when something is\n",
    "expected to finish means you can use your time until then wisely, and it can also clue\n",
    "you in that something isn’t working properly (or an approach is unworkable) if the\n",
    "expected time to completion is much larger than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8 Evaluating the model: Getting 99.7% correct means we’re done, right?\n",
    "Let’s take a look at some (abridged) output from our training script. As a reminder,\n",
    "we’ve run this with the command line `python -m p2ch11.training`:\n",
    "```\n",
    "E1 Training ----/969, starting\n",
    "...\n",
    "E1 LunaTrainingApp\n",
    "E1 trn 2.4576 loss, 99.7% correct\n",
    "...\n",
    "E1 val 0.0172 loss, 99.8% correct\n",
    "...\n",
    "```\n",
    "\n",
    "After one epoch of training, both the training and validation set show at least 99.7%\n",
    "correct results. That’s an A+! Time for a round of high-fives, or at least a satisfied nod\n",
    "and smile. We just solved cancer! … Right? \\\n",
    "Well, no. \\\n",
    "Let’s take a closer (less-abridged) look at that epoch 1 output:\n",
    "```\n",
    "E1 LunaTrainingApp\n",
    "E1 trn 2.4576 loss, 99.7% correct,\n",
    "E1 trn_neg 0.1936 loss, 99.9% correct (494289 of 494743)\n",
    "E1 trn_pos 924.34 loss, 0.2% correct (3 of 1215)\n",
    "...\n",
    "E1 val 0.0172 loss, 99.8% correct,\n",
    "E1 val_neg 0.0025 loss, 100.0% correct (494743 of 494743)\n",
    "E1 val_pos 5.9768 loss, 0.0% correct (0 of 1215)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification output remains the same—none of the nodule (aka positive) samples are correctly identified. It’s interesting that we’re starting to see some decrease in\n",
    "the `val_pos` loss, however, while not seeing a corresponding increase in the `val_neg`\n",
    "loss. This implies that the network is learning something. Unfortunately, it’s learning\n",
    "very, very slowly.\n",
    "\n",
    "Even worse, this particular failure mode is the most dangerous in the real world!\n",
    "We want to avoid the situation where we classify a tumor as an innocuous structure, because that would not facilitate a patient getting the evaluation and eventual treatment they might need. It’s important to understand the consequences for misclassification for all your projects, as that can have a large impact on how you design, train,\n",
    "and evaluate your model. We’ll discuss this more in the next chapter.\n",
    "\n",
    "Before we get to that, however, we need to upgrade our tooling to make the results\n",
    "easier to understand. We’re sure you love to squint at columns of numbers as much as\n",
    "anyone, but pictures are worth a thousand words. Let’s graph some of these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 Graphing training metrics with TensorBoard\n",
    "We’re going to use a tool called **TensorBoard** as a quick and easy way to get our training metrics out of our training loop and into some pretty graphs. This will allow us to\n",
    "follow the trends of those metrics, rather than only look at the instantaneous values per\n",
    "epoch. It gets much, much easier to know whether a value is an outlier or just the latest in a trend when you’re looking at a visual representation.\n",
    "\n",
    "“Hey, wait,” you might be thinking, “isn’t TensorBoard part of the TensorFlow project? What’s it doing here in my PyTorch book?”\n",
    "\n",
    "Well, yes, it is part of another deep learning framework, but our philosophy is “use\n",
    "what works.” There’s no reason to restrict ourselves by not using a tool just because it’s\n",
    "bundled with another project we’re not using. Both the PyTorch and TensorBoard\n",
    "devs agree, because they collaborated to add official support for TensorBoard into\n",
    "PyTorch. TensorBoard is great, and it’s got some easy-to-use PyTorch APIs that let us\n",
    "hook data from just about anywhere into it for quick and easy display. If you stick with\n",
    "deep learning, you’ll probably be seeing (and using) a lot of TensorBoard.\n",
    "\n",
    "In fact, if you’ve been running the chapter examples, you should already have\n",
    "some data on disk ready and waiting to be displayed. Let’s see how to run TensorBoard, and look at what it can show us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.9.1 Running TensorBoard\n",
    "By default, our training script will write metrics data to the `runs/ subdirectory`. If you\n",
    "list the directory content, you might see something like this during your Bash shell\n",
    "session:\n",
    "```\n",
    "$ ls -lA runs/p2ch11/\n",
    "total 24\n",
    "drwxrwxr-x 2 elis elis 4096 Sep 15 13:22 2020-01-01_12.55.27-trn-dlwpt/ # The single-epoch\n",
    "drwxrwxr-x 2 elis elis 4096 Sep 15 13:22 2020-01-01_12.55.27-val-dlwpt/ # run from earlier\n",
    "drwxrwxr-x 2 elis elis 4096 Sep 15 15:14 2020-01-01_13.31.23-trn-dwlpt/ # The more recent 10\n",
    "drwxrwxr-x 2 elis elis 4096 Sep 15 15:14 2020-01-01_13.31.23-val-dwlpt/ # epoch training run\n",
    "```\n",
    "\n",
    "To get the `tensorboard` program, install the `tensorflow` (https://pypi.org/project/\n",
    "tensorflow) Python package. Since we’re not actually going to use TensorFlow proper,\n",
    "it’s fine if you install the default CPU-only package. If you have another version of TensorBoard installed already, using that is fine too. Either make sure the appropriate\n",
    "directory is on your path, or invoke it with `../path/to/tensorboard --logdir runs/`.\n",
    "It doesn’t really matter where you invoke it from, as long as you use the `--logdir` argument to point it at where your data is stored. It’s a good idea to segregate your data into\n",
    "separate folders, as TensorBoard can get a bit unwieldy once you get over 10 or 20\n",
    "experiments. You’ll have to decide the best way to do that for each project as you go.\n",
    "Don’t be afraid to move data around after the fact if you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start TensorBoard now:\n",
    "```\n",
    "$ tensorboard --logdir runs/\n",
    "2020-01-01 12:13:16.163044: I tensorflow/core/platform/cpu_feature_guard.cc:140]\n",
    "Your CPU supports instructions that this TensorFlow binary was not\n",
    "➥ compiled to use: AVX2 FMA 1((CO17-2))\n",
    "TensorBoard 1.14.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```\n",
    "Once that’s done, you should be able to point your browser at http://localhost:6006\n",
    "and see the main dashboard. Figure 11.10 shows us what that looks like.\n",
    "\n",
    "![](images/11.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the top of the browser window, you should see the orange header. The right\n",
    "side of the header has the typical widgets for settings, a link to the GitHub repository,\n",
    "and the like. We can ignore those for now. The left side of the header has items for the\n",
    "data types we’ve provided. You should have at least the following:\n",
    "- Scalars (the default tab)\n",
    "- Histograms\n",
    "- Precision-Recall Curves (shown as PR Curves)\n",
    "\n",
    "You might see Distributions as well as the second UI tab (to the right of Scalars in figure 11.10). We won’t use or discuss those here. Make sure you’ve selected Scalars by\n",
    "clicking it.\n",
    "\n",
    "On the left is a set of controls for display options, as well as a list of runs that are\n",
    "present. The smoothing option can be useful if you have particularly noisy data; it will\n",
    "calm things down so that you can pick out the overall trend. The original nonsmoothed data will still be visible in the background as a faded line in the same color.\n",
    "Figure 11.11 shows this, although it might be difficult to discern when printed in black\n",
    "and white.\n",
    "\n",
    "Depending on how many times you’ve run the training script, you might have multiple runs to select from. With too many runs being rendered, the graphs can get\n",
    "overly noisy, so don’t hesitate to deselect runs that aren’t of interest at the moment.\n",
    "\n",
    "![](images/11.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to permanently remove a run, the data can be deleted from disk while\n",
    "TensorBoard is running. You can do this to get rid of experiments that crashed, had bugs, didn’t converge, or are so old they’re no longer interesting. The number of runs\n",
    "can grow pretty quickly, so it can be helpful to prune it often and to rename runs or\n",
    "move runs that are particularly interesting to a more permanent directory so they\n",
    "don’t get deleted by accident. To remove both the train and validation runs, execute the following (after changing the chapter, date, and time to match the run you\n",
    "want to remove):\n",
    "```\n",
    "$ rm -rf runs/p2ch11/2020-01-01_12.02.15_*\n",
    "```\n",
    "Keep in mind that removing runs will cause the runs that are later in the list to move\n",
    "up, which will result in them being assigned new colors.\n",
    "OK, let’s get to the point of TensorBoard: the pretty graphs! The main part of the\n",
    "screen should be filled with data from gathering training and validation metrics, as\n",
    "shown in figure 11.12.\n",
    "\n",
    "![](images/11.12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s much easier to parse and absorb than `E1 trn_pos 924.34 loss, 0.2% correct\n",
    "(3 of 1215)`! Although we’re going to save discussion of what these graphs are telling\n",
    "us for section 11.10, now would be a good time to make sure it’s clear what these numbers correspond to from our training program. Take a moment to cross-reference the numbers you get by mousing over the lines with the numbers spit out by training.py\n",
    "during the same training run. You should see a direct correspondence between the\n",
    "Value column of the tooltip and the values printed during training. Once you’re comfortable and confident that you understand exactly what TensorBoard is showing you,\n",
    "let’s move on and discuss how to get these numbers to appear in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.9.2 Adding TensorBoard support to the metrics logging function\n",
    "We are going to use the `torch.utils.tensorboard` module to write data in a format\n",
    "that TensorBoard will consume. This will allow us to write metrics for this and any\n",
    "other project quickly and easily. TensorBoard supports a mix of NumPy arrays and\n",
    "PyTorch tensors, but since we don’t have any reason to put our data into NumPy\n",
    "arrays, we’ll use PyTorch tensors exclusively.\n",
    "\n",
    "The first thing we need do is to create our `SummaryWriter` objects (which we\n",
    "imported from `torch.utils.tensorboard`). The only parameter we’re going to pass\n",
    "in `is log_dir`, which we will initialize to something like `runs/p2ch11/2020-01-01_12.55.27-trn-dlwpt`. We can add a comment argument to our training script to change\n",
    "`dlwpt` to something more informative; use `python -m p2ch11.training --help` for\n",
    "more information.\n",
    "\n",
    "We create two writers, one each for the training and validation runs. Those writers\n",
    "will be reused for every epoch. When the `SummaryWriter` class gets initialized, it also\n",
    "creates the `log_dir` directories as a side effect. These directories show up in TensorBoard and can clutter the UI with empty runs if the training script crashes before any\n",
    "data gets written, which can be common when you’re experimenting with something.\n",
    "To avoid writing too many empty junk runs, we wait to instantiate the `SummaryWriter`\n",
    "objects until we’re ready to write data for the first time. This function is called from\n",
    "`logMetrics()`.\n",
    "#### Listing 11.21 training.py:127, `.initTensorboardWriters`\n",
    "```\n",
    "def initTensorboardWriters(self):\n",
    "    if self.trn_writer is None:\n",
    "        log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "        \n",
    "        self.trn_writer = SummaryWriter(\n",
    "            log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n",
    "        self.val_writer = SummaryWriter(\n",
    "            log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, the first epoch is kind of a mess, with the early output in the training\n",
    "loop being essentially random. When we save the metrics from that first batch, those\n",
    "random results end up skewing things a bit. Recall from figure 11.11 that TensorBoard has smoothing to remove noise from the trend lines, which helps somewhat.\n",
    "\n",
    "Another approach could be to skip metrics entirely for the first epoch’s training\n",
    "data, although our model trains quickly enough that it’s still useful to see the first epoch’s results. Feel free to change this behavior as you see fit; the rest of part 2 will\n",
    "continue with this pattern of including the first, noisy training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITING SCALARS TO TENSORBOARD\n",
    "Writing scalars is straightforward. We can take the `metrics_dict` we’ve already constructed and pass in each key/value pair to the `writer.add_scalar` method. The\n",
    "`torch.utils.tensorboard.SummaryWriter` class has the add_scalar method (http://mng.bz/RAqj) with the following signature.\n",
    "\n",
    "#### Listing 11.22 PyTorch torch/utils/tensorboard/writer.py:267\n",
    "```\n",
    "def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
    "# ...\n",
    "```\n",
    "The `tag` parameter tells TensorBoard which graph we’re adding values to, and the\n",
    "`scalar_value` parameter is our data point’s Y-axis value. The `global_step` parameter\n",
    "acts as the X-axis value.\n",
    "\n",
    "Recall that we updated the `totalTrainingSamples_count` variable inside the\n",
    "doTraining function. We’ll use `totalTrainingSamples_count` as the X-axis of our\n",
    "TensorBoard plots by passing it in as the `global_step` parameter. Here’s what that\n",
    "looks like in our code.\n",
    "#### Listing 11.23 training.py:323, `LunaTrainingApp.logMetrics`\n",
    "```\n",
    "for key, value in metrics_dict.items():\n",
    "writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
    "```\n",
    "Note that the slashes in our key names (such as `loss/all`) result in TensorBoard\n",
    "grouping the charts by the substring before the `/`.\n",
    "\n",
    "The documentation suggests that we should be passing in the epoch number as the\n",
    "`global_step parameter`, but that results in some complications. By using the number\n",
    "of training samples presented to the network, we can do things like change the number\n",
    "of samples per epoch and still be able to compare those future graphs to the ones we’re\n",
    "creating now. Saying that a model trains in half the number of epochs is meaningless if\n",
    "each epoch takes four times as long! Keep in mind that this might not be standard practice, however; expect to see a variety of values used for the global step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10 Why isn’t the model learning to detect nodules?\n",
    "Our model is clearly learning something—the loss trend lines are consistent as epochs\n",
    "increase, and the results are repeatable. There is a disconnect, however, between what\n",
    "the model is learning and what we want it to learn. What’s going on? Let’s use a quick\n",
    "metaphor to illustrate the problem.\n",
    "\n",
    "Imagine that a professor gives students a final exam consisting of 100 True/False\n",
    "questions. The students have access to previous versions of this professor’s tests going\n",
    "back 30 years, and every time there are only one or two questions with a True answer.\n",
    "The other 98 or 99 are False, every time.\n",
    "\n",
    "Assuming that the grades aren’t on a curve and instead have a typical scale of 90%\n",
    "correct or better being an A, and so on, it is trivial to get an A+: just mark every question as False! Let’s imagine that this year, there is only one True answer. A student like\n",
    "the one on the left in figure 11.13 who mindlessly marked every answer as False would\n",
    "get a 99% on the final but wouldn’t really demonstrate that they had learned anything\n",
    "(beyond how to cram from old tests, of course). That’s basically what our model is\n",
    "doing right now.\n",
    "\n",
    "![](images/11.13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast that with a student like the one on the right who also got 99% of the questions correct, but did so by answering two questions with True. Intuition tells us that\n",
    "the student on the right in figure 11.13 probably has a much better grasp of the material than the all-False student. Finding the one True question while only getting one\n",
    "answer wrong is pretty difficult! Unfortunately, neither our students’ grades nor our\n",
    "model’s grading scheme reflect this gut feeling.\n",
    "We have a similar situation, where 99.7% of the answers to “Is this candidate a nodule?” are “Nope.” Our model is taking the easy way out and answering False on every\n",
    "question.\n",
    "Still, if we look back at our model’s numbers more closely, the loss on the training\n",
    "and validation sets is decreasing! The fact that we’re getting any traction at all on the\n",
    "cancer-detection problem should give us hope. It will be the work of the next chapter\n",
    "to realize this potential. We’ll start chapter 12 by introducing some new, relevant terminology, and then we’ll come up with a better grading scheme that doesn’t lend\n",
    "itself to being gamed quite as easily as what we’ve done so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.11 Conclusion\n",
    "We’ve come a long way this chapter—we now have a model and a training loop, and\n",
    "are able to consume the data we produced in the last chapter. Our metrics are being\n",
    "logged to the console as well as graphed visually.\n",
    "\n",
    "While our results aren’t usable yet, we’re actually closer than it might seem. In\n",
    "chapter 12, we will improve the metrics we’re using to track our progress, and use\n",
    "them to inform the changes we need to make to get our model producing reasonable\n",
    "results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.12 Exercises\n",
    "1. Implement a program that iterates through a `LunaDataset` instance by wrapping it in a `DataLoader` instance, while timing how long it takes to do so. Compare these times to the times from the exercises in chapter 10. Be aware of the state of the cache when running the script.\n",
    "    - What impact does setting `num_workers=… `to 0, 1, and 2 have?\n",
    "    - What are the highest values your machine will support for a given combination of `batch_size=…` and `num_workers=…` without running out of memory?\n",
    "2. Reverse the sort order of noduleInfo_list. How does that change the behavior of the model after one epoch of training?\n",
    "3. Change `logMetrics` to alter the naming scheme of the runs and keys that are used in TensorBoard.\n",
    "    - Experiment with different forward-slash placement for keys passed in to `writer.add_scalar`.\n",
    "    - Have both training and validation runs use the same writer, and add the `trn` or `val` string to the name of the key.\n",
    "    - Customize the naming of the log directory and keys to suit your taste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.13 Summary\n",
    "- Data loaders can be used to load data from arbitrary datasets in multiple processes. This allows otherwise-idle CPU resources to be devoted to preparing data to feed to the GPU.\n",
    "- Data loaders load multiple samples from a dataset and collate them into a batch. PyTorch models expect to process batches of data, not individual samples.\n",
    "- Data loaders can be used to manipulate arbitrary datasets by changing the relative frequency of individual samples. This allows for “after-market” tweaks to a dataset, though it might make more sense to change the dataset implementation directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
